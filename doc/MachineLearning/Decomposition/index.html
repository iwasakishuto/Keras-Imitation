<!DOCTYPE html>
<html class="no-js" lang="en" >
  <head>
    <link rel="shortcut icon" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" />

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Shuto" />
    <meta name="twitter:card" content="summary" />
    <meta property="og:url" content="https://iwasakishuto.github.io/Kerasy/doc//Users/iwasakishuto/Github/portfolio/Kerasy/MkDocs/site/MachineLearning/Decomposition/index.html" />
    <meta property="og:title" content="Decomposition" />
    <meta property="og:description" content="Brief explanation of Decomposition and introduction of modules that can be used in Kerasy" />
    <meta property="og:image" content="https://iwasakishuto.github.io/images/FacebookImage/Kerasy.png" />
    <meta property="og:type" content="article" />

    <title>Decomposition - Kerasy Documentation</title>
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css' />

    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/jupyter.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme_extra.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme.css" />

    <script>
      // Current page data
      var mkdocs_page_name = "Decomposition";
      var mkdocs_page_input_path = "MachineLearning/Decomposition.md";
      var mkdocs_page_url = "/Kerasy/MachineLearning/Decomposition/";
    </script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/jquery-2.1.1.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/modernizr-2.8.3.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/theme.js" defer></script>

    <!-- Common CSS in my portofolio  -->
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/jupyter.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/custom.css" media="screen" />
    <link rel="apple-touch-icon" sizes="152x152" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" type="image/png" />
    <!-- Use fontawesome Icon -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css" rel="stylesheet" type="text/css" />
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css" />
    <link href="https://use.fontawesome.com/releases/v5.7.0/css/all.css" rel="stylesheet" integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous" />
    <!-- Syntax highlight -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/github.min.css" />
    <!-- Custom CSS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
      
    </script>
    <!-- Mermaid -->
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js" charset="UTF-8"></script>
    <script>
      mermaid.initialize({
        startOnLoad:true
      });
    </script>
    <script src="https://iwasakishuto.github.io/js/smooth-scroll.polyfills.min.js"></script>
  </head>
<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Kerasy Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../..">Home</a>
  </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">DeepLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/CNN/">CNN</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Optimizers/">Optimizers</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/NeuralNetwork/">Neural Network</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/ComputationalGraph/">Computational Graph</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Initializers/">Initializers</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">MachineLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  
    
    <li class="navtree toctree-l2 page current">
      <a class="current" href="./">
        Decomposition
          <span class="toctree-expand"></span>
      </a>
    </li>
    
      

  <li class="toctree-l2 current with-children">
    <a href="#pca">
      PCA
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#kernel-pca">
      Kernel PCA
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#sne">
      SNE
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#tsne">
      tSNE
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#umap">
      UMAP
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2">
    <ul class="subnav-l2 toc-hidden">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#tsne-vs-umap">tSNE vs. UMAP</a>
        </li>
    
    </ul>
  </li>


  
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../sampling/">Sampling</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Linear%20Regression/">Linear Regression</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Cluster/">Cluster</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../HMM/">HMM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../EM%20algorithm/">EM algorithm</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Tree/">Tree</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Support%20Vector%20Machine/">Support Vector Machine</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">BioInformatics</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Tandem%20Repeats/">Tandem Repeats</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/RefSeq/">RefSeq</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Microarray/">Microarray</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Secondary%20Structure/">Secondary Structure</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Alignment/">Alignment</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/String%20Search/">String Search</a>
  </li>
        
      </ul>
    </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Kerasy Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>MachineLearning &raquo;</li>
        
      
    
    <li>Decomposition</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/iwasakishuto/Kerasy"> Visit github &nbsp; <i class="fab fa-github"></i></a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <!-- SoC -->

<div class="frame">
  <h4>Purpose</h4>
  <p>The purpose of <b>decomposition (dimensionality reduction, feature extraction)</b> is to find the <font color="red"><b>"best"</b></font> <b>principal subspace</b> where data are expressed with much less dimensions, but keep the information as much as possible.</p>
</div>

<div class="admonition tip">
  <p class="admonition-title">Notebook</p>
  <p>Example Notebook: <a href="https://nbviewer.jupyter.org/github/iwasakishuto/Kerasy/blob/gh-pages/examples/decomposition.ipynb">Kerasy.examples.decomposition.ipynb</a></p>
</div>

<h2 id="pca">PCA</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

<p>Consider a dataset ${\mathbf{x}_n}$. If we consider a $D$-dimensional vector $\mathbf{u}_1$, each data point $\mathbf{x}_n\in\mathbb{R}^{D}$ is projected onto a scalar value $\mathbf{u}_1^T\mathbf{x}_n$. In that (one-dimensional) space,</p>

<ul>
<li>Mean:
$$\tilde{\mathbf{x}} = \frac{1}{N}\sum_{n=1}^N\mathbf{x}_n$$</li>
<li>Variance:
$$\frac{1}{N}\sum_{n=1}^N\left{\mathbf{u}<em n="1">1^T\mathbf{x}_n - \mathbf{u}_1^T\tilde{\mathbf{x}}\right}^2 = \mathbf{u}_1^T\mathbf{Su}_1\quad\mathbf{S}= \frac{1}{N}\sum</em>^N\left(\mathbf{x}-\tilde{\mathbf{x}}\right)\left(\mathbf{x}-\tilde{\mathbf{x}}\right)^T$$</li>
</ul>

<p>We now maximize the projected variance $\mathbf{u}_1^T\mathbf{Su}_1\quad\mathbf{S}$ with respect to $\mathbf{u}_1$, because we can think <font color="red"><b>"Larger variance" ↔︎ "Features are more expressed"</b></font>.</p>

<p>To avoid $|\mathbf{u}_1|\rightarrow\infty$, we introduce the normalization condition $\mathbf{u}_1^T\mathbf{u}_1=1$ using Lagrange multiplier $\lambda_1$.</p>

<p>Then, the object function is given by</p>

<p>$$\begin{aligned}
L &amp;= \mathbf{u}_1^T\mathbf{Su}_1 + \lambda_1\left(1-\mathbf{u}_1^T\mathbf{u}_1\right)\
\frac{\partial L}{\partial \mathbf{u}_1} &amp;= 2\mathbf{S}\mathbf{u}_1 - 2\lambda_1\mathbf{u}_1 = 0\
\therefore\mathbf{u}_1^T\mathbf{Su}_1 &amp;= \lambda_1 \quad (\because \text{left-multiplied $\mathbf{u}_1^T$})
\end{aligned}$$</p>

<p>The variance will be a maximum when we set $\mathbf{u}_1$ eaual to the eigenvector having the largest eigenvalue $\lambda_1$. This eigenvector is known as the first principal component.</p>

<p>We can define additional principal components in the same way.</p>

<div class="admonition summary">
  <p class="admonition-title">Summary</p>
  <p>If we consider an M-dimensional projection space, the optimal linear projection is defined by the M eigenvectors u1,...,uM of the data covariance matrix S corresponding to the M largest eigenvalues λ1,...,λM.</p>
</div>

<h2 id="kernel-pca">Kernel PCA</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kernelargs</span><span class="p">)</span>
</code></pre></div>

<p>The main idea is same with PCA, but we will perform it <strong>in the feature space</strong>, which implicitly defines a nonlinear principal component model in the original data space.</p>

<p>If we <font color="red"><b>assume that the projected data set has zero mean</b></font>, The covariance matrix in feature space is given by</p>

<p>$$
\mathbf{C} = \frac{1}{N}\sum_{n=1}^N\phi(\mathbf{x}_n)\phi(\mathbf{x}_n)^T
$$</p>

<p>and its <strong>eigenvector expansion</strong> is defined by</p>

<p>$$\begin{aligned}
\mathbf{C}\mathbf{v}<em n="1">i &amp;= \lambda_i\mathbf{v}_i\
\frac{1}{N}\sum</em>^N\phi(\mathbf{x}_n)\underset{\text{scalar}}{\left{\phi(\mathbf{x}_n)^T\mathbf{v}_i\right}} &amp;= \lambda_i\mathbf{v}_i\quad (\ast)\
\end{aligned}$$</p>

<p>From this equation, we see that the vector $\mathbf{v}_i$ is given by a linear combination of the $\phi(\mathbf{x}_n)$</p>

<p>$$\mathbf{v}<em n="1">i = \sum</em>^N a_{in}\phi(\mathbf{x}_n)$$</p>

<p>Substituting this expansion back into the eigenvector equation $(\ast)$, we obtain</p>

<p>$$\begin{aligned}
\frac{1}{N}\sum_{n=1}^N\phi(\mathbf{x_n})\phi(\mathbf{x}<em m="1">n)^T\sum</em>^Na_{im}\phi(\mathbf{x}<em n="1">m) &amp;= \lambda_i\sum</em>^Na_{in}\phi(\mathbf{x}<em n="1">n)\
\frac{1}{N}\sum</em>^Nk(\mathbf{x}<em m="1">l,\mathbf{x}_n)\sum</em>^Na_{i,m}k(\mathbf{x}<em n="1">n,\mathbf{x}_m) &amp;= \lambda_i\sum</em>^Na_{in}k(\mathbf{x}_l,\mathbf{x}_n)\quad(\because\text{multiplied $\phi(\mathbf{x}_l)^T$})\
\mathbf{K}^2\mathbf{a}_i &amp;= \lambda_iN\mathbf{Ka}_i\quad (\because\text{written in matrix notation})\
\end{aligned}$$</p>

<p>We can find solutions for $\mathbf{a}_i$ by solving the following eigenvalue problem. The variance in feature space will be a maximum when we set $\mathbf{a}_1$ eaual to the eigenvector having the largest eigenvalue $\lambda_i$.</p>

<p>$$\mathbf{Ka}_i = \lambda_iN\mathbf{a}_i$$</p>

<p>Having solved the eigenvector problem, the projection of a point $\mathbf{x}$ onto eigenvector $i$ is given by</p>

<p>$$y_i(\mathbf{x}) = \phi(\mathbf{x})^T\mathbf{v}<em n="1">i = \sum</em>^Na_{in}\phi(\mathbf{x})^T\phi(\mathbf{x}<em n="1">n) = \sum</em>^Na_{in}k(\mathbf{x},\mathbf{x}_n)$$</p>

<p>If <font color="red"><b>projected data set doesn't have zero mean</b></font>, the projected data points after centralizing are given by</p>

<p>$$\tilde{\phi(\mathbf{x}<em l="1">n)} = \phi(\mathbf{x}_n)-\frac{1}{N}\sum</em>^N\phi(\mathbf{x}_l)$$</p>

<p>and the corresponding elements of the Gram matrix are given by</p>

<p>$$\tilde{\mathbf{K}} = \mathbf{K} - \mathbf{1_NK} - \mathbf{K1_N} +\mathbf{1_NK1_N}$$</p>

<p>where $\mathbf{1}_N$ denotes the $N\times N$ matrix in which every element takes the value $1/N$.</p>

<div class="admonition warning">
  <p class="admonition-title">Warning</p>
  <p>I don't understand clearly how to deal with new data point xn.</p>
</div>

<h2 id="sne">SNE</h2>
<p><font color="red"><b>Stochastic Neighbor Embedding (SNE)</b></font> starts by <b>converting the high-dimensional Euclidean distances between datapoints into conditional probabilities</b>. The similarity of datapoint $\mathbf{x}<em j_i="j|i">j$ to datapoint $\mathbf{x}_i$ is conditional probability $p</em>$, that $\mathbf{x}_i$ would pick $\mathbf{x}_j$ as its neighbor <b>if neighbors were picked in propotion to their probability density under <font color="red">Gaussian centered at $\mathbf{x}_i$</font></b>.</p>

<p>Mathmatically, the conditional probability $p_{j|i}$ is given by</p>

<p>$$p_{j|i} = \frac{\exp\left(-|\mathbf{x}<em i k_neq="k\neq">i-\mathbf{x}_j|^2/2\boldsymbol{\sigma}_i^2\right)}{\sum</em>\exp\left(-|\mathbf{x}_i-\mathbf{x}_k|^2/2\boldsymbol{\sigma}_i^2\right)}$$</p>

<p>Because we are only interested in modeling pairwise similarities, we set the value of $p_{i|i}$ to zero.</p>

<p>For the low-dimensional counterparts $\mathbf{y}<em j_i="j|i">i$ and $\mathbf{y}_j$ of the high-dimensional data points $\mathbf{x}_i$ and $\mathbf{x}_j$, it is possible to compute a similar conditional probability, which we denote by $q</em>$</p>

<p>$$q_{j|i} = \frac{\exp\left(-|\mathbf{y}<em i k_neq="k\neq">i-\mathbf{y}_j|^2\right)}{\sum</em>\exp\left(-|\mathbf{y}_i-\mathbf{y}_k|^2\right)}$$</p>

<p>Again, we set $q_{i|i} = 0$, and the variance of the Gaussian to $\frac{1}{\sqrt{2}}$</p>

<p>Then, the cost function is given by</p>

<p>$$C = \sum_i\mathrm{KL}(P_i|Q_i) = \sum_i\sum_j p_{j|i}\log\frac{p_{j|i}}{q_{j|i}}$$</p>

<p>in which $P_i$ represents <b>the conditional probability distribution over all other data points given data points $\mathbf{x}_i$</b></p>

<p>The remaining parameter to be selected is the variance $\boldsymbol{\sigma_i}$. It is defined to produce a $P_i$ with a fixed <font color="red"><b>perplexity</b></font> that is specified by the user. The perplexity is defined as</p>

<p>$$Perp(P_i) = 2^{H(P_i)},\quad H(P_i) = -\sum_jp_{j|i}\log_2p_{j|i}$$</p>

<p>The minimization of the cost function is performed using a gradient descent method. The gradient has a surprisingly simple form</p>

<p>$$\frac{\partial C}{\partial \mathbf{y}<em j_i="j|i">i} = 2\sum_j\left(p</em> - q_{j|i} + p_{i|j} - q_{i|j}\right)\left(\mathbf{y}_i-\mathbf{y}_j\right)$$</p>

<h2 id="tsne">tSNE</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">tSNE</span><span class="p">(</span><span class="n">initial_momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">final_momoentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">min_gain</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">prec_max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div>

<p>As an alternative to minimizing the sum of the Kullback-Leibler divergence between a joint probability distribution $p_{j|i}$ and $q_{j|i}$, it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution:</p>

<p>$$C = KL\left(P|Q\right) = \sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}$$</p>

<p>In t-SNE,
- Employ a <b>Student t-distribution with one degree of freedom (which is the same as a Cauchy distribution)</b> as the heavy-tailed distribution in the <b>low-dimensional map.</b> Using this distribution, the joint probabilities $q_{ij}$ are defined as
$$q_{ij} = \frac{\left(1 + |\mathbf{y}<em k_neq="k\neq" l>i-\mathbf{y}_j|^2\right)^{-1}}{\sum</em>\left(1 + |\mathbf{y}<em ij>k-\mathbf{y}_l|^2\right)^{-1}}$$
- If there is an outlier $\mathbf{x}_i$, low-dimensional map point $\mathbf{y}_i$ has very little effect on the cost function, so set
$$p</em> = \frac{p_{j|i} + p_{i|j}}{2n}$$
to ensure that $\sum_jp_{ij}&gt;\frac{1}{2n}$ for all datapoints $\mathbf{x}_i$</p>

<p>The gradient is given by</p>

<p>$$\frac{\partial C}{\partial \mathbf{y}<em ij>i} = 4\sum_j\left(p</em> - q_{ij}\right) \left(\mathbf{y}_i - \mathbf{y}_j\right)\left(1 + |\mathbf{y}_i - \mathbf{y}_j|^2\right)^{-1}$$</p>

<div class="admonition reference">
  <p class="admonition-title">Reference</p>
  <ul>
    <li><a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf">Visualizing Data using t-SNE</a></li>
    <li><a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">Pattern Recognition and Machine Learning by Christopher Bishop</a></li>
  </ul>
</div>

<h2 id="umap">UMAP</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="n">metric_kwds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sigma_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">sigma_tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">sigma_lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_upper</span><span class="o">=</span><span class="mf">1e3</span><span class="p">)</span>
</code></pre></div>

<p>UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology.</p>

<div class="admonition reference">
  <p class="admonition-title">Reference</p>
  <ul>
    <li><a href="https://arxiv.org/pdf/1802.03426.pdf">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></li>
  </ul>
</div>

<h4 id="tsne-vs-umap">tSNE vs. UMAP</h4>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">tSNE</th>
<th align="center">UMAP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"></td>
<td align="center"><strong>pure Machine Learning semi-empirical algorithm</strong></td>
<td align="center"><strong>based on solid mathematical principles</strong></td>
</tr>
<tr>
<td align="center"></td>
<td align="center">Preserves <strong>Only Local</strong> Structure</td>
<td align="center">Preserves <strong>Global</strong> Structure</td>
</tr>
<tr>
<td align="center">probability in <strong>high-dimensional space</strong></td>
<td align="center" i j_mid="j\mid">$$p_</td>
<td align="center" i_mid="i\mid" j>$$p_</td>
</tr>
<tr>
<td align="center">joint probability in <strong>high-dimensional space</strong></td>
<td align="center" ij>$$p_</td>
<td align="center" ij>$$p_</td>
</tr>
<tr>
<td align="center"></td>
<td Perplexity align="center">$$\text</td>
<td align="center"><strong>number of nearest neighbors</strong>$$k = 2^{\sum_jp_{i\mid j}}$$</td>
</tr>
<tr>
<td align="center">probability in <strong>low-dimensional space</strong></td>
<td align="center" ij>$$q_</td>
<td align="center" ij>$$q_</td>
</tr>
<tr>
<td align="center">loss function</td>
<td KL align="center">$\mathrm</td>
<td CE align="center">$$\mathrm</td>
</tr>
<tr>
<td align="center">Optimization</td>
<td align="center"><strong>Gradient Descent</strong></td>
<td align="center"><strong>Stochastic Gradient Descent</strong></td>
</tr>
<tr>
<td align="center">Gradients</td>
<td _partial_mathrm_KL="\partial\mathrm{KL" align="center">$$\frac</td>
<td align="center">$$\frac{\partial\mathrm{CE}}{\partial y_i} = \sum_j\left<a href="y_i-y_j">\frac{2abd_{ij}^{2(b-1)}P(X)}{1 + ad_{ij}^{2b}}-\frac{2b(1-P(X))}{d_{ij}^2\left(1 + ad_{ij}^{2b}\right)}\right</a>$$</td>
</tr>
<tr>
<td align="center">Initial low-dimensional coordinates</td>
<td align="center"><strong>Random Normal Initialization</strong></td>
<td align="center"><strong>Graph Laplacian</strong></td>
</tr>
</tbody>
</table>

<!-- EoC -->

<p></section>
    </div>
</div>
</div></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../sampling/" class="btn btn-neutral float-right" title="Sampling">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../DeepLearning/Initializers/" class="btn btn-neutral" title="Initializers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../DeepLearning/Initializers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../sampling/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
