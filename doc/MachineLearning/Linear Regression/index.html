<!DOCTYPE html>
<html class="no-js" lang="en" >
  <head>
    <link rel="shortcut icon" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" />

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shuto" />
    <meta name="twitter:card" content="summary">
    <meta property="og:url" content="https://iwasakishuto.github.io/Kerasy/doc/MachineLearning/Linear Regression/index.html" />
    <meta property="og:title" content="Linear Regression">
    <meta property="og:description" content="Brief explanation of Linear Regression and introduction of modules that can be used in Kerasy">
    <meta property="og:image" content="https://iwasakishuto.github.io/images/FacebookImage/Kerasy.png"/>
    <meta property="og:type" content="article" />

    <title>Linear Regression - Kerasy Documentation</title>
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/custom.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/jupyter.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme_extra.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme.css">

    <script>
      // Current page data
      var mkdocs_page_name = "Linear Regression";
      var mkdocs_page_input_path = "MachineLearning/Linear Regression.md";
      var mkdocs_page_url = "/Kerasy/MachineLearning/Linear Regression/";
    </script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/jquery-2.1.1.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/modernizr-2.8.3.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/theme.js" defer></script>

    <!-- Common CSS in my portofolio  -->
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/jupyter.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/custom.css" media="screen">
    <link rel="apple-touch-icon" sizes="152x152" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" type="image/png" />
    <!-- Use fontawesome Icon -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://use.fontawesome.com/releases/v5.7.0/css/all.css" rel="stylesheet" integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous">
    <!-- Syntax highlight -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/github.min.css">
    <!-- Custom CSS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
    </script>
    <!-- Mermaid -->
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js" charset="UTF-8"></script>
    <script>
      mermaid.initialize({
        startOnLoad:true
      });
    </script>
    <script src="https://iwasakishuto.github.io/js/smooth-scroll.polyfills.min.js"></script>
  </head>
<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Kerasy Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../..">Home</a>
  </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">DeepLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/CNN/">CNN</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Optimizers/">Optimizers</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/NeuralNetwork/">Neural Network</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/ComputationalGraph/">Computational Graph</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Initializers/">Initializers</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">MachineLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Decomposition/">Decomposition</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../sampling/">Sampling</a>
  </li>
        
          


  
    
    <li class="navtree toctree-l2 page current">
      <a class="current" href="./">
        Linear Regression
          <span class="toctree-expand"></span>
      </a>
    </li>
    
      



      

  <li class="toctree-l2">
    <a href="#linear-regression-ridge-l2">
      Linear Regression (Ridge, L2)
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#linear-regression-lasso-l1">
      Linear Regression (LASSO, L1)
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#bayesian-linear-regression">
      Bayesian Linear Regression
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#evidence-approximation-bayesian-regression">
      Evidence Approximation Bayesian Regression
      <span class="toctree-expand"></span>
    </a>
  </li>




  
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Cluster/">Cluster</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../HMM/">HMM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../EM%20algorithm/">EM algorithm</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Tree/">Tree</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Support%20Vector%20Machine/">Support Vector Machine</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">BioInformatics</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Tandem%20Repeats/">Tandem Repeats</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/RefSeq/">RefSeq</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Microarray/">Microarray</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Secondary%20Structure/">Secondary Structure</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Alignment/">Alignment</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/String%20Search/">String Search</a>
  </li>
        
      </ul>
    </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Kerasy Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>MachineLearning &raquo;</li>
        
      
    
    <li>Linear Regression</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/iwasakishuto/Kerasy"> Visit github &nbsp; <i class="fab fa-github"></i></a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <div class="admonition tip">
  <p class="admonition-title">Notebook</p>
  <p>Example Notebook: <a href="https://nbviewer.jupyter.org/github/iwasakishuto/Kerasy/blob/gh-pages/examples/linear.ipynb">Kerasy.examples.linear.ipynb</a></p>
</div>

<h2 id="linear-regression">Linear Regression</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>


<p>The simplest model for regression is</p>
<p>$$y(\mathbf{x},\mathbf{w}) = w_0 + w_1x_1 + \cdots + w_Dx_D$$</p>
<p>However, because of the linearity for the input variables $x_i$, this model has a poor expressive ability. Therefore, we extend it by considering linear combinations of <strong>fixed nonlinear functions</strong> of the input variables, of the form</p>
<p>$$y(\mathbf{x},\mathbf{w}) = \sum_{j=0}^{M-1}w_j\phi_j(\mathbf{x}) = \mathbf{w}^T\boldsymbol{\phi}(\mathbf{x})$$</p>
<p>In terms of the target variable $t$, we assume that it is given by a $y(\mathbf{x},\mathbf{w})$ <strong>(deterministic)</strong> with additive <strong>Gaussian noise</strong> so that</p>
<p>$$p(t|\mathbf{x},\mathbf{w},\beta) = \mathcal{N}\left(t|y(\mathbf{x},\mathbf{w}),\beta^{-1}\right)$$</p>
<p>Therefore, if we observe data, we obtain the following expression for the likelihood function</p>
<p>$$
\begin{aligned}
p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) = \prod_{n=1}^N\mathcal{N}\left(t_n|\mathbf{w}^T\phi(\mathbf{x}<em>n),\beta^{-1}\right)\
\begin{cases}
\begin{aligned}
\ln p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) &amp;=\sum</em>{n=1}^N\ln\mathcal{N}\left(t_n|\mathbf{w}^T\phi(\mathbf{x}<em>n),\beta^{-1}\right)\
&amp;= \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi) - \beta E_D(\mathbf{w})\
E_D(\mathbf{w}) &amp;= \frac{1}{2}\sum</em>{n=1}^N\left{t_n - \mathbf{w}^T\phi(\mathbf{x}_n)\right}^2
\end{aligned}
\end{cases}
\end{aligned}
$$</p>
<p>As we obtain such a likelihood, it is easy to maximize them respect to $\mathbf{w}$ and $\beta$. (Setting the gradient to zero.)</p>
<p>$$
\begin{aligned}
\nabla\ln p(\mathbf{t}|\mathbf{w},\beta) &amp;= \sum_{n=1}^N\left{t_n - \mathbf{w}^T\phi(\mathbf{x}<em>n)\right}\phi(\mathbf{x}_n)^T = 0\
0 &amp;= \sum</em>{n=1}^Nt_n\phi(\mathbf{x}<em>n)^T - \mathbf{w}^T\left(\sum</em>{n=1}^N\phi(\mathbf{x}<em>n)\phi(\mathbf{x}_n)^T\right)\
\therefore\mathbf{w}</em>{\text{ML}} &amp;= \left(\Phi^T\Phi\right)^{-1}\Phi^T\mathbf{t}\
\frac{1}{\beta_{\text{ML}}} &amp;= \frac{1}{N}\sum_{n=1}^N\left{t_n -\mathbf{w}_{\text{ML}}^T\phi(\mathbf{x}_n)\right}^2
\end{aligned}
$$</p>
<ul>
<li>$\Phi$ is an $N\times M$ matrix, called the <strong>design matrix</strong>, whose elements are given by $\Phi_{nj} = \phi_j(\mathbf{x}_n)$</li>
<li>The quantity $\Phi^{\dagger}\equiv\left(\Phi^T\Phi\right)^{-1}\Phi^T$ is known as the <font color="red"><b>Moore-Penrose pseudo-inverse</b></font> of the matrix $\Phi$</li>
</ul>
<h2 id="linear-regression-ridge-l2">Linear Regression (Ridge, L2)</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">LinearRegressionRidge</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>


<p>As you see at the <a href="https://nbviewer.jupyter.org/github/iwasakishuto/Kerasy/blob/gh-pages/examples/linear.ipynb">notebook</a>, it is likely to <font color="red"><b>over-fit</b></font> especially when model is too complicated. ($M\rightarrow\text{Large}$) For avoiding it, we introduce the idea of <strong>adding a regularization term to an error function.</strong></p>
<p>$$E_D(\mathbf{w}) + \lambda E_{\mathbf{w}}(\mathbf{w})$$</p>
<p>where $\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\mathbf{w})$ and the regularization term $E_{\mathbf{w}}(\mathbf{w})$.</p>
<p>One of the simplest forms of regularizer is given by</p>
<p>$$E_{\mathbf{w}}(\mathbf{w}) = \frac{1}{2}\mathbf{w}^T\mathbf{w}$$</p>
<p>Then, the total error function and Maximum likelihood estimated $\mathbf{w}$ is expressed as</p>
<p>$$
\frac{1}{2}\sum_{n=1}^N\left{t_n- \mathbf{w}^T\phi(\mathbf{x}<em>n)\right}^2 + \frac{\lambda}{2}\mathbf{w}^T\mathbf{w}\
\mathbf{w}</em>{\text{ML}} = \left(\lambda\mathbf{I} + \Phi^T\Phi\right)^{-1}\Phi^T\mathbf{t}
$$</p>
<h2 id="linear-regression-lasso-l1">Linear Regression (LASSO, L1)</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">LinearRegressionLASSO</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>


<p>Consider the regularizer given by</p>
<p>$$E_{\mathbf{w}}(\mathbf{w}) = |\mathbf{w}|$$</p>
<p>This function is not differentiable, so we use <font color="red"><b>ADMM(Alternating Direction Method of Multipliers)</b></font>. In this algorithm, we use <font color="red"><b>Extended Lagrange multiplier</b></font>.</p>
<p>$$\begin{aligned}
L_{\rho}\left(\mathbf{w},\mathbf{z},\boldsymbol{\alpha}\right) &amp;= \frac{1}{2}|\mathbf{y} - \mathbf{Xw}|^2 + \lambda|\mathbf{z}| + \boldsymbol{\alpha}^T\left(\mathbf{w}-\mathbf{z}\right) + \frac{\rho}{2}|\mathbf{w}-\mathbf{z}|^2\
\text{subject to  }\mathbf{w} &amp;= \mathbf{z}
\end{aligned}$$</p>
<p>We minimize $L_{\rho}$ respect to $\mathbf{w}$ and $\mathbf{z}$ <strong>repeatedly</strong>.</p>
<p>$$
\begin{cases}
  \begin{aligned}
    \mathbf{w}&amp;\leftarrow\underset{\mathbf{w}}{\text{argmin}}L_{\rho}\left(\mathbf{w},\mathbf{z},\boldsymbol{\alpha}\right)\text{ with fixed with $\mathbf{z},\boldsymbol{\alpha}$}\
    &amp;=\left(\mathbf{X}^T\mathbf{X} + \rho\mathbf{I}\right)^{-1}\left(\mathbf{X}^T\mathbf{y} - \boldsymbol{\alpha} + \rho\mathbf{z}\right)\
    \mathbf{z}&amp;\leftarrow\underset{\mathbf{z}}{\text{argmin}}L_{\rho}\left(\mathbf{w},\mathbf{z},\boldsymbol{\alpha}\right)\text{ with fixed with $\mathbf{w},\boldsymbol{\alpha}$}\
    &amp;= \text{prox}<em>{\frac{\lambda}{\rho}|\ast|}\left(w_i + \frac{\alpha_i}{\rho}\right)\
    \boldsymbol{\alpha}&amp;\leftarrow\boldsymbol{\alpha} + \rho\left(\mathbf{w}-\mathbf{z}\right)
  \end{aligned}
\end{cases}\
\text{prox}</em>{c|\ast|}(z_0) := \underset{z}{\text{argmin}}\left{c|z| + \frac{1}{2}\left(z-z_0\right)^2\right} = \begin{cases}z_0-c &amp; (c &lt; z_0)\0 &amp; (-c\leq z_0 \leq c)\z_0 + c &amp; (z_0 &lt; -c)\end{cases}
$$</p>
<hr />
<p>A more general regularizer is sometimes used, for which the regularized error takes the form</p>
<p>$$\frac{1}{2}\sum_{n=1}^N\left{t_n-\mathbf{w}^T\phi(\mathbf{x}<em>n)\right}^2 + \frac{\lambda}{2}\sum</em>{j=1}^M|w_j|^q$$</p>
<hr />
<h2 id="bayesian-linear-regression">Bayesian Linear Regression</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">BayesianLinearRegression</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>


<p>We turn to a <strong>Bayesian treatment</strong> of linear regression, which will <strong>avoid the over-fitting problem</strong> of maximum likelihood, and which will also lead to <strong>automatic methods</strong> of determining model complexity <strong>using the training data alone.</strong> (Not requiring <strong>Hold-out methods</strong>)</p>
<ol>
<li>We assume that the prior distribution of $\mathbf{w}$ is given by a Gaussian distribution of the form
$$p(\mathbf{w}|\alpha) = \mathcal{N}\left(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I}\right)$$</li>
<li>As I mentioned before, likelihood is given by
$$p(t|\mathbf{x},\mathbf{w},\beta) = \mathcal{N}\left(t|y(\mathbf{x},\mathbf{w}),\beta^{-1}\right)\
p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) = \prod_{n=1}^N\mathcal{N}\left(t_n|\mathbf{w}^T\phi(\mathbf{x}_n),\beta^{-1}\right)$$</li>
<li>From the Bayes' theorem, <strong>posterior distribution is proportional to the product of prior distribution and likelihood</strong>. Therefore, posterior distribution is described as
$$p(\mathbf{w}|\mathbf{t}) = \mathcal{N}\left(\mathbf{m}_N,\mathbf{S}_N\right)\
\begin{cases}\mathbf{m}_N &amp;= \beta\mathbf{S}_N\Phi^T\mathbf{t}\\mathbf{S}_N^{-1} &amp;= \alpha\mathbf{I} + \beta\Phi^T\Phi\end{cases}$$</li>
<li>Finally, the <strong>predictive distribution</strong> defined by:
$$\begin{aligned}
p(t|\mathbf{t},\alpha,\beta) &amp;= \int p(t|\mathbf{w},\beta)p(\mathbf{w}|\mathbf{t},\alpha,\beta)d\mathbf{w}\
&amp;= \mathcal{N}\left(t|\mathbf{m}_N^T\phi(\mathbf{x}),\sigma^2_N(\mathbf{x})\right)\
\sigma^2_N(\mathbf{x}) &amp;= \frac{1}{\beta} + \phi(\mathbf{x})^T\mathbf{S}\phi(\mathbf{x})
\end{aligned}$$</li>
</ol>
<div class="admonition summary">
  <p class="admonition-title">Reference</p>
  <p>If you want to know the derivation process (calculation process) in detail, please visit <a href="https://iwasakishuto.github.io/University/3A/生物データマイニング論-4.html">here</a>.</p>
</div>

<h2 id="evidence-approximation-bayesian-regression">Evidence Approximation Bayesian Regression</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">EvidenceApproxBayesianRegression</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>


<p>In a fully Bayesian treatment of the linear basis function model, we would <strong>introduce prior distributions over the hyperparameters $\alpha$, and $\beta$.</strong></p>
<p>$$
\begin{aligned}
p(t|\mathbf{t},\mathbf{X},\mathbf{x},\alpha,\beta)
&amp;= \int p(t|\mathbf{w},\mathbf{x},\beta)p(\mathbf{w}|\mathbf{t},\mathbf{X},\alpha,\beta)d\mathbf{w}\
&amp;\Rightarrow \int p(t|\mathbf{w},\mathbf{x},\beta)p(\mathbf{w}|\mathbf{t},\mathbf{X},\alpha,\beta)p(\alpha,\beta|\mathbf{t},\mathbf{X})d\mathbf{w}d\alpha d\beta \quad (\ast)
\end{aligned}
$$</p>
<p>Marginalize with respect to these hyperparameters as well as with respect to the parameters $\mathbf{w}$ to make predictions.</p>
<p>As, the <strong>complete marginalization over all of these variables is analytically intractable</strong>, We will maximize $(\ast)$ in line with the framework of <font color="red"><b>empirical Bayes</b></font> (or <strong>type 2 maximum likelihood</strong>, <strong>generalized maximum likelihood</strong>, <strong>evidence approximation</strong>)</p>
<p>In the framework, we repeat the following process.</p>
<ol>
<li>Obtain the marginal likelihood function by first integrating over the parameters $\mathbf{w}$
$$p(\mathbf{t}|\alpha,\beta) = \int p(\mathbf{t}|\mathbf{w},\mathbf{X},\beta)p(\mathbf{w}|\alpha)d\mathbf{w}$$</li>
<li>Maximize $p(\mathbf{t}|\alpha,\beta)$ with respect to $\alpha$ and $\beta$.
$$
\begin{cases}
  \begin{aligned}
    \gamma &amp;= \sum_{i}\frac{\lambda_i}{\lambda_i + \alpha^{\text{old}}}\
    \alpha^{\text{new}} &amp;= \frac{\gamma}{\mathbf{m}<em>N^T\mathbf{m}_N}\
    \frac{1}{\beta^{\text{new}}} &amp;= \frac{1}{N-\gamma}\sum</em>{n=1}^N\left{t_n - \mathbf{m}_N^T\phi(\mathbf{x}_n)\right}^2
  \end{aligned}
\end{cases}
$$</li>
</ol>
<div class="admonition reference">
  <p class="admonition-title">Reference</p>
  <ul>
    <li><a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">Pattern Recognition and Machine Learning by Christopher Bishop</a></li>
  </ul>
</div>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Cluster/" class="btn btn-neutral float-right" title="Cluster">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../sampling/" class="btn btn-neutral" title="Sampling"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../sampling/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Cluster/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
