{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Kerasy Kerasy: The Python Deep Learning library \"Keras\" is a high-level neural networks API, written in Python and capable of running on top of TensorFlow , CNTK , or Theano . It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Danger This is not Keras which is well known deep learning framework, so if you want to enjoy deep learning, visit Keras . Installation There are two ways to install Keras: Install Kerasy from PyPI (recommended): $ sudo pip install kerasy Alternatively: install Kerasy from the GitHub source: $ git clone https://github.com/iwasakishuto/Kerasy.git $ cd Kerasy $ sudo python setup.py install Why I made Kerasy I study deep learning, and machine learning theory at a University, and have practical experiences in internships, hackathons, and competitions. Through those experiences, I felt I wanted to go further, next step, building my original model , which is different from the conventional one (NN, CNN, RNN, ..., backpropagation series.) On the other hand, I thought it is extremely important to understand the conventional one properly, so by building the framework on my own, I try confirming my understanding of them. As it is too hard to build a deep learning framework from scratch, I decided to imitate Keras , one of the most popular frameworks. Future prospects I am particularly interested in \"synthetic biology\" (redesigning, or remodeling organisms by engineering them to have new abilities) and \"neuroscience\" (Back-engineering the flexibility of brain structure concerned with mechanism of memory, pattern recognition, and so on) . Therefore, I want to study deeply about them and apply them for Human augmentation purpose to create the more wonderful world. Slack Themes Copy and paste these values to your Slack settings Sidebar Theme section: #061C32,#A51C24,#D5E8E7,#404040,#858100,#FFFFFF,#46C146,#83274E,#83274E,#FCFCFC","title":"Home"},{"location":"#welcome-to-kerasy","text":"Kerasy: The Python Deep Learning library \"Keras\" is a high-level neural networks API, written in Python and capable of running on top of TensorFlow , CNTK , or Theano . It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Danger This is not Keras which is well known deep learning framework, so if you want to enjoy deep learning, visit Keras .","title":"Welcome to Kerasy"},{"location":"#installation","text":"There are two ways to install Keras: Install Kerasy from PyPI (recommended): $ sudo pip install kerasy Alternatively: install Kerasy from the GitHub source: $ git clone https://github.com/iwasakishuto/Kerasy.git $ cd Kerasy $ sudo python setup.py install","title":"Installation"},{"location":"#why-i-made-kerasy","text":"I study deep learning, and machine learning theory at a University, and have practical experiences in internships, hackathons, and competitions. Through those experiences, I felt I wanted to go further, next step, building my original model , which is different from the conventional one (NN, CNN, RNN, ..., backpropagation series.) On the other hand, I thought it is extremely important to understand the conventional one properly, so by building the framework on my own, I try confirming my understanding of them. As it is too hard to build a deep learning framework from scratch, I decided to imitate Keras , one of the most popular frameworks.","title":"Why I made Kerasy"},{"location":"#future-prospects","text":"I am particularly interested in \"synthetic biology\" (redesigning, or remodeling organisms by engineering them to have new abilities) and \"neuroscience\" (Back-engineering the flexibility of brain structure concerned with mechanism of memory, pattern recognition, and so on) . Therefore, I want to study deeply about them and apply them for Human augmentation purpose to create the more wonderful world.","title":"Future prospects"},{"location":"#slack-themes","text":"Copy and paste these values to your Slack settings Sidebar Theme section: #061C32,#A51C24,#D5E8E7,#404040,#858100,#FFFFFF,#46C146,#83274E,#83274E,#FCFCFC","title":"Slack Themes"},{"location":"BioInformatics/Alignment/","text":"Notebook Example Notebook: Kerasy.examples.alignment.ipynb Why compare DNA sequences? To understand the function of the DNA. (The more similar the DNA sequences are the more similar the functions are.) To find where are the important region. To know the phylogenetic tree of organisms. To inspect where the DNA come from. Needleman-Wunsch-Gotoh class kerasy . Bio . alignment . NeedlemanWunshGotoh ( ** kwargs ) Global Alignment : Align the entire two sequences. Direction: Forward params: match mismatch gap_opening gap_extension $$ \\begin{aligned} M(i,j) &= \\max \\begin{cases} M(i-1,j-1) + s(x_i, y_j)\\ X(i-1,j-1) - s(x_i, y_j)\\ Y(i-1,j-1) - s(x_i, y_j) \\end{cases}\\ X(i,j) &= \\max \\begin{cases} M(i-1,j) - d\\ X(i-1,j) - d\\ \\end{cases}\\ Y(i,j) &= \\max \\begin{cases} M(i,j-1) - d\\ Y(i,j-1) - d\\ \\end{cases}\\ d &= \\text{gap opening penalty}\\ e &= \\text{gap extension penalty}\\ s(x_i,y_j) &= \\begin{cases} \\text{match score} & (\\text{if $x_i=y_j$})\\ \\text{mismatch score} & (\\text{otherwise.})\\ \\end{cases} \\end{aligned} $$ Backward Needleman-Wunsh-Gotoh class kerasy . Bio . alignment . BackwardNeedlemanWunshGotoh ( ** kwargs ) Global Alignment : Direction: Backward params: match mismatch gap_opening gap_extension $$ \\begin{aligned} bM(i,j) &= \\max \\begin{cases} bM(i+1,j+1) + s(x_{i+1}, y_{j+1})\\ bX(i+1,j) - d\\ bY(i,j+1) - d \\end{cases}\\ bX(i,j) &= \\max \\begin{cases} bM(i+1,j+1) + s(x_{i+1},y_{j+1})\\ bX(i+1,j) - e \\end{cases}\\ bY(i,j) &= \\max \\begin{cases} bM(i+1,j+1) - s(x_{i+1},y_{j+1})\\ Y(i,j+1) - e\\ \\end{cases}\\ \\end{aligned} $$ This algorithm is for find the maximum alignment under the condition of $x_i$ and $y_j$ are aligned. It is given by: $$A(i,j) = M(i,j) + bM(i,j)$$ Smith-Waterman class kerasy . Bio . alignment . SmithWaterman ( ** kwargs ) Local Alignment : Detect the most similar part of the sequences and align them. Direction: Forward params: match mismatch gap_opening gap_extension $$ \\begin{aligned} M(i,j) &= \\max \\begin{cases} 0\\ M(i-1,j-1) + s(x_i, y_j)\\ X(i-1,j-1) - s(x_i, y_j)\\ Y(i-1,j-1) - s(x_i, y_j) \\end{cases}\\ X(i,j) &= \\max \\begin{cases} M(i-1,j) - d\\ X(i-1,j) - d\\ \\end{cases}\\ Y(i,j) &= \\max \\begin{cases} M(i,j-1) - d\\ Y(i,j-1) - d\\ \\end{cases}\\ \\end{aligned} $$ Pair HMM class kerasy . Bio . alignment . PairHMM ( ** kwargs ) We can interpret alignment probabilistically. $$ \\begin{aligned} P(\\pi|x,y) &= \\frac{P(x,y,\\pi)}{P(x,y)}\\ P(x,y) &= \\sum_{\\text{alignments }\\pi}P(x,y,\\pi)\\ P(x_i\\diamondsuit y_i|x,y) &= \\frac{\\sum_{\\pi\\in\\Omega_{ij}}P(x,y,\\pi)}{P(x,y)}\\ &= \\frac{F^M(i,j)\\cdot B^M(i,j)}{P(x,y)} \\end{aligned} $$ Probability Explanation $$P(\\pi\\mid x,y)$$ The probability of alignment $\\pi$ is obtained given $x_i$ and $y_j$. $$P(x_i\\diamondsuit y_i\\mid x,y)$$ The probability of $x_i$ and $y_j$ are aligned. Global Alignment Direction: Forward params: delta epsilon tau px_e_y px_ne_y qx qy Viterbi Algorithm $$ \\begin{aligned} V^M(i,j) &= p_{x_iy_j}\\max\\begin{cases}(1-2\\delta-\\tau)V^M(i-1,j-1)\\(1-\\varepsilon-\\tau)V^X(i-1,j-1)\\(1-\\varepsilon-\\tau)V^Y(i-1,j-1)\\end{cases}\\ V^X(i,j) &= q_{x_i}\\max\\begin{cases}\\delta V^M(i-1,j)\\\\varepsilon V^X(i-1,j)\\end{cases}\\ V^Y(i,j) &= q_{y_j}\\max\\begin{cases}\\delta V^M(i,j-1)\\\\varepsilon V^X(i,j-1)\\end{cases}\\ V^E(n,m) &= \\tau\\max\\left(V^M(n,m),V^X(n,m),V^Y(n,m)\\right) \\end{aligned} $$ Forward Algorithm $$ \\begin{aligned} F^M(i,j) &= p_{x_iy_j}\\text{sum}\\begin{cases}(1-2\\delta-\\tau)F^M(i-1,j-1)\\(1-\\varepsilon-\\tau)F^X(i-1,j-1)\\(1-\\varepsilon-\\tau)F^Y(i-1,j-1)\\end{cases}\\ F^X(i,j) &= q_{x_i}\\text{sum}\\begin{cases}\\delta F^M(i-1,j)\\\\varepsilon F^X(i-1,j)\\end{cases}\\ F^Y(i,j) &= q_{y_j}\\text{sum}\\begin{cases}\\delta F^M(i,j-1)\\\\varepsilon F^X(i,j-1)\\end{cases}\\ F^E(n,m) &= \\tau\\ \\text{sum}\\left(F^M(n,m),F^X(n,m),F^Y(n,m)\\right) \\end{aligned} $$ Backward Algorithm $$ \\begin{aligned} B^M(i,j) &= \\text{sum}\\begin{cases}(1-2\\delta-\\tau)p_{x_iy_j}B^M(i+1,j+1)\\\\delta q_{x_{i+1}}B(i+1,j)\\\\delta q_{y_{j+1}}B^Y(i,j+1)\\end{cases}\\ B^X(i,j) &= (1-\\varepsilon-\\tau)p_{x_{i+1},y_{j+1}}B^M(i+1,j+1) + \\varepsilon q_{x_{i+1}}B^X(i+1,j)\\ B^Y(i,j) &= (1-\\varepsilon-\\tau)p_{x_{i+1},y_{j+1}}B^M(i+1,j+1) + \\varepsilon q_{y_{j+1}}B^Y(i,j+1)\\ \\end{aligned} $$ Baum-Welch Algorithm Although the ordinary Hidden Markov Models calculate the optimal parameters from the perspective of time span $t$, PairHMM focus on the position $u,v$ of each sequence. Estep $$ \\begin{aligned} \\xi_{u,v}\\left(i,j\\right) &=\\frac{f_i\\left(u,v\\right)A_{ij}\\phi_j\\left(\\diamond_1\\right)b_j\\left(\\diamond_2,\\diamond_3\\right)}{\\sum_{k=1}^Kf_k\\left(u,v\\right)b_k\\left(u,v\\right)}\\ \\gamma_i\\left(u,v\\right) &=\\frac{f_i\\left(u,v\\right)b_j\\left(u,v\\right)}{\\sum_{k=1}^Kf_k\\left(u,v\\right)b_k\\left(u,v\\right)} \\end{aligned} $$ variable meaning $f_i\\left(u,v\\right)$ The probability of ending up in state $i$ after aligning the two sequences $\\mathbf{X,Y}$ up to observation $u$ and $v$ respectively. $A_{ij}$ the transition probability from state $i$ to state $j$. $\\phi_j\\left(\\diamond_1\\right)$ the emission probability of emitting $\\diamond_1$ in state $j$. $b_j\\left(\\diamond_2,\\diamond_3\\right)$ the probability of being in state $j$, given the sequences $\\mathbf{X,Y}$ are aligned from observation $\\diamond_2+1$ and $\\diamond_3+1$ to the end of the sequences respectively. \u203b \"$\\diamond_{[1,2,3]}$\" depends on the states ($M,X,Y$) as below: state\\variable $\\phi_j\\left(\\diamond_1\\right)$ $b_j\\left(\\diamond_2,\\diamond_3\\right)$ $M$ $e\\left(x_{u+1},y_{v+1}\\right)$ $b_j\\left(u+1,v+1\\right)$ $X$ $e\\left(x_{u+1}\\right)$ $b_j\\left(u+1,v\\right)$ $Y$ $e\\left(y_{v+1}\\right)$ $b_j\\left(u,v+1\\right)$ M step $\\pi_k$ $$\\pi^{\\star}_i = \\sum_w\\gamma_i^w(0,0)$$ $A_{ij}$ $$A^{\\star} {ij} = \\frac{\\sum_w\\sum {u=0}^{\\tilde{U}}\\sum_{v=0}^{\\tilde{V}}\\xi_{u,v}^{w}\\left(i,j\\right)}{\\sum_w\\sum_{u=0}^{\\tilde{U}}\\sum_{v=0}^{\\tilde{V}}\\sum_{j}^{K}\\xi_{u,v}^{w}\\left(i,j\\right)}$$ final emitting state $\\tilde{U}$ $\\tilde{V}$ $M$ $U-1$ $V-1$ $X$ $U-1$ $V$ $Y$ $U$ $V-1$ $\\phi_{i}(k)$ $$\\phi_i\\left(k\\right) = \\frac{\\sum_w\\sum_{u=0}^{U\\ast1}\\sum_{v=0}^{V\\ast2}\\gamma_i^w\\left(u,v\\right)}{\\sum_w\\sum_{u=0}^U\\sum_{v=0}^V\\gamma_i^w\\left(u,v\\right)}$$ $\\ast1$\uff1a$x_u=k_x$ and state $i$ equals the state $M$ or $X$. (in the state $Y$, a gap is present in observation stream $x$ therefore $k_x$ is not present.) $\\ast2$\uff1a$y_v=k_y$ and state $j$ equals the state $M$ or $Y$. Reference Wahle, Johannes and Armin Buch. \"Alignment and word comparison with Pair Hidden Markov Models.\" (2013). pp.22-31 Martijn B. Wieling. \"Comparison of Dutch Dialects\" (2007) pp.36-50","title":"Alignment"},{"location":"BioInformatics/Alignment/#needleman-wunsch-gotoh","text":"class kerasy . Bio . alignment . NeedlemanWunshGotoh ( ** kwargs ) Global Alignment : Align the entire two sequences. Direction: Forward params: match mismatch gap_opening gap_extension $$ \\begin{aligned} M(i,j) &= \\max \\begin{cases} M(i-1,j-1) + s(x_i, y_j)\\ X(i-1,j-1) - s(x_i, y_j)\\ Y(i-1,j-1) - s(x_i, y_j) \\end{cases}\\ X(i,j) &= \\max \\begin{cases} M(i-1,j) - d\\ X(i-1,j) - d\\ \\end{cases}\\ Y(i,j) &= \\max \\begin{cases} M(i,j-1) - d\\ Y(i,j-1) - d\\ \\end{cases}\\ d &= \\text{gap opening penalty}\\ e &= \\text{gap extension penalty}\\ s(x_i,y_j) &= \\begin{cases} \\text{match score} & (\\text{if $x_i=y_j$})\\ \\text{mismatch score} & (\\text{otherwise.})\\ \\end{cases} \\end{aligned} $$","title":"Needleman-Wunsch-Gotoh"},{"location":"BioInformatics/Alignment/#backward-needleman-wunsh-gotoh","text":"class kerasy . Bio . alignment . BackwardNeedlemanWunshGotoh ( ** kwargs ) Global Alignment : Direction: Backward params: match mismatch gap_opening gap_extension $$ \\begin{aligned} bM(i,j) &= \\max \\begin{cases} bM(i+1,j+1) + s(x_{i+1}, y_{j+1})\\ bX(i+1,j) - d\\ bY(i,j+1) - d \\end{cases}\\ bX(i,j) &= \\max \\begin{cases} bM(i+1,j+1) + s(x_{i+1},y_{j+1})\\ bX(i+1,j) - e \\end{cases}\\ bY(i,j) &= \\max \\begin{cases} bM(i+1,j+1) - s(x_{i+1},y_{j+1})\\ Y(i,j+1) - e\\ \\end{cases}\\ \\end{aligned} $$ This algorithm is for find the maximum alignment under the condition of $x_i$ and $y_j$ are aligned. It is given by: $$A(i,j) = M(i,j) + bM(i,j)$$","title":"Backward Needleman-Wunsh-Gotoh"},{"location":"BioInformatics/Alignment/#smith-waterman","text":"class kerasy . Bio . alignment . SmithWaterman ( ** kwargs ) Local Alignment : Detect the most similar part of the sequences and align them. Direction: Forward params: match mismatch gap_opening gap_extension $$ \\begin{aligned} M(i,j) &= \\max \\begin{cases} 0\\ M(i-1,j-1) + s(x_i, y_j)\\ X(i-1,j-1) - s(x_i, y_j)\\ Y(i-1,j-1) - s(x_i, y_j) \\end{cases}\\ X(i,j) &= \\max \\begin{cases} M(i-1,j) - d\\ X(i-1,j) - d\\ \\end{cases}\\ Y(i,j) &= \\max \\begin{cases} M(i,j-1) - d\\ Y(i,j-1) - d\\ \\end{cases}\\ \\end{aligned} $$","title":"Smith-Waterman"},{"location":"BioInformatics/Alignment/#pair-hmm","text":"class kerasy . Bio . alignment . PairHMM ( ** kwargs ) We can interpret alignment probabilistically. $$ \\begin{aligned} P(\\pi|x,y) &= \\frac{P(x,y,\\pi)}{P(x,y)}\\ P(x,y) &= \\sum_{\\text{alignments }\\pi}P(x,y,\\pi)\\ P(x_i\\diamondsuit y_i|x,y) &= \\frac{\\sum_{\\pi\\in\\Omega_{ij}}P(x,y,\\pi)}{P(x,y)}\\ &= \\frac{F^M(i,j)\\cdot B^M(i,j)}{P(x,y)} \\end{aligned} $$ Probability Explanation $$P(\\pi\\mid x,y)$$ The probability of alignment $\\pi$ is obtained given $x_i$ and $y_j$. $$P(x_i\\diamondsuit y_i\\mid x,y)$$ The probability of $x_i$ and $y_j$ are aligned. Global Alignment Direction: Forward params: delta epsilon tau px_e_y px_ne_y qx qy","title":"Pair HMM"},{"location":"BioInformatics/Alignment/#viterbi-algorithm","text":"$$ \\begin{aligned} V^M(i,j) &= p_{x_iy_j}\\max\\begin{cases}(1-2\\delta-\\tau)V^M(i-1,j-1)\\(1-\\varepsilon-\\tau)V^X(i-1,j-1)\\(1-\\varepsilon-\\tau)V^Y(i-1,j-1)\\end{cases}\\ V^X(i,j) &= q_{x_i}\\max\\begin{cases}\\delta V^M(i-1,j)\\\\varepsilon V^X(i-1,j)\\end{cases}\\ V^Y(i,j) &= q_{y_j}\\max\\begin{cases}\\delta V^M(i,j-1)\\\\varepsilon V^X(i,j-1)\\end{cases}\\ V^E(n,m) &= \\tau\\max\\left(V^M(n,m),V^X(n,m),V^Y(n,m)\\right) \\end{aligned} $$","title":"Viterbi Algorithm"},{"location":"BioInformatics/Alignment/#forward-algorithm","text":"$$ \\begin{aligned} F^M(i,j) &= p_{x_iy_j}\\text{sum}\\begin{cases}(1-2\\delta-\\tau)F^M(i-1,j-1)\\(1-\\varepsilon-\\tau)F^X(i-1,j-1)\\(1-\\varepsilon-\\tau)F^Y(i-1,j-1)\\end{cases}\\ F^X(i,j) &= q_{x_i}\\text{sum}\\begin{cases}\\delta F^M(i-1,j)\\\\varepsilon F^X(i-1,j)\\end{cases}\\ F^Y(i,j) &= q_{y_j}\\text{sum}\\begin{cases}\\delta F^M(i,j-1)\\\\varepsilon F^X(i,j-1)\\end{cases}\\ F^E(n,m) &= \\tau\\ \\text{sum}\\left(F^M(n,m),F^X(n,m),F^Y(n,m)\\right) \\end{aligned} $$","title":"Forward Algorithm"},{"location":"BioInformatics/Alignment/#backward-algorithm","text":"$$ \\begin{aligned} B^M(i,j) &= \\text{sum}\\begin{cases}(1-2\\delta-\\tau)p_{x_iy_j}B^M(i+1,j+1)\\\\delta q_{x_{i+1}}B(i+1,j)\\\\delta q_{y_{j+1}}B^Y(i,j+1)\\end{cases}\\ B^X(i,j) &= (1-\\varepsilon-\\tau)p_{x_{i+1},y_{j+1}}B^M(i+1,j+1) + \\varepsilon q_{x_{i+1}}B^X(i+1,j)\\ B^Y(i,j) &= (1-\\varepsilon-\\tau)p_{x_{i+1},y_{j+1}}B^M(i+1,j+1) + \\varepsilon q_{y_{j+1}}B^Y(i,j+1)\\ \\end{aligned} $$","title":"Backward Algorithm"},{"location":"BioInformatics/Alignment/#baum-welch-algorithm","text":"Although the ordinary Hidden Markov Models calculate the optimal parameters from the perspective of time span $t$, PairHMM focus on the position $u,v$ of each sequence.","title":"Baum-Welch Algorithm"},{"location":"BioInformatics/Alignment/#estep","text":"$$ \\begin{aligned} \\xi_{u,v}\\left(i,j\\right) &=\\frac{f_i\\left(u,v\\right)A_{ij}\\phi_j\\left(\\diamond_1\\right)b_j\\left(\\diamond_2,\\diamond_3\\right)}{\\sum_{k=1}^Kf_k\\left(u,v\\right)b_k\\left(u,v\\right)}\\ \\gamma_i\\left(u,v\\right) &=\\frac{f_i\\left(u,v\\right)b_j\\left(u,v\\right)}{\\sum_{k=1}^Kf_k\\left(u,v\\right)b_k\\left(u,v\\right)} \\end{aligned} $$ variable meaning $f_i\\left(u,v\\right)$ The probability of ending up in state $i$ after aligning the two sequences $\\mathbf{X,Y}$ up to observation $u$ and $v$ respectively. $A_{ij}$ the transition probability from state $i$ to state $j$. $\\phi_j\\left(\\diamond_1\\right)$ the emission probability of emitting $\\diamond_1$ in state $j$. $b_j\\left(\\diamond_2,\\diamond_3\\right)$ the probability of being in state $j$, given the sequences $\\mathbf{X,Y}$ are aligned from observation $\\diamond_2+1$ and $\\diamond_3+1$ to the end of the sequences respectively. \u203b \"$\\diamond_{[1,2,3]}$\" depends on the states ($M,X,Y$) as below: state\\variable $\\phi_j\\left(\\diamond_1\\right)$ $b_j\\left(\\diamond_2,\\diamond_3\\right)$ $M$ $e\\left(x_{u+1},y_{v+1}\\right)$ $b_j\\left(u+1,v+1\\right)$ $X$ $e\\left(x_{u+1}\\right)$ $b_j\\left(u+1,v\\right)$ $Y$ $e\\left(y_{v+1}\\right)$ $b_j\\left(u,v+1\\right)$","title":"Estep"},{"location":"BioInformatics/Alignment/#m-step","text":"","title":"M step"},{"location":"BioInformatics/Alignment/#pi_k","text":"$$\\pi^{\\star}_i = \\sum_w\\gamma_i^w(0,0)$$","title":"$\\pi_k$"},{"location":"BioInformatics/Alignment/#a_ij","text":"$$A^{\\star} {ij} = \\frac{\\sum_w\\sum {u=0}^{\\tilde{U}}\\sum_{v=0}^{\\tilde{V}}\\xi_{u,v}^{w}\\left(i,j\\right)}{\\sum_w\\sum_{u=0}^{\\tilde{U}}\\sum_{v=0}^{\\tilde{V}}\\sum_{j}^{K}\\xi_{u,v}^{w}\\left(i,j\\right)}$$ final emitting state $\\tilde{U}$ $\\tilde{V}$ $M$ $U-1$ $V-1$ $X$ $U-1$ $V$ $Y$ $U$ $V-1$","title":"$A_{ij}$"},{"location":"BioInformatics/Alignment/#phi_ik","text":"$$\\phi_i\\left(k\\right) = \\frac{\\sum_w\\sum_{u=0}^{U\\ast1}\\sum_{v=0}^{V\\ast2}\\gamma_i^w\\left(u,v\\right)}{\\sum_w\\sum_{u=0}^U\\sum_{v=0}^V\\gamma_i^w\\left(u,v\\right)}$$ $\\ast1$\uff1a$x_u=k_x$ and state $i$ equals the state $M$ or $X$. (in the state $Y$, a gap is present in observation stream $x$ therefore $k_x$ is not present.) $\\ast2$\uff1a$y_v=k_y$ and state $j$ equals the state $M$ or $Y$. Reference Wahle, Johannes and Armin Buch. \"Alignment and word comparison with Pair Hidden Markov Models.\" (2013). pp.22-31 Martijn B. Wieling. \"Comparison of Dutch Dialects\" (2007) pp.36-50","title":"$\\phi_{i}(k)$"},{"location":"BioInformatics/Microarray/","text":"1. Design of DNA Miroarrays The idea of DNA microarray technology is to monitor gene expression processes by measuring levels of RNA species in biological samples. There are two main techinques for doing that. [complementary-DNA arrays] RNA molecules in the samples are labeled by using appropriate techniques and presented to an array of spots, where complementary-DNA (cDNA) fragments corresponding to known coding DNA sequences are placed. [oligonucleotide arrays] By a reverse transcription mechanism, copy RNA sequences back to a DNA strand, label the DNA with fluorescent dyes and hybridize it to a complementary-DNA probe fixed on the microarray. After hybridization of the labeled target molecules, estimation of the level of RNA in the sample by reading the intensity of the signals from the dots of the DNA array. The basic difference between the two microarray formats is in the lengths of the complementary-DNA probes. [complementary-DNA arrays] In cDNA arrays, the lengths of the cDNA strands differ between spots, and the different probe lengths change the reaction rates between spots. Therefore, the results must be compared to controls. [oligonucleotide arrays] In oligonucleotide arrays the probes are of constant length in the range of 20\u201350 base pairs, so the readouts of fluorescence intensities are comparable between different spots. 2. Kinetics of the Binding Process The hybridization reaction can be represented by the following scheme $$R + L \\overset{k_f}{\\underset{k_r}{\\rightleftarrows}}C$$ var unit description $R$ numbers of molecules The number of oligonucleotide strands available for reaction. $L$ $[\\text{mol}/L]$ The molar concentration of free target RNA samples. $C$ numbers of molecules. The number of bound complementary complexes. $k_f$ $[\\text{mol}^{-1}\\text{time}^{-1}]$ The forward (binding) reaction rates. $k_r$ $[\\text{time}^{-1}]$ The reverse (unbinding) reaction rates. $V$ $[L]$ The molar volume of the solution interacting with the probe, so we can compute the number of free target RNA molecules in the solution as $LVN_A$ The rate of forward (binding) of target molecules to immobilized probes is proportional to the product of the concentrations of the target strands and free probes . The rate of reverse (unbinding) process is a first-order reaction with a rate proportional to $C$. This results in the following balance of flows: $$\\frac{dC}{dt} = k_fRL - k_rC$$ We assume that at the beginning, i.e., at $t=0$, there are no hybridization complexes, and total number of oligonucleotides are available for hybridization. $$C(0)=0, L(0)=L_0, R(0)=R_T$$ Since one RNA strand binds to one oligonucleotide, resulting in one binding coplex, the following equalities for the flows hold $$\\frac{dR}{dt} = VN_A\\frac{dL}{dt} = -\\frac{dC}{dt},$$ which results in $R(t)+C(t)=R_T$ and $VN_AL(t)+C(t)=VN_AL_0$. So $L(t)$ and $R(t)$ can be expressed in terms of $C(t)$, and get the following equation: $$ \\begin{aligned} \\frac{dC}{dt} &= k_fRL - k_rC \\ &= k_f\\left[R_T-C(t)\\right]\\left[L_0 - \\frac{C(t)}{VN_A}\\right] - k_rC(t)\\ \\end{aligned} $$ Often it is possible to approximate the above dynamics to only one exponent. Specifically, one of the following two asymptotic situations may hold. [Often] There is a large excess of particles in the immobile probe over the potential number of binding targets, i.e., $R_T\\gg C$, or $(R_T \u2212 C)/RT \\approx 1$, which results in $$ \\begin{aligned} \\frac{dC}{dt} &= k_f\\left[R_T-C(t)\\right]\\left[L_0 - \\frac{C(t)}{VN_A}\\right] - k_rC(t)\\ &= k_f\\left[\\underbrace{\\frac{R_T-C(t)}{R_T}}_{\\approx 1}\\cdot R_T\\right]\\left[L_0 - \\frac{C(t)}{VN_A}\\right] - k_rC(t)\\ &\\approx k_fR_TL_0 - \\left[\\frac{k_fR_T}{VN_A} + k_r\\right]C(t)\\ \\therefore C(t) &= \\frac{R_TL_0}{K_D+R_T/N_AV}\\left[1-\\exp\\left(-\\frac{t}{\\tau_R}\\right)\\right]\\ & K_D=k_r/k_f, \\quad \\tau_R = \\frac{1}{k_f\\left(K_D + R_T/N_AV\\right)} \\end{aligned} $$ There is a large excess of free RNA strands with respect to the number of binding complexes, i.e., $L_0 \\gg C/N_AV$, or $(L_0 \u2212 C/N_AV )/L_0 \\approx 1$, which leads to $$ \\begin{aligned} C(t) &= \\frac{R_TL_0}{K_D+L_0}\\left[1-\\exp\\left(-\\frac{t}{\\tau_T}\\right)\\right]\\ \\tau_L &= \\frac{1}{k_f\\left(K_D+L_0\\right)} \\end{aligned} $$ Microarray experiments are very often planned in such a way that there is a large excess of particles in the immobile probe over the potential number of binding targets, i.e., case $1$ holds. From the equation $$ \\begin{aligned} C(t) &= \\frac{R_TL_0}{K_D+R_T/N_AV}\\left[1-\\exp\\left(-\\frac{t}{\\tau_R}\\right)\\right]\\ &\\underset{t\\rightarrow\\infty}{\\Rightarrow}\\frac{R_TL_0}{K_D+R_T/N_AV}, \\end{aligned} $$ one can see that, after equilibrium has been reached , or at a predefined instant of time , the intensity of the fluorescence signal measured at a microarray spot is proportional to the level of the corresponding RNA species in the analyzed sample, i.e., $C \\sim L_0$ 3. Data Preprocessing and Normalization","title":"Microarray"},{"location":"BioInformatics/Microarray/#1-design-of-dna-miroarrays","text":"The idea of DNA microarray technology is to monitor gene expression processes by measuring levels of RNA species in biological samples. There are two main techinques for doing that. [complementary-DNA arrays] RNA molecules in the samples are labeled by using appropriate techniques and presented to an array of spots, where complementary-DNA (cDNA) fragments corresponding to known coding DNA sequences are placed. [oligonucleotide arrays] By a reverse transcription mechanism, copy RNA sequences back to a DNA strand, label the DNA with fluorescent dyes and hybridize it to a complementary-DNA probe fixed on the microarray. After hybridization of the labeled target molecules, estimation of the level of RNA in the sample by reading the intensity of the signals from the dots of the DNA array. The basic difference between the two microarray formats is in the lengths of the complementary-DNA probes. [complementary-DNA arrays] In cDNA arrays, the lengths of the cDNA strands differ between spots, and the different probe lengths change the reaction rates between spots. Therefore, the results must be compared to controls. [oligonucleotide arrays] In oligonucleotide arrays the probes are of constant length in the range of 20\u201350 base pairs, so the readouts of fluorescence intensities are comparable between different spots.","title":"1. Design of DNA Miroarrays"},{"location":"BioInformatics/Microarray/#2-kinetics-of-the-binding-process","text":"The hybridization reaction can be represented by the following scheme $$R + L \\overset{k_f}{\\underset{k_r}{\\rightleftarrows}}C$$ var unit description $R$ numbers of molecules The number of oligonucleotide strands available for reaction. $L$ $[\\text{mol}/L]$ The molar concentration of free target RNA samples. $C$ numbers of molecules. The number of bound complementary complexes. $k_f$ $[\\text{mol}^{-1}\\text{time}^{-1}]$ The forward (binding) reaction rates. $k_r$ $[\\text{time}^{-1}]$ The reverse (unbinding) reaction rates. $V$ $[L]$ The molar volume of the solution interacting with the probe, so we can compute the number of free target RNA molecules in the solution as $LVN_A$ The rate of forward (binding) of target molecules to immobilized probes is proportional to the product of the concentrations of the target strands and free probes . The rate of reverse (unbinding) process is a first-order reaction with a rate proportional to $C$. This results in the following balance of flows: $$\\frac{dC}{dt} = k_fRL - k_rC$$ We assume that at the beginning, i.e., at $t=0$, there are no hybridization complexes, and total number of oligonucleotides are available for hybridization. $$C(0)=0, L(0)=L_0, R(0)=R_T$$ Since one RNA strand binds to one oligonucleotide, resulting in one binding coplex, the following equalities for the flows hold $$\\frac{dR}{dt} = VN_A\\frac{dL}{dt} = -\\frac{dC}{dt},$$ which results in $R(t)+C(t)=R_T$ and $VN_AL(t)+C(t)=VN_AL_0$. So $L(t)$ and $R(t)$ can be expressed in terms of $C(t)$, and get the following equation: $$ \\begin{aligned} \\frac{dC}{dt} &= k_fRL - k_rC \\ &= k_f\\left[R_T-C(t)\\right]\\left[L_0 - \\frac{C(t)}{VN_A}\\right] - k_rC(t)\\ \\end{aligned} $$ Often it is possible to approximate the above dynamics to only one exponent. Specifically, one of the following two asymptotic situations may hold. [Often] There is a large excess of particles in the immobile probe over the potential number of binding targets, i.e., $R_T\\gg C$, or $(R_T \u2212 C)/RT \\approx 1$, which results in $$ \\begin{aligned} \\frac{dC}{dt} &= k_f\\left[R_T-C(t)\\right]\\left[L_0 - \\frac{C(t)}{VN_A}\\right] - k_rC(t)\\ &= k_f\\left[\\underbrace{\\frac{R_T-C(t)}{R_T}}_{\\approx 1}\\cdot R_T\\right]\\left[L_0 - \\frac{C(t)}{VN_A}\\right] - k_rC(t)\\ &\\approx k_fR_TL_0 - \\left[\\frac{k_fR_T}{VN_A} + k_r\\right]C(t)\\ \\therefore C(t) &= \\frac{R_TL_0}{K_D+R_T/N_AV}\\left[1-\\exp\\left(-\\frac{t}{\\tau_R}\\right)\\right]\\ & K_D=k_r/k_f, \\quad \\tau_R = \\frac{1}{k_f\\left(K_D + R_T/N_AV\\right)} \\end{aligned} $$ There is a large excess of free RNA strands with respect to the number of binding complexes, i.e., $L_0 \\gg C/N_AV$, or $(L_0 \u2212 C/N_AV )/L_0 \\approx 1$, which leads to $$ \\begin{aligned} C(t) &= \\frac{R_TL_0}{K_D+L_0}\\left[1-\\exp\\left(-\\frac{t}{\\tau_T}\\right)\\right]\\ \\tau_L &= \\frac{1}{k_f\\left(K_D+L_0\\right)} \\end{aligned} $$ Microarray experiments are very often planned in such a way that there is a large excess of particles in the immobile probe over the potential number of binding targets, i.e., case $1$ holds. From the equation $$ \\begin{aligned} C(t) &= \\frac{R_TL_0}{K_D+R_T/N_AV}\\left[1-\\exp\\left(-\\frac{t}{\\tau_R}\\right)\\right]\\ &\\underset{t\\rightarrow\\infty}{\\Rightarrow}\\frac{R_TL_0}{K_D+R_T/N_AV}, \\end{aligned} $$ one can see that, after equilibrium has been reached , or at a predefined instant of time , the intensity of the fluorescence signal measured at a microarray spot is proportional to the level of the corresponding RNA species in the analyzed sample, i.e., $C \\sim L_0$","title":"2. Kinetics of the Binding Process"},{"location":"BioInformatics/Microarray/#3-data-preprocessing-and-normalization","text":"","title":"3. Data Preprocessing and Normalization"},{"location":"BioInformatics/RefSeq/","text":"RefSeq In kerasy, we can use kerasy.datasets.ncbi.getSeq method to collect sequence data from Reference Sequence (RefSeq) database , which is an open access , annotated and curated collection of publicly available nucleotide sequences (DNA, RNA) and their protein products. This database is built by NCBI(National Center for Biotechnology Information) , and, unlike GenBank , which is also build by it provides only a single record for each natural biological molecule (i.e. DNA, RNA or protein) for major organisms ranging from viruses to bacteria to eukaryotes. def kerasy . dataset . ncbi . getSeq ( refSeq_num , asfasta = False , path = \"\" ) RefSeq Number Accession prefix Description NC Complete genomic molecules NG Incomplete genomic region NM mRNA NR ncRNA NP Protein XM predicted mRNA model XR predicted ncRNA model XP predicted Protein model (eukaryotic sequences) WP predicted Protein model (prokaryotic sequences) Ref: Table 7. [Entrez queries to retrieve sets of RefSeq records.]. - The NCBI Handbook - NCBI Bookshelf FAQ If the method doesn't work, please look at Genomes Download FAQ","title":"RefSeq"},{"location":"BioInformatics/RefSeq/#refseq","text":"In kerasy, we can use kerasy.datasets.ncbi.getSeq method to collect sequence data from Reference Sequence (RefSeq) database , which is an open access , annotated and curated collection of publicly available nucleotide sequences (DNA, RNA) and their protein products. This database is built by NCBI(National Center for Biotechnology Information) , and, unlike GenBank , which is also build by it provides only a single record for each natural biological molecule (i.e. DNA, RNA or protein) for major organisms ranging from viruses to bacteria to eukaryotes. def kerasy . dataset . ncbi . getSeq ( refSeq_num , asfasta = False , path = \"\" )","title":"RefSeq"},{"location":"BioInformatics/RefSeq/#refseq-number","text":"Accession prefix Description NC Complete genomic molecules NG Incomplete genomic region NM mRNA NR ncRNA NP Protein XM predicted mRNA model XR predicted ncRNA model XP predicted Protein model (eukaryotic sequences) WP predicted Protein model (prokaryotic sequences) Ref: Table 7. [Entrez queries to retrieve sets of RefSeq records.]. - The NCBI Handbook - NCBI Bookshelf","title":"RefSeq Number"},{"location":"BioInformatics/RefSeq/#faq","text":"If the method doesn't work, please look at Genomes Download FAQ","title":"FAQ"},{"location":"BioInformatics/Secondary%20Structure/","text":"Notebook Example Notebook: Kerasy.examples.structure.ipynb Free energy minimization of RNA secondary structure In cells, RNAs are likely to form energetically stable secondary structure, so the correctness of secondary structure should be evaluated based on the free energy. Nussinov Algorithm Nussinov Algorithm calculate the free energy by the number of base-pairs. class kerasy . Bio . structure . Nussinov ( ** kwargs ) params: nucleic_acid WatsonCrick minspan variable definition $$\\gamma\\left(i,j\\right)$$ the maximum number of base-pairs for subsequence from $i$ to $j$. $$\\omega\\left(i,j\\right)$$ the maximum number of base-pairs excluding subsequence from $i$ to $j$. $$Z(i,j)$$ the maximum number of base-pairs if $x_i$ and $y_j$ form base pair. $$ \\begin{aligned} \\gamma(i,j) &= \\max \\begin{cases} \\gamma(i+1,j)\\ \\gamma(i,j-1)\\ \\gamma(i+1,j-1)+\\delta(i,j)\\ \\max_{i\\leq k\\verb|<|j}\\left[\\gamma(i,k) + \\gamma(k+1,j)\\right] \\end{cases}\\ \\omega(i,j) &= \\max \\begin{cases} \\omega(i-1,j)\\ \\omega(i,j+1)\\ \\omega(i-1,j+1)+\\delta(i-1,j+1)\\ \\max_{1\\leq k\\verb|<|i}\\left[\\omega(k,j) + \\gamma(k,i-1)\\right]\\ \\max_{j\\verb|<| k\\leq L}\\left[\\omega(j+1,k) + \\gamma(i,k)\\right] \\end{cases}\\ Z(i,j) &= \\begin{cases} \\gamma\\left(i+1,j-1\\right) + 1 + \\omega\\left(i,j\\right) & (\\text{if }i\\text{ and }j\\text{-th nucleotides can form a base-pair})\\ 0 & (\\text{otherwise}) \\end{cases} \\end{aligned} $$ Zuker Algorithm class kerasy . Bio . structure . Zuker ( ** kwargs ) params: nucleic_acid WatsonCrick hairpin internal buldge a b c stacking_cols stacking_score The free energy of a secondary structure is approximated as the sum of the free energy of \"loops\". $$E = \\sum_iE_i$$ The free energy of individual loop is given by experimental data. (ex. $\\mathrm{C-G: }-3.4\\mathrm{kcal/mol}$, $\\mathrm{U-A: }-0.9\\mathrm{kcal/mol}$) Five types of \"loops\" hairpin loop stacking bulge loop internal loop multi-loop F1(i,j) F2(i,j,h,l) Fm=a+bk+cu a,b,c: constant k: the number of base-pairs in a multi-loop u: the number of single stranded nucleotides in a multi-loop variable meaning $W(i,j)$ the minimum free energy of subsequence from $i$ to $j$. $V(i,j)$ the minimum free energy of subsequence from $i$ to $j$ when $i$ to $j$ forms a base-pair. $M(i,j)$ the minimum free energy of subsequence when subsequence from $i$ to $j$ are in the multi-loop and contain one or more base pairs which close it. $$ \\begin{aligned} W(i,j) &= \\min \\begin{cases} W(i+1,j)\\W(i,j-1)\\V(i,j)\\\\min_{i\\leq k<j}\\left{W(i,k) + W(k+1,j)\\right} \\end{cases}\\ V(i,j) &= \\min \\begin{cases} F_1(i,j)\\\\min_{i<h<l<j}F_2(i,j,h,l) + V(h,l)\\\\min_{i+1\\leq k<j-1}\\left[M(i+1,k) + M(k+1,j)\\right]+a+b \\end{cases}\\ M(i,j) &= \\min \\begin{cases} V(i,j)+b\\M(i+1,j)+c\\M(i,j-1)+c\\\\min_{i\\leq<k<j}\\left[M(i,k) + M(k+1,j)\\right] \\end{cases}\\ \\end{aligned} $$ Warning None of the algorithms we see this week can deal with pseudoknot","title":"Secondary Structure"},{"location":"BioInformatics/Secondary%20Structure/#nussinov-algorithm","text":"Nussinov Algorithm calculate the free energy by the number of base-pairs. class kerasy . Bio . structure . Nussinov ( ** kwargs ) params: nucleic_acid WatsonCrick minspan variable definition $$\\gamma\\left(i,j\\right)$$ the maximum number of base-pairs for subsequence from $i$ to $j$. $$\\omega\\left(i,j\\right)$$ the maximum number of base-pairs excluding subsequence from $i$ to $j$. $$Z(i,j)$$ the maximum number of base-pairs if $x_i$ and $y_j$ form base pair. $$ \\begin{aligned} \\gamma(i,j) &= \\max \\begin{cases} \\gamma(i+1,j)\\ \\gamma(i,j-1)\\ \\gamma(i+1,j-1)+\\delta(i,j)\\ \\max_{i\\leq k\\verb|<|j}\\left[\\gamma(i,k) + \\gamma(k+1,j)\\right] \\end{cases}\\ \\omega(i,j) &= \\max \\begin{cases} \\omega(i-1,j)\\ \\omega(i,j+1)\\ \\omega(i-1,j+1)+\\delta(i-1,j+1)\\ \\max_{1\\leq k\\verb|<|i}\\left[\\omega(k,j) + \\gamma(k,i-1)\\right]\\ \\max_{j\\verb|<| k\\leq L}\\left[\\omega(j+1,k) + \\gamma(i,k)\\right] \\end{cases}\\ Z(i,j) &= \\begin{cases} \\gamma\\left(i+1,j-1\\right) + 1 + \\omega\\left(i,j\\right) & (\\text{if }i\\text{ and }j\\text{-th nucleotides can form a base-pair})\\ 0 & (\\text{otherwise}) \\end{cases} \\end{aligned} $$","title":"Nussinov Algorithm"},{"location":"BioInformatics/Secondary%20Structure/#zuker-algorithm","text":"class kerasy . Bio . structure . Zuker ( ** kwargs ) params: nucleic_acid WatsonCrick hairpin internal buldge a b c stacking_cols stacking_score The free energy of a secondary structure is approximated as the sum of the free energy of \"loops\". $$E = \\sum_iE_i$$ The free energy of individual loop is given by experimental data. (ex. $\\mathrm{C-G: }-3.4\\mathrm{kcal/mol}$, $\\mathrm{U-A: }-0.9\\mathrm{kcal/mol}$)","title":"Zuker Algorithm"},{"location":"BioInformatics/Secondary%20Structure/#five-types-of-loops","text":"hairpin loop stacking bulge loop internal loop multi-loop F1(i,j) F2(i,j,h,l) Fm=a+bk+cu a,b,c: constant k: the number of base-pairs in a multi-loop u: the number of single stranded nucleotides in a multi-loop variable meaning $W(i,j)$ the minimum free energy of subsequence from $i$ to $j$. $V(i,j)$ the minimum free energy of subsequence from $i$ to $j$ when $i$ to $j$ forms a base-pair. $M(i,j)$ the minimum free energy of subsequence when subsequence from $i$ to $j$ are in the multi-loop and contain one or more base pairs which close it. $$ \\begin{aligned} W(i,j) &= \\min \\begin{cases} W(i+1,j)\\W(i,j-1)\\V(i,j)\\\\min_{i\\leq k<j}\\left{W(i,k) + W(k+1,j)\\right} \\end{cases}\\ V(i,j) &= \\min \\begin{cases} F_1(i,j)\\\\min_{i<h<l<j}F_2(i,j,h,l) + V(h,l)\\\\min_{i+1\\leq k<j-1}\\left[M(i+1,k) + M(k+1,j)\\right]+a+b \\end{cases}\\ M(i,j) &= \\min \\begin{cases} V(i,j)+b\\M(i+1,j)+c\\M(i,j-1)+c\\\\min_{i\\leq<k<j}\\left[M(i,k) + M(k+1,j)\\right] \\end{cases}\\ \\end{aligned} $$ Warning None of the algorithms we see this week can deal with pseudoknot","title":"Five types of \"loops\""},{"location":"BioInformatics/String%20Search/","text":"Notebook Example Notebook: Kerasy.examples.string.ipynb Suffix Array def kerasy . Bio . string . SAIS ( string ) Suffix array is a sorted array of all suffixes of a string, and can be built in $\\mathrm{O}(n)$. This procedure is as following: Define the each character S/L-type. Find the LMS(Left Most S-type) Fill in $-1$ to initialize Suffix Array. Prepare the empty buckets whose sizes are correspond to the Initial character grouping. Sort the LMS Scan SA in forward order and fill L-type suffixes. Scan SA in backward order and fill S-type suffixes. Step1. Definition of S/L-type If $i$-th suffix is [smaller, bigger] then $i+1$-th suffix in lexicographical order, $i$-th character is [S,L]-type. Exceptionally, \"$\" is the initial character and defined S-type. In the above definition, obviously the following holds: $i$-th suffix is S-tyep when $S[i]$ = \"$\" $S[i] < S[i+1]$ $S[i] == S[i+1]$ and $i+1$-th suffix is S-type . $i$-th suffix is L-tyep when $S[i] > S[i+1]$ $S[i] == S[i+1]$ and $i+1$-th suffix is L-type . $i=5$ $i=4$ $i=3$ Step2. Definition of LMS If $S[i]$ is the S-type character and $S[i-1]$ is the L-type, $S[i]$ is called LMS(Left Most S-type) . Step3. Initialize Suffix Array. Step4. Prepare the empty buckets Step5. Sort the LMS \u203b This sorting is not easy, but ignore for consistency of explanation. (Touch again later.) Induced Sorting We induce the order from the already known suffix order. Step6. Induced sorting for L-type suffixes Scan SA in forward order and fill L-type suffixes. If $S[\\text{Focusing Number}-1]$ is L-type, put $\\text{Focusing Number}-1$ into the corresponding bucket from the smaller. This is justified by Same bucket (Same initial) L-tyep (Smaller than S-type) One right is the smallest of the rest. Step7. Induced sorting for S-type suffixes Scan SA in backward order and fill S-type suffixes. At this time, we have to reorder the LMS. Finally, we get How to Sort LMS suffix?? (Step5) We could sort entire SA if LMS is correctly sorted. Then How to sort LMS?? The answer of this question is \"Sort without Sorting\" . Recursion of SA-IS Step4' Step5' At this time, we couldn't know the order of LMS, so we \"equality\" instead of \"inequality\" . Step6' Scan SA in forward order and fill L-type suffixes. At this moment, equality relationship is also passed . Step7' Scan SA in backward order and fill S-type suffixes. Encode LMS-substrings We induce the order from the already known suffix order. Step5 (Recursion) Step6 (Recursion) Step7 (Recursion) We could sort LMS correctly!! Computational complexity In the each recursion step, it is guaranteed that the length of encoded strings is less than half of the previous length. $$\\mathrm{O}(n) + \\mathrm{O}(n/2) + \\mathrm{O}(n/4) + \\cdots = \\mathrm{O}(2n)$$ Reference Two Efficient Algorithms for Linear Time Suffix Array Construction Burrows-Wheeler Transform; BWT def kerasy . Bio . string . mkBWT ( string , SA ) Explanation ex. S=TATAA$ BWT(Burrows-Wheeler Transform) is the \"characters just to the left of the suffixes in the suffix array\", and it can be used to recover the entire string. It is given by$$BWT[i] = \\begin{cases}S[SA[i] - 1] & (\\text{if $SA[i]>0$})\\\\mathrm{\\$} & (\\text{otherwise})\\end{cases}$$ BWT is useful for compression , since it tends to rearrange a character string into runs of similar characters. More importantly, this transformation is reversible, without needing to store any additional data . The BWT is thus a \"free\" method of improving the efficiency of text compression algorithms, costing only some extra computation. Reverse BWT with Suffix Array We will denote $T$ and $ISA$ as followings. $$ \\begin{cases} SA[k] = v &\\Longleftrightarrow ISA[v] = k\\ \\mathrm{T}[i] &\\Longleftrightarrow \\mathrm{ISA}\\left[\\mathrm{SA}[i] - 1\\right]\\ \\end{cases} $$ This means that the order of suffix whose initial character is $BWT[i]$ is $T[i]$ , and under the above definition, the following equation holds. $$ \\begin{aligned} \\mathrm{BWT}\\left[\\mathrm{T}\\left[i\\right]\\right] &= \\mathrm{BWT}\\left[\\mathrm{ISA}\\left[\\mathrm{SA}\\left[i\\right] - 1\\right]\\right]\\ &= \\mathrm{BWT}\\left[k\\ |\\ \\mathrm{SA}[k] = \\mathrm{SA}[i] - 1\\right]\\ &= \\mathrm{S}\\left[SA[k] - 1\\ |\\ \\mathrm{SA}[k] = \\mathrm{SA}[i] - 1\\right]\\ &= \\mathrm{S}\\left[\\left(SA[i] - 1\\right) - 1\\right]\\ &= \\text{$BWT[i]$'s one left character in $S$} \\end{aligned} $$ Now that we can get the original $S$ from BWT and SA. Reverse BWT without Suffix Array In fact, we could recover the original strings only from BWT, because we can make $T$ only from BWT. One of the important properties to realize it is LF Mapping . LF Mapping The i-th occurrence of a character x in L and the i-th occurrence of x in F correspond to the same occurrence. To calculate $T[i]$ only from BWT, we will introduce auxiliary storages $C[x]$, and $Occ(x,i)$ Name Explanation $C[x]$ the total occurrence of a characters which is smaller than $x$ in lexicographical order in BWT. $Occ(x,i)$ the total occurrence of a character $x$ up to $i$-th in the BWT. Using these variable, we can calculate $T[i]$ by $$T[i] = \\begin{cases} 0 &(\\mathrm{BWT}[i] = \\text{\\$})\\ C\\left[\\mathrm{BWT}[i]\\right] + Occ(\\mathrm{BWT}[i], i) - 1 & (\\text{otherwise.})\\ \\end{cases} $$ The above formula is guaranteed by the following properties. $$ \\begin{cases} [lb(W),ub(W)] &= \\text{the index range of words in SA whose prefix is W}\\ lb({}) &= 0\\ ub({}) &= len(S) - 1\\ Occ(x, -1) &= 0\\ lb(xW) &= C(x) + Occ(x, lb(W) - 1) &\\cdots (1)\\ ub(xW) &= C(x) + Occ(x, ub(W)) - 1 &\\cdots (2) \\end{cases} $$ These properties (especially the recursion (1) and (2)) are proved by $(1)$ $(2)$ This allows us to search the occurrence range of characters with a specific prefix. Reference Burrows\u2013Wheeler transform and palindromic richness Longest Common Prefix; LCP def kerasy . Bio . string . mkHeight ( string , SA ) LCP(Longest Common Prefix) is \"how many characters two strings have in common with each other\", and LCP array is an array where each adjacent index stores lcp between $i$-th suffix and $i+1$-th suffix. Speaking of LCParray , it usually refers to the suffix array, Reference Linear-Time Longest-Common-Prefix Computation in Suffix Arrays and Its Applications ( If you read this paper, please look this . It will help to read. )","title":"String Search"},{"location":"BioInformatics/String%20Search/#suffix-array","text":"def kerasy . Bio . string . SAIS ( string ) Suffix array is a sorted array of all suffixes of a string, and can be built in $\\mathrm{O}(n)$. This procedure is as following: Define the each character S/L-type. Find the LMS(Left Most S-type) Fill in $-1$ to initialize Suffix Array. Prepare the empty buckets whose sizes are correspond to the Initial character grouping. Sort the LMS Scan SA in forward order and fill L-type suffixes. Scan SA in backward order and fill S-type suffixes.","title":"Suffix Array"},{"location":"BioInformatics/String%20Search/#step1-definition-of-sl-type","text":"If $i$-th suffix is [smaller, bigger] then $i+1$-th suffix in lexicographical order, $i$-th character is [S,L]-type. Exceptionally, \"$\" is the initial character and defined S-type. In the above definition, obviously the following holds: $i$-th suffix is S-tyep when $S[i]$ = \"$\" $S[i] < S[i+1]$ $S[i] == S[i+1]$ and $i+1$-th suffix is S-type . $i$-th suffix is L-tyep when $S[i] > S[i+1]$ $S[i] == S[i+1]$ and $i+1$-th suffix is L-type . $i=5$ $i=4$ $i=3$","title":"Step1. Definition of S/L-type"},{"location":"BioInformatics/String%20Search/#step2-definition-of-lms","text":"If $S[i]$ is the S-type character and $S[i-1]$ is the L-type, $S[i]$ is called LMS(Left Most S-type) .","title":"Step2. Definition of LMS"},{"location":"BioInformatics/String%20Search/#step3-initialize-suffix-array","text":"","title":"Step3. Initialize Suffix Array."},{"location":"BioInformatics/String%20Search/#step4-prepare-the-empty-buckets","text":"","title":"Step4. Prepare the empty buckets"},{"location":"BioInformatics/String%20Search/#step5-sort-the-lms","text":"\u203b This sorting is not easy, but ignore for consistency of explanation. (Touch again later.) Induced Sorting We induce the order from the already known suffix order.","title":"Step5. Sort the LMS"},{"location":"BioInformatics/String%20Search/#step6-induced-sorting-for-l-type-suffixes","text":"Scan SA in forward order and fill L-type suffixes. If $S[\\text{Focusing Number}-1]$ is L-type, put $\\text{Focusing Number}-1$ into the corresponding bucket from the smaller. This is justified by Same bucket (Same initial) L-tyep (Smaller than S-type) One right is the smallest of the rest.","title":"Step6. Induced sorting for L-type suffixes"},{"location":"BioInformatics/String%20Search/#step7-induced-sorting-for-s-type-suffixes","text":"Scan SA in backward order and fill S-type suffixes. At this time, we have to reorder the LMS. Finally, we get How to Sort LMS suffix?? (Step5) We could sort entire SA if LMS is correctly sorted. Then How to sort LMS?? The answer of this question is \"Sort without Sorting\" .","title":"Step7. Induced sorting for S-type suffixes"},{"location":"BioInformatics/String%20Search/#recursion-of-sa-is","text":"","title":"Recursion of SA-IS"},{"location":"BioInformatics/String%20Search/#step4","text":"","title":"Step4'"},{"location":"BioInformatics/String%20Search/#step5","text":"At this time, we couldn't know the order of LMS, so we \"equality\" instead of \"inequality\" .","title":"Step5'"},{"location":"BioInformatics/String%20Search/#step6","text":"Scan SA in forward order and fill L-type suffixes. At this moment, equality relationship is also passed .","title":"Step6'"},{"location":"BioInformatics/String%20Search/#step7","text":"Scan SA in backward order and fill S-type suffixes. Encode LMS-substrings We induce the order from the already known suffix order.","title":"Step7'"},{"location":"BioInformatics/String%20Search/#step5-recursion","text":"","title":"Step5 (Recursion)"},{"location":"BioInformatics/String%20Search/#step6-recursion","text":"","title":"Step6 (Recursion)"},{"location":"BioInformatics/String%20Search/#step7-recursion","text":"We could sort LMS correctly!!","title":"Step7 (Recursion)"},{"location":"BioInformatics/String%20Search/#computational-complexity","text":"In the each recursion step, it is guaranteed that the length of encoded strings is less than half of the previous length. $$\\mathrm{O}(n) + \\mathrm{O}(n/2) + \\mathrm{O}(n/4) + \\cdots = \\mathrm{O}(2n)$$ Reference Two Efficient Algorithms for Linear Time Suffix Array Construction","title":"Computational complexity"},{"location":"BioInformatics/String%20Search/#burrows-wheeler-transform-bwt","text":"def kerasy . Bio . string . mkBWT ( string , SA ) Explanation ex. S=TATAA$ BWT(Burrows-Wheeler Transform) is the \"characters just to the left of the suffixes in the suffix array\", and it can be used to recover the entire string. It is given by$$BWT[i] = \\begin{cases}S[SA[i] - 1] & (\\text{if $SA[i]>0$})\\\\mathrm{\\$} & (\\text{otherwise})\\end{cases}$$ BWT is useful for compression , since it tends to rearrange a character string into runs of similar characters. More importantly, this transformation is reversible, without needing to store any additional data . The BWT is thus a \"free\" method of improving the efficiency of text compression algorithms, costing only some extra computation.","title":"Burrows-Wheeler Transform; BWT"},{"location":"BioInformatics/String%20Search/#reverse-bwt-with-suffix-array","text":"We will denote $T$ and $ISA$ as followings. $$ \\begin{cases} SA[k] = v &\\Longleftrightarrow ISA[v] = k\\ \\mathrm{T}[i] &\\Longleftrightarrow \\mathrm{ISA}\\left[\\mathrm{SA}[i] - 1\\right]\\ \\end{cases} $$ This means that the order of suffix whose initial character is $BWT[i]$ is $T[i]$ , and under the above definition, the following equation holds. $$ \\begin{aligned} \\mathrm{BWT}\\left[\\mathrm{T}\\left[i\\right]\\right] &= \\mathrm{BWT}\\left[\\mathrm{ISA}\\left[\\mathrm{SA}\\left[i\\right] - 1\\right]\\right]\\ &= \\mathrm{BWT}\\left[k\\ |\\ \\mathrm{SA}[k] = \\mathrm{SA}[i] - 1\\right]\\ &= \\mathrm{S}\\left[SA[k] - 1\\ |\\ \\mathrm{SA}[k] = \\mathrm{SA}[i] - 1\\right]\\ &= \\mathrm{S}\\left[\\left(SA[i] - 1\\right) - 1\\right]\\ &= \\text{$BWT[i]$'s one left character in $S$} \\end{aligned} $$ Now that we can get the original $S$ from BWT and SA.","title":"Reverse BWT with Suffix Array"},{"location":"BioInformatics/String%20Search/#reverse-bwt-without-suffix-array","text":"In fact, we could recover the original strings only from BWT, because we can make $T$ only from BWT. One of the important properties to realize it is LF Mapping . LF Mapping The i-th occurrence of a character x in L and the i-th occurrence of x in F correspond to the same occurrence. To calculate $T[i]$ only from BWT, we will introduce auxiliary storages $C[x]$, and $Occ(x,i)$ Name Explanation $C[x]$ the total occurrence of a characters which is smaller than $x$ in lexicographical order in BWT. $Occ(x,i)$ the total occurrence of a character $x$ up to $i$-th in the BWT. Using these variable, we can calculate $T[i]$ by $$T[i] = \\begin{cases} 0 &(\\mathrm{BWT}[i] = \\text{\\$})\\ C\\left[\\mathrm{BWT}[i]\\right] + Occ(\\mathrm{BWT}[i], i) - 1 & (\\text{otherwise.})\\ \\end{cases} $$ The above formula is guaranteed by the following properties. $$ \\begin{cases} [lb(W),ub(W)] &= \\text{the index range of words in SA whose prefix is W}\\ lb({}) &= 0\\ ub({}) &= len(S) - 1\\ Occ(x, -1) &= 0\\ lb(xW) &= C(x) + Occ(x, lb(W) - 1) &\\cdots (1)\\ ub(xW) &= C(x) + Occ(x, ub(W)) - 1 &\\cdots (2) \\end{cases} $$ These properties (especially the recursion (1) and (2)) are proved by $(1)$ $(2)$ This allows us to search the occurrence range of characters with a specific prefix. Reference Burrows\u2013Wheeler transform and palindromic richness","title":"Reverse BWT without Suffix Array"},{"location":"BioInformatics/String%20Search/#longest-common-prefix-lcp","text":"def kerasy . Bio . string . mkHeight ( string , SA ) LCP(Longest Common Prefix) is \"how many characters two strings have in common with each other\", and LCP array is an array where each adjacent index stores lcp between $i$-th suffix and $i+1$-th suffix. Speaking of LCParray , it usually refers to the suffix array, Reference Linear-Time Longest-Common-Prefix Computation in Suffix Arrays and Its Applications ( If you read this paper, please look this . It will help to read. )","title":"Longest Common Prefix; LCP"},{"location":"BioInformatics/Tandem%20Repeats/","text":"Notebook Example Notebook: Kerasy.examples.tandem.ipynb Tandem Repeats Tandem Repeats are simple sequence repeats that are extremely common throughout the genomes of a wide range of species. Tandem repeats are helpful to determine the inherited traits of gene from its parent and in determining the ancestor relationship. Tandem repeats are proven to be biologically significant as they can help in discovery of dynamic mutations for genetic diseases. There are two main methods like dynamic programming , suffix array induced sorting , to find exact tandem repeats. # Suffix Array - Induced Sorting Dynamic Programming Construction $$O\\left(n\\right)$$ $$O\\left(n^2\\right)$$ Tandem repeats discovery $$O\\left(n\\ast m\\right)$$ $$O\\left(n^2\\right)$$ Total time $$O\\left(n + n\\ast m\\right)$$ $$O\\left(n^2\\right)$$ Space complexity $$O\\left(n\\right)$$ $$O\\left(n^2\\right)$$ here, $n$ is the length of the sequence and $m$ is the average length of the suffix. The time required to identify tandem repeats mainly depends on construction of the required data structure. For Dynamic programming method, a square matrix of size of the sequence is constructed and values are calculated which takes the time and space complexity of $O\\left(n^2\\right)$. In SA-IS method, the time and space needed for construction is $O\\left(n\\right)$. Dynamic Programming Method class kerasy . Bio . tandem . find ( method = \"DP\" ) To find all tandem repeats in a sequence we perform slight modifications to the Smith-Waterman algorithm for local alignments. The time complexity of the method will be $O\\left(n\\ast n\\right)$ as we will be filling a two-dimensional matrix. We then align the sequence with itself to find tandem repeats. The modifications we perform to the algorithm are: Align the string with itself by placing the string on the top and on the left. Set all diagonal elements to zero so we don't align a character by itself. Since both strings are same, we only need to calculate one half of the matrix. When calculating the next cell of the matrix, we assign a score of zero for mismatch or gap. just increment the score by one for match. The scores in this matrix represent the length of the tandem repeat. We repeat the longest tandem repeat in the sequences. If we have multiple repeats of the same length, we repeat all of them. Suffix Array - Induced Sorting Method \u203b If you are not familiar with Suffix Array, please look at String Search page . class kerasy . Bio . tandem . find ( method = \"SAIS\" ) In Kerasy, we use the $O(n)$ searching method which is introduced in \" Finding Maximal Repetitions in a Word in Linear Time \" Since various auxiliary data, theorems, and algorithms are used in this implementation, only the important ones are shown below. s-factorization s-factorization , or Lempel-Ziv(LZ77) decomposition of a string $S$ is a factorization $S=f_1\\cdots f_n$ where each factor $f_k$ is either a single character if that character does not occur in $f_1\\cdots f_{k-1}$. the longest prefix of the rest of the string which occurs at least twice in $f_1\\cdots f_{ks}$. Most recent efficient linear time algorithms are off-line, running in $O\\left(N\\right)$ time for integer alphabets using $O\\left(N\\right)$ space. They first construct the suffix array of the string, then compute two arrays called Longest Common Prefix(LCP) array , and Longest Previous Factor(LPF) array from which the LZ factorization can be easily computed. Theorem 5 ( paper1 ) Let $w=u_1u_2\\cdots u_k$ be the s-factorization of $w$, and let $r$ be a maximal repetition in $w$, we can classify $r$ into two classes: $$ \\begin{aligned} &\\begin{cases} \\text{initpos}(r)\\leq\\text{initpos}(u_i)\\ \\text{initpos}(u_i)\\leq\\text{endpos}(r) < \\text{endpos}(u_i) \\end{cases}&\\cdots\\left(\\text{type}1\\right)\\ &\\text{initpos}(u_i) < \\text{initpos}(r) < \\text{endpos}(r) < \\text{endpos}(u_i)&\\cdots\\left(\\text{type}2\\right)\\ \\end{aligned} $$ See more detail: Simpler and Faster Lempel Ziv Factorization Find repetitions of (type1) rightmax periodicities Let us focus on maximal repetitions $r$ which have a period in $u$. ( rightmax periodicities ) We need two auxiliary functions: Name Domain Definition Description $$LP(i)$$ $$2\\leq i \\leq n+1$$ $$LP(i) = \\begin{cases}\\max\\left{j\\mid u[1\\ldots j] = u[i\\ldots i+j-1]\\right}&\\left(2\\leq i\\leq n\\right)\\0 &\\left(i=n+1\\right)\\end{cases}$$ $LP(i)$ is the length of the longest prefix of $u$ which is also a prefix of $u[i\\ldots n]$ $$LS(i)$$ $$1\\leq i \\leq n$$ $$LS(i) = \\max\\left{j\\mid t[m-j+1\\ldots m] = v[m+i-j+1\\ldots m+i]\\right}$$ $LS(i)$ is the length of the longest suffix of $t$ which is also a suffix of $tu[1\\ldots i]$. Theorem 5.1 ( paper2 ) Let $m = |t|, n = |u|$ $j$ is an integer. $r$ is a substring of $tu$ with at least one character in $t$ and at least $j$ characters in $u$, and with $|r|\\geq2j$. Then, $r$ is a periodicity with period length $j$ iff it begins at or after $t[m-LS(j)+1]$ and ends at or before $u[j+LP(j+1)]$. Proof 5.1 ( paper2 ) Let $a > 0$ be the number of characters of $r$ in $t$ $b \\geq 0$ be the number of characters of $r$ in $u[j+1\\ldots n]$ $i=|r|= a+b+j$ For $r$ to be a periodicity with period length $j$, it is necessary and sufficient for the first $i-j$ characters of $r$ to match the last $i-j$ characters of $r$. ($r[1\\ldots a+b]$ matches $r[1+j\\ldots i]$.) This can be broken into two separate conditions: $r[1\\ldots a]$ matches $r[1+j\\ldots a+j]$ $r[a+1\\ldots a+b]$ matches $r[a+j+1\\ldots i]$ These condition is equivalent to requiring \"$r[1+j\\ldots a+j]=r[1+j]\\ldots u[j]$ is suffix of $t$\" is equivalent to requiring \"$r$ to begin at or after $t[m-LS(j)+1]$. \" $$\\begin{aligned} &(a+j)-(1+j)+1\\leq LS(j)\\ \\Longleftrightarrow &a\\leq LS(j) \\ \\Longleftrightarrow &m-\\text{initpos}(r)+1\\leq LS(j)\\ \\Longleftrightarrow &\\text{initpos}(r)\\geq m-LS(j)+1 \\end{aligned}$$ requiring $r[a+j+1\\ldots i]=u[j+1]\\ldots r[i]$ is prefix of $u$, so this is equivalent to requiring $r$ to end at or before $u[j+LP(j+1)]$ $$\\begin{aligned} &i-(a+j+1)+1\\leq LP(j+1)\\ \\Longleftrightarrow &b\\leq LP(j+1)\\ \\Longleftrightarrow &\\text{endpos}(r)-(m+j)\\leq LP(j+1)\\ \\Longleftrightarrow &\\text{endpos}(r)\\leq m+j+LP(j+1) \\end{aligned}$$ Thus, conditions $1$ and $2$ together are equivalent to the requirement stated in Theorem 5.1. Theorem 5.2 ( paper2 ) Let $j(1\\leq j\\leq n)$ is an integer. There exists a rightmax periodicity (in $tu$) with period length $j$ iff $LS(j)+LP(j+1)\\geq j$. When this inequality holds, there is exactly one rightmax periodicity with period length $j$, and this periodicity occurs at $t[m-LS(j)+1]\\ldots u[j+LP(j+1)]$ Proof 5.2 ( paper2 ) If the inequality fails , then there can be no substring of length $2j$ or more which meets the beginning and ending requirements of Theoreme 5.1 , and hence no periodicity with period length $j$ occurs with at least one character in $t$ and at least $j$ characters in $u$. This implies there are no rightmax periodicities (in $tu$) with period length $j$. On the other hand, if the inequality holds, then Theoreme 5.1 states that every substring (with length $2j$ or more) which starts in $t[m-LS(j)+1\\ldots m]$ and ends in $u[j\\ldots j+LP(j+1)]$ is a periodicity with period length $j$. Moreover, these are the only such periodicities which have at least one character in $t$ and at least one period length in $u$. Of these periodicities, only one is maximal - the one that starts as far left as possible ($t[m-LS(j)+1]$) and continues as far right as possible ($u[j+LP(j+1)]$). leftmax periodicities As described in the table below, we can find leftmax periodicities (type1) symmetrically. rightmax periodicities leftmax periodicities maximal repetitions $r$ which have a period in $u$ maximal repetitions $r$ which have a period in $t$ Find repetitions of (type2) The task is greatly simplified by the fact that every repetition of type 2 occurs entirely inside some s-factor $u_i$ and each $u_i$ has an earlier occurrence in $w$. Let $v_i$ be thhe earlier occurrence of $u_i$, and let $\\Delta_i = \\text{initpos}(u_i) - \\text{initpos}(v_i).$ Obiously, each repetition of type2 occurring inside $u_i$ is a copy of a maximal repetition occurring inside $v_i$ shifted by $\\Deltai$ to the right. Reference Study, Analysis, and Implementation of Different Techniques to Find Tandem Repeats. Finding Maximal Repetitions in a Word in Linear Time. [paper1] mreps: efficient and flexible detection of tandem repeats in DNA Simpler and Faster Lempel Ziv Factorization M. Crochemore and W. Rytter. Text algorithms. OxfordUniversity Pres M. G. Main. Detecting leftmost maximal periodicities. [paper2] An O(n log n) algorithm for finding all repetitions in a string terminology Name symbol Description word $$w$$ $$w = a_1a_2\\ldots a_n$$ subword $$w[i\\ldots j]$$ $$w[i\\ldots j] = a_i\\ldots a_j$$ position $$\\pi$$ Each position $\\pi$ in $w$ defines a factorization $w = w_1w_2$ where $\\mid w_1\\mid=\\pi$. A position in a word $w = a_1a_2\\ldots a_n$ is an integer between $0$ and $n$, and the position of letter $a_i$ in $w$ is $i-1$. initpos , endpos $$initpos(v), endpos(v)$$ If $v=w[i\\ldots j]$, we denote $initpos(v) = i-1$ and $endpos(v)=j$. cross We say that subword $v=w[i\\ldots j]$ crosses a position $\\pi$ in $w$, if $i\\leq\\pi<j$ root $u$ If $w$ is a subword of $u^n$ for some natural $n$, $\\mid u \\mid$ is called a period of $w$, and word $u$ is a root of $w$. period $$p$$ smallest positive integer such that $a_i=a_{i+p}$ for all $i$, provided $1\\leq i, i+p\\leq n$. minimal period $$p(w)$$ Each word $w$ has the minimal period $p(w)$ and call the period of $w$. exponent $$e(w)$$ The ratio $\\frac{\\mid w\\mid}{p(w)}$ is called the exponent of $w$ and denoted $e(w)$. primitive a root $u$ of $w$ such that $\\mid u\\mid = p(w)$, is primitive , that is $u$ cannot be written as $v^k$ for $k\\geq2$. repetition $$r$$ A repetition in $w$ is any subword occurrence $r=w[i\\ldots j]$ with $e(r)\\geq2$ maximal repetition A maximal repetition in $w$ is a repetition $r=w[i\\ldots j]$ such that $p\\left(w[i\\ldots j]\\right) < p\\left(w[i-1\\ldots j]\\right)$ whenever $i > 1$, and $p\\left(w[i\\ldots j]\\right) < p\\left(w[i\\ldots j+1]\\right)$ whenever $j < n$.","title":"Tandem Repeats"},{"location":"BioInformatics/Tandem%20Repeats/#tandem-repeats","text":"Tandem Repeats are simple sequence repeats that are extremely common throughout the genomes of a wide range of species. Tandem repeats are helpful to determine the inherited traits of gene from its parent and in determining the ancestor relationship. Tandem repeats are proven to be biologically significant as they can help in discovery of dynamic mutations for genetic diseases. There are two main methods like dynamic programming , suffix array induced sorting , to find exact tandem repeats. # Suffix Array - Induced Sorting Dynamic Programming Construction $$O\\left(n\\right)$$ $$O\\left(n^2\\right)$$ Tandem repeats discovery $$O\\left(n\\ast m\\right)$$ $$O\\left(n^2\\right)$$ Total time $$O\\left(n + n\\ast m\\right)$$ $$O\\left(n^2\\right)$$ Space complexity $$O\\left(n\\right)$$ $$O\\left(n^2\\right)$$ here, $n$ is the length of the sequence and $m$ is the average length of the suffix. The time required to identify tandem repeats mainly depends on construction of the required data structure. For Dynamic programming method, a square matrix of size of the sequence is constructed and values are calculated which takes the time and space complexity of $O\\left(n^2\\right)$. In SA-IS method, the time and space needed for construction is $O\\left(n\\right)$.","title":"Tandem Repeats"},{"location":"BioInformatics/Tandem%20Repeats/#dynamic-programming-method","text":"class kerasy . Bio . tandem . find ( method = \"DP\" ) To find all tandem repeats in a sequence we perform slight modifications to the Smith-Waterman algorithm for local alignments. The time complexity of the method will be $O\\left(n\\ast n\\right)$ as we will be filling a two-dimensional matrix. We then align the sequence with itself to find tandem repeats. The modifications we perform to the algorithm are: Align the string with itself by placing the string on the top and on the left. Set all diagonal elements to zero so we don't align a character by itself. Since both strings are same, we only need to calculate one half of the matrix. When calculating the next cell of the matrix, we assign a score of zero for mismatch or gap. just increment the score by one for match. The scores in this matrix represent the length of the tandem repeat. We repeat the longest tandem repeat in the sequences. If we have multiple repeats of the same length, we repeat all of them.","title":"Dynamic Programming Method"},{"location":"BioInformatics/Tandem%20Repeats/#suffix-array-induced-sorting-method","text":"\u203b If you are not familiar with Suffix Array, please look at String Search page . class kerasy . Bio . tandem . find ( method = \"SAIS\" ) In Kerasy, we use the $O(n)$ searching method which is introduced in \" Finding Maximal Repetitions in a Word in Linear Time \" Since various auxiliary data, theorems, and algorithms are used in this implementation, only the important ones are shown below.","title":"Suffix Array - Induced Sorting Method"},{"location":"BioInformatics/Tandem%20Repeats/#s-factorization","text":"s-factorization , or Lempel-Ziv(LZ77) decomposition of a string $S$ is a factorization $S=f_1\\cdots f_n$ where each factor $f_k$ is either a single character if that character does not occur in $f_1\\cdots f_{k-1}$. the longest prefix of the rest of the string which occurs at least twice in $f_1\\cdots f_{ks}$. Most recent efficient linear time algorithms are off-line, running in $O\\left(N\\right)$ time for integer alphabets using $O\\left(N\\right)$ space. They first construct the suffix array of the string, then compute two arrays called Longest Common Prefix(LCP) array , and Longest Previous Factor(LPF) array from which the LZ factorization can be easily computed.","title":"s-factorization"},{"location":"BioInformatics/Tandem%20Repeats/#theorem-5-paper1","text":"Let $w=u_1u_2\\cdots u_k$ be the s-factorization of $w$, and let $r$ be a maximal repetition in $w$, we can classify $r$ into two classes: $$ \\begin{aligned} &\\begin{cases} \\text{initpos}(r)\\leq\\text{initpos}(u_i)\\ \\text{initpos}(u_i)\\leq\\text{endpos}(r) < \\text{endpos}(u_i) \\end{cases}&\\cdots\\left(\\text{type}1\\right)\\ &\\text{initpos}(u_i) < \\text{initpos}(r) < \\text{endpos}(r) < \\text{endpos}(u_i)&\\cdots\\left(\\text{type}2\\right)\\ \\end{aligned} $$ See more detail: Simpler and Faster Lempel Ziv Factorization","title":"Theorem 5 (paper1)"},{"location":"BioInformatics/Tandem%20Repeats/#find-repetitions-of-type1","text":"","title":"Find repetitions of (type1)"},{"location":"BioInformatics/Tandem%20Repeats/#rightmax-periodicities","text":"Let us focus on maximal repetitions $r$ which have a period in $u$. ( rightmax periodicities ) We need two auxiliary functions: Name Domain Definition Description $$LP(i)$$ $$2\\leq i \\leq n+1$$ $$LP(i) = \\begin{cases}\\max\\left{j\\mid u[1\\ldots j] = u[i\\ldots i+j-1]\\right}&\\left(2\\leq i\\leq n\\right)\\0 &\\left(i=n+1\\right)\\end{cases}$$ $LP(i)$ is the length of the longest prefix of $u$ which is also a prefix of $u[i\\ldots n]$ $$LS(i)$$ $$1\\leq i \\leq n$$ $$LS(i) = \\max\\left{j\\mid t[m-j+1\\ldots m] = v[m+i-j+1\\ldots m+i]\\right}$$ $LS(i)$ is the length of the longest suffix of $t$ which is also a suffix of $tu[1\\ldots i]$.","title":"rightmax periodicities"},{"location":"BioInformatics/Tandem%20Repeats/#theorem-51-paper2","text":"Let $m = |t|, n = |u|$ $j$ is an integer. $r$ is a substring of $tu$ with at least one character in $t$ and at least $j$ characters in $u$, and with $|r|\\geq2j$. Then, $r$ is a periodicity with period length $j$ iff it begins at or after $t[m-LS(j)+1]$ and ends at or before $u[j+LP(j+1)]$.","title":"Theorem 5.1 (paper2)"},{"location":"BioInformatics/Tandem%20Repeats/#proof-51-paper2","text":"Let $a > 0$ be the number of characters of $r$ in $t$ $b \\geq 0$ be the number of characters of $r$ in $u[j+1\\ldots n]$ $i=|r|= a+b+j$ For $r$ to be a periodicity with period length $j$, it is necessary and sufficient for the first $i-j$ characters of $r$ to match the last $i-j$ characters of $r$. ($r[1\\ldots a+b]$ matches $r[1+j\\ldots i]$.) This can be broken into two separate conditions: $r[1\\ldots a]$ matches $r[1+j\\ldots a+j]$ $r[a+1\\ldots a+b]$ matches $r[a+j+1\\ldots i]$ These condition is equivalent to requiring \"$r[1+j\\ldots a+j]=r[1+j]\\ldots u[j]$ is suffix of $t$\" is equivalent to requiring \"$r$ to begin at or after $t[m-LS(j)+1]$. \" $$\\begin{aligned} &(a+j)-(1+j)+1\\leq LS(j)\\ \\Longleftrightarrow &a\\leq LS(j) \\ \\Longleftrightarrow &m-\\text{initpos}(r)+1\\leq LS(j)\\ \\Longleftrightarrow &\\text{initpos}(r)\\geq m-LS(j)+1 \\end{aligned}$$ requiring $r[a+j+1\\ldots i]=u[j+1]\\ldots r[i]$ is prefix of $u$, so this is equivalent to requiring $r$ to end at or before $u[j+LP(j+1)]$ $$\\begin{aligned} &i-(a+j+1)+1\\leq LP(j+1)\\ \\Longleftrightarrow &b\\leq LP(j+1)\\ \\Longleftrightarrow &\\text{endpos}(r)-(m+j)\\leq LP(j+1)\\ \\Longleftrightarrow &\\text{endpos}(r)\\leq m+j+LP(j+1) \\end{aligned}$$ Thus, conditions $1$ and $2$ together are equivalent to the requirement stated in Theorem 5.1.","title":"Proof 5.1 (paper2)"},{"location":"BioInformatics/Tandem%20Repeats/#theorem-52-paper2","text":"Let $j(1\\leq j\\leq n)$ is an integer. There exists a rightmax periodicity (in $tu$) with period length $j$ iff $LS(j)+LP(j+1)\\geq j$. When this inequality holds, there is exactly one rightmax periodicity with period length $j$, and this periodicity occurs at $t[m-LS(j)+1]\\ldots u[j+LP(j+1)]$","title":"Theorem 5.2 (paper2)"},{"location":"BioInformatics/Tandem%20Repeats/#proof-52-paper2","text":"If the inequality fails , then there can be no substring of length $2j$ or more which meets the beginning and ending requirements of Theoreme 5.1 , and hence no periodicity with period length $j$ occurs with at least one character in $t$ and at least $j$ characters in $u$. This implies there are no rightmax periodicities (in $tu$) with period length $j$. On the other hand, if the inequality holds, then Theoreme 5.1 states that every substring (with length $2j$ or more) which starts in $t[m-LS(j)+1\\ldots m]$ and ends in $u[j\\ldots j+LP(j+1)]$ is a periodicity with period length $j$. Moreover, these are the only such periodicities which have at least one character in $t$ and at least one period length in $u$. Of these periodicities, only one is maximal - the one that starts as far left as possible ($t[m-LS(j)+1]$) and continues as far right as possible ($u[j+LP(j+1)]$).","title":"Proof 5.2 (paper2)"},{"location":"BioInformatics/Tandem%20Repeats/#leftmax-periodicities","text":"As described in the table below, we can find leftmax periodicities (type1) symmetrically. rightmax periodicities leftmax periodicities maximal repetitions $r$ which have a period in $u$ maximal repetitions $r$ which have a period in $t$","title":"leftmax periodicities"},{"location":"BioInformatics/Tandem%20Repeats/#find-repetitions-of-type2","text":"The task is greatly simplified by the fact that every repetition of type 2 occurs entirely inside some s-factor $u_i$ and each $u_i$ has an earlier occurrence in $w$. Let $v_i$ be thhe earlier occurrence of $u_i$, and let $\\Delta_i = \\text{initpos}(u_i) - \\text{initpos}(v_i).$ Obiously, each repetition of type2 occurring inside $u_i$ is a copy of a maximal repetition occurring inside $v_i$ shifted by $\\Deltai$ to the right. Reference Study, Analysis, and Implementation of Different Techniques to Find Tandem Repeats. Finding Maximal Repetitions in a Word in Linear Time. [paper1] mreps: efficient and flexible detection of tandem repeats in DNA Simpler and Faster Lempel Ziv Factorization M. Crochemore and W. Rytter. Text algorithms. OxfordUniversity Pres M. G. Main. Detecting leftmost maximal periodicities. [paper2] An O(n log n) algorithm for finding all repetitions in a string","title":"Find repetitions of (type2)"},{"location":"BioInformatics/Tandem%20Repeats/#terminology","text":"Name symbol Description word $$w$$ $$w = a_1a_2\\ldots a_n$$ subword $$w[i\\ldots j]$$ $$w[i\\ldots j] = a_i\\ldots a_j$$ position $$\\pi$$ Each position $\\pi$ in $w$ defines a factorization $w = w_1w_2$ where $\\mid w_1\\mid=\\pi$. A position in a word $w = a_1a_2\\ldots a_n$ is an integer between $0$ and $n$, and the position of letter $a_i$ in $w$ is $i-1$. initpos , endpos $$initpos(v), endpos(v)$$ If $v=w[i\\ldots j]$, we denote $initpos(v) = i-1$ and $endpos(v)=j$. cross We say that subword $v=w[i\\ldots j]$ crosses a position $\\pi$ in $w$, if $i\\leq\\pi<j$ root $u$ If $w$ is a subword of $u^n$ for some natural $n$, $\\mid u \\mid$ is called a period of $w$, and word $u$ is a root of $w$. period $$p$$ smallest positive integer such that $a_i=a_{i+p}$ for all $i$, provided $1\\leq i, i+p\\leq n$. minimal period $$p(w)$$ Each word $w$ has the minimal period $p(w)$ and call the period of $w$. exponent $$e(w)$$ The ratio $\\frac{\\mid w\\mid}{p(w)}$ is called the exponent of $w$ and denoted $e(w)$. primitive a root $u$ of $w$ such that $\\mid u\\mid = p(w)$, is primitive , that is $u$ cannot be written as $v^k$ for $k\\geq2$. repetition $$r$$ A repetition in $w$ is any subword occurrence $r=w[i\\ldots j]$ with $e(r)\\geq2$ maximal repetition A maximal repetition in $w$ is a repetition $r=w[i\\ldots j]$ such that $p\\left(w[i\\ldots j]\\right) < p\\left(w[i-1\\ldots j]\\right)$ whenever $i > 1$, and $p\\left(w[i\\ldots j]\\right) < p\\left(w[i\\ldots j+1]\\right)$ whenever $j < n$.","title":"terminology"},{"location":"DeepLearning/CNN/","text":"CNN 2019-08-30(\u91d1) convolutional.py Notebook Example Notebook: Kerasy.examples.MNIST.ipynb CNN is originated from Neocognitron , which was devised based on the neurophysiological knowledge ( visual cortex of the living organism's brain), and has a structure specialized for image processing. Neocognitron consists of: Convolutional Layer : corresponding to simple cells (S-cells) for feature extraction Pooling Layer : corresponding to complex cells (C-cells) having a function of allowing positional deviation. CNN can learn well by using backpropagation . The algorithm is shown mathematically below. Convolutional Layer convolutional.py Mono Multi forward \u00b6 $$ \\begin{cases} \\begin{aligned} a_{i,j,c'}^{{k+1}} &= \\sum_c\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}w_{m,n,c,c'}^{k+1}z_{i+m,j+n,c}^{k} + b_{c'}^{k+1}\\\\ z_{i,j,c'}^{{k}} &= h^{k}\\left(a_{i,j,c'}^{{k}}\\right) \\end{aligned} \\end{cases} $$ backprop \u00b6 Check image individually $w_{m,n,c,c'}^k, b_{c'}^k$ $$ \\begin{aligned} \\frac{\\partial E}{\\partial w_{m,n,c,c'}^{k+1}} &= \\sum_{i}\\sum_{j}\\frac{\\partial E}{\\partial a_{i,j,c'}^{k+1}}\\frac{\\partial a_{i,j,c'}^{k+1}}{\\partial w_{m,n,c,c'}^{k+1}}\\\\ &= \\sum_{i}\\sum_{j}\\frac{\\partial E}{\\partial a_{i,j,c'}^{k+1}}z_{i+m,j+n,c}^{k}\\\\ &= \\sum_{i}\\sum_{j}\\delta_{i,j,c'}^{k+1}\\cdot z_{i+m,j+n,c}^{k}\\\\ \\frac{\\partial E}{\\partial b_{c'}^{k+1}} &= \\sum_{i}\\sum_{j}\\delta_{i,j,c'}^{k+1} \\end{aligned} $$ $\\delta_{i,j,c}^k$ $$ \\begin{aligned} \\delta_{i,j,c}^{k} &= \\frac{\\partial E}{\\partial a_{i,j,c}^{k}} \\\\ &= \\sum_{c'}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}\\left(\\frac{\\partial E}{\\partial a_{i-m,j-n,c'}^{k+1}}\\right)\\left(\\frac{\\partial a_{i-m,j-n,c'}^{k+1}}{\\partial a_{i,j,c}^k}\\right)\\\\ &= \\sum_{c'}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1} \\left(\\delta_{i-m,j-n,c'}^{k+1}\\right)\\left(w_{m,n,c,c'}^{k+1}h'\\left(a_{i,j,c}^k\\right)\\right) \\\\ &= h'\\left(a_{i,j,c}^k\\right)\\sum_{c'}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1} \\delta_{i-m,j-n,c'}^{k+1}\\cdot w_{m,n,c,c'}^{k+1} \\end{aligned} $$ Pooling Layer pooling.py forward \u00b6 backprop \u00b6 ex. MNIST \u00b6 The MNIST database (Modified National Institute of Standards and Technology database) is a large (60,000 training images and 10,000 testing images) database of handwritten digits that is commonly used for training various image processing systems. 0 1 2 3 4 5 6 7 8 9 In [1]: import numpy as np from kerasy.datasets import mnist from kerasy.models import Sequential from kerasy.layers import Dense , Dropout , Flatten , Conv2D , MaxPooling2D , Input from kerasy.utils import CategoricalEncoder In [2]: # Datasets Parameters. num_classes = 10 n_samples = 1_000 # Training Parameters. batch_size = 16 epochs = 20 keep_prob1 = 0.75 keep_prob2 = 0.5 In [3]: # input image dimensions img_rows , img_cols = 28 , 28 In [4]: # the data, split between train and test sets ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () In [5]: x_train = np . expand_dims ( x_train , axis =- 1 ) x_test = np . expand_dims ( x_test , axis =- 1 ) input_shape = ( img_rows , img_cols , 1 ) In [6]: x_train = x_train [: n_samples ] y_train = y_train [: n_samples ] x_test = x_test [: n_samples ] y_test = y_test [: n_samples ] In [7]: x_train = x_train . astype ( 'float64' ) x_test = x_test . astype ( 'float64' ) x_train /= 255 x_test /= 255 print ( 'x_train shape:' , x_train . shape ) print ( x_train . shape [ 0 ], 'train samples' ) print ( x_test . shape [ 0 ], 'test samples' ) x_train shape: (1000, 28, 28, 1) 1000 train samples 1000 test samples In [8]: # convert class vectors to binary class matrices encoder = CategoricalEncoder () y_train = encoder . to_onehot ( y_train , num_classes ) y_test = encoder . to_onehot ( y_test , num_classes ) Dictionaly for Encoder is already made. In [9]: model = Sequential () model . add ( Input ( input_shape = input_shape )) model . add ( Conv2D ( 32 , kernel_size = ( 3 , 3 ), activation = 'relu' )) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Dropout ( keep_prob = keep_prob1 )) model . add ( Flatten ()) model . add ( Dense ( 128 , activation = 'relu' )) model . add ( Dropout ( keep_prob = keep_prob2 )) model . add ( Dense ( num_classes , activation = 'softmax' )) In [10]: model . compile ( optimizer = 'adagrad' , loss = 'categorical_crossentropy' , metrics = [ 'categorical_accuracy' ] ) /Users/iwasakishuto/Github/portfolio/Kerasy/kerasy/engine/sequential.py:67: UserWarning: Kerasy Warnings ------------------------------------------------------------ When calculating the CategoricalCrossentropy loss and the derivative of the Softmax layer, the gradient disappears when backpropagating the actual value, so the SoftmaxCategoricalCrossentropy is implemented instead. ------------------------------------------------------------ \"so the \\033[34mSoftmaxCategoricalCrossentropy\\033[0m is implemented instead.\\n\" + '-'*60) In [11]: model . summary () ----------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================= input_1 (Input) (None, 28, 28, 1) 0 ----------------------------------------------------------------- conv2d_1 (Conv2D) (None, 26, 26, 32) 320 ----------------------------------------------------------------- conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 ----------------------------------------------------------------- maxpooling2d_1 (MaxPooling2D (None, 12, 12, 64) 0 ----------------------------------------------------------------- dropout_1 (Dropout) (None, 12, 12, 64) 0 ----------------------------------------------------------------- flatten_1 (Flatten) (None, 9216) 0 ----------------------------------------------------------------- dense_1 (Dense) (None, 128) 1179776 ----------------------------------------------------------------- dropout_2 (Dropout) (None, 128) 0 ----------------------------------------------------------------- dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 ----------------------------------------------------------------- In [12]: model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( x_test , y_test ) ) Epoch 01/20 | 63/63[####################]100.00% - 1442.691[s] categorical_crossentropy : 1381.319 , categorical_accuracy : 54.4% , val_categorical_crossentropy : 740.757 , val_categorical_accuracy : 78.9% Epoch 02/20 | 63/63[####################]100.00% - 1439.734[s] categorical_crossentropy : 573.693 , categorical_accuracy : 83.7% , val_categorical_crossentropy : 584.554 , val_categorical_accuracy : 86.2% Epoch 03/20 | 63/63[####################]100.00% - 1318.618[s] categorical_crossentropy : 405.320 , categorical_accuracy : 88.2% , val_categorical_crossentropy : 418.508 , val_categorical_accuracy : 90.4% Epoch 04/20 | 63/63[####################]100.00% - 1010.807[s] categorical_crossentropy : 318.465 , categorical_accuracy : 90.0% , val_categorical_crossentropy : 440.983 , val_categorical_accuracy : 90.1% Epoch 05/20 | 63/63[####################]100.00% - 1020.449[s] categorical_crossentropy : 261.625 , categorical_accuracy : 92.6% , val_categorical_crossentropy : 496.641 , val_categorical_accuracy : 89.6% Epoch 06/20 | 63/63[####################]100.00% - 1018.260[s] categorical_crossentropy : 215.082 , categorical_accuracy : 92.9% , val_categorical_crossentropy : 419.564 , val_categorical_accuracy : 92.7% Epoch 07/20 | 63/63[####################]100.00% - 1013.837[s] categorical_crossentropy : 168.225 , categorical_accuracy : 95.0% , val_categorical_crossentropy : 461.159 , val_categorical_accuracy : 91.4% Epoch 08/20 | 63/63[####################]100.00% - 1013.345[s] categorical_crossentropy : 164.291 , categorical_accuracy : 95.1% , val_categorical_crossentropy : 479.793 , val_categorical_accuracy : 91.0% Epoch 09/20 | 63/63[####################]100.00% - 1021.595[s] categorical_crossentropy : 134.688 , categorical_accuracy : 96.0% , val_categorical_crossentropy : 450.356 , val_categorical_accuracy : 91.4% Epoch 10/20 | 63/63[####################]100.00% - 1012.004[s] categorical_crossentropy : 132.845 , categorical_accuracy : 96.3% , val_categorical_crossentropy : 449.674 , val_categorical_accuracy : 92.1% Epoch 11/20 | 63/63[####################]100.00% - 1015.242[s] categorical_crossentropy : 113.595 , categorical_accuracy : 96.2% , val_categorical_crossentropy : 480.528 , val_categorical_accuracy : 92.1% Epoch 12/20 | 63/63[####################]100.00% - 1015.592[s] categorical_crossentropy : 93.535 , categorical_accuracy : 96.8% , val_categorical_crossentropy : 497.775 , val_categorical_accuracy : 91.7% Epoch 13/20 | 63/63[####################]100.00% - 1021.730[s] categorical_crossentropy : 95.980 , categorical_accuracy : 96.9% , val_categorical_crossentropy : 477.428 , val_categorical_accuracy : 91.8% Epoch 14/20 | 63/63[####################]100.00% - 1020.249[s] categorical_crossentropy : 68.847 , categorical_accuracy : 98.3% , val_categorical_crossentropy : 520.389 , val_categorical_accuracy : 91.4% Epoch 15/20 | 63/63[####################]100.00% - 1020.915[s] categorical_crossentropy : 84.820 , categorical_accuracy : 96.9% , val_categorical_crossentropy : 517.873 , val_categorical_accuracy : 91.9% Epoch 16/20 | 63/63[####################]100.00% - 1019.250[s] categorical_crossentropy : 70.326 , categorical_accuracy : 97.7% , val_categorical_crossentropy : 509.199 , val_categorical_accuracy : 92.7% Epoch 17/20 | 63/63[####################]100.00% - 1012.677[s] categorical_crossentropy : 71.140 , categorical_accuracy : 97.4% , val_categorical_crossentropy : 475.805 , val_categorical_accuracy : 92.6% Epoch 18/20 | 63/63[####################]100.00% - 1013.460[s] categorical_crossentropy : 71.360 , categorical_accuracy : 98.0% , val_categorical_crossentropy : 548.380 , val_categorical_accuracy : 92.2% Epoch 19/20 | 63/63[####################]100.00% - 1043.271[s] categorical_crossentropy : 65.436 , categorical_accuracy : 97.8% , val_categorical_crossentropy : 480.036 , val_categorical_accuracy : 92.5% Epoch 20/20 | 63/63[####################]100.00% - 1163.611[s] categorical_crossentropy : 47.725 , categorical_accuracy : 98.5% , val_categorical_crossentropy : 519.467 , val_categorical_accuracy : 92.3% In [13]: model . save_weights ( \"MNIST_example_notebook_adagrad.pickle\" ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"CNN"},{"location":"DeepLearning/ComputationalGraph/","text":"Computational Graph 2019-08-23(\u91d1) activations.py A computational graph is a directed graph where the nodes correspond to opecodes (operations). the edges correspond to operands (variables). By using computational graph, it is possible to obtain the final calculation result by propagating the local calculation , which simplifies the problem. Especially in the backpropagation method, the \"chain rule\" is exactly the localization of the calculation, so it has good compatibility. Chain rule vs. Computational graph \u00b6 $$\\text{ex.)}\\quad z=(x+y)^2$$ Chain rule Computational graph $$\\begin{cases}z = t^2\\\\t = x+y\\end{cases}\\\\ \\Longrightarrow\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\times \\frac{\\partial t}{\\partial x}$$ Various operation nodes Let's look at backpropagation in various nodes. Add Multiple Inverse exp dot Add $$\\text{ex.)}\\quad z=x+y$$ The derivative can be calculated as $$ \\begin{cases} \\begin{aligned} \\frac{\\partial z}{\\partial x} = 1\\\\ \\frac{\\partial z}{\\partial y} = 1\\\\ \\end{aligned} \\end{cases} $$ Therefore, the backpropagation of the Add node is expressed as follows: \u203b Just pass the differential value transmitted from the upstream to the downstream. Mul $$\\text{ex.)}\\quad z=xy$$ The derivative can be calculated as $$ \\begin{cases} \\begin{aligned} \\frac{\\partial z}{\\partial x} = y\\\\ \\frac{\\partial z}{\\partial y} = x\\\\ \\end{aligned} \\end{cases} $$ Therefore, the backpropagation of the Mul node is expressed as follows: \u203b Multiply the \"turned over value\" of the input signal. Inverse $$\\text{ex.)}\\quad y=\\frac{1}{x}$$ The derivative can be calculated as $$ \\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2} = -y^2 $$ Therefore, the backpropagation of the Inverse node is expressed as follows: \u203b Multiply the minus square of \"the output of forward propagation\". exp $$\\text{ex.)}\\quad y=\\exp(x)$$ The derivative can be calculated as $$ \\frac{\\partial y}{\\partial x} = \\exp\\left(x\\right) = y $$ Therefore, the backpropagation of the exp node is expressed as follows: \u203b Multiply the minus square of \"the output of forward propagation\". Connecting nodes By connecting the above four nodes, we can easily calculate the backpropagation of Softmax and Cross Entropy Error Supplementary explanation The backpropagation of the Softmax can be expressed as follows. $$ \\begin{cases} \\begin{aligned} &\\sum_i\\left(\\underbrace{\\underbrace{-\\frac{t_i}{y_i}\\times\\exp(a_i)}_{-t_iS}\\times\\left(-\\frac{1}{S^2}\\right)}_{\\frac{t_i}{S}}\\right) \\times \\exp(a_i) = y_i\\left(\\because\\sum_it_i=1\\right)\\\\ &\\underbrace{-\\frac{t_i}{y_i}\\times\\frac{1}{S}}_{-\\frac{t_i}{\\exp(a_i)}}\\times \\exp(a_i) = -t_i \\end{aligned} \\end{cases} $$ dot $$\\text{ex.)}\\quad \\mathbf{Y} = \\mathbf{X}\\cdot\\mathbf{W} + \\mathbf{B}$$ The derivative can be calculated as $$ \\begin{cases} \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{X}}\\cdot \\mathbf{W}^T\\\\ \\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{X}^T\\cdot\\frac{\\partial L}{\\partial \\mathbf{Y}}\\\\ \\end{aligned} \\end{cases} $$ Therefore, the backpropagation of the dot node is expressed as follows: \u203b You can easily understand the content of the figure above by decomposing a matrix calculation into its elements and using a computational graphs in smaller units. Implementation \u00b6 Add \u00b6 In [1]: class Add : def __init__ ( self ): pass def forward ( self , x , y ): out = x + y return out def backward ( self , dout ): dx = dout * 1 dy = dout * 1 return dx , dy Mul \u00b6 In [2]: class Mul : def __init__ ( self ): self . x = None self . y = None def forward ( self , x , y ): self . x = x self . y = y out = x * y return out def backward ( self , dout ): dx = dout * self . y dy = dout * self . x return dx , dy Inverse \u00b6 In [1]: class Inverse : def __init__ ( self ): self . out = None def forward ( self , x ): out = 1 / x self . out = out return out def backward ( self , dout ): dx = dout * ( - self . out ** 2 ) return dx exp \u00b6 In [4]: class Exp : def __init__ ( self ): self . out = None def forward ( self , x ): out = np . exp ( x ) self . out = out return out def backward ( self , dout ): dx = dout * self . out return dx dot \u00b6 In [5]: class DotLayer : def __init__ ( self ): self . X = None self . W = None def forward ( self , X , W ): \"\"\" @param X : shape=(1,a) @param W : shape=(a,b) @return out: shape=(1,b) \"\"\" self . X = X self . W = W out = np . dot ( X , W ) # shape=(1,b) return out def backward ( self , dout ): \"\"\" @param dout: shape=(1,b) @return dX : shape=(1,a) @return dW : shape=(a,b) \"\"\" dX = np . dot ( dout , self . W . T ) dW = np . dot ( self . X . T , dout ) return dX , dW Reference \u30bc\u30ed\u304b\u3089\u4f5c\u308b Deep Learning ( Github ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Computational Graph"},{"location":"DeepLearning/Initializers/","text":"Initializers 2019-08-28(\u6c34) initializers.py Available initializers Zeros Ones Constant RandomNormal RandomUniform TruncatedNormal VarianceScaling Orthogonal Identity GlorotNormal GlorotUniform HeNormal LeCunNormal HeUniform LeCunUniform \u203b Followed by Plot function In [18]: fig , axes = galleryplot ( func = visualizeInitialKernel , argnames = \"kernel_name\" , iterator = list ( KerasyInitializerFunctions . keys ()), sharex = \"all\" , ) Zeros \u00b6 Generates values initialized to $0$. This is often used as the bias initializer so that your network could learn without affected by bias parameter. Warning If you apply this initializer to edge weights between layers, loss will not convey (back propagation), so notwork could not learn anymore. def kerasy . initializers . Zeros ( shape , dtype = None ) In [29]: plotMonoInitializer ( \"zeros\" , xmin =- 0.3 , xmax = 1.3 ) Ones \u00b6 Generates values initialized to $1$. def kerasy . initializers . Ones ( shape , dtype = None ) In [33]: plotMonoInitializer ( \"ones\" , xmin =- 0.3 , xmax = 1.3 ) Constant \u00b6 Generates values initialized to user defined constant (= value ) . def kerasy . initializers . Constant ( shape , value = 0 , dtype = None ) In [32]: plotMonoInitializer ( \"constant\" , xmin =- 0.3 , xmax = 1.3 ) Random Normal \u00b6 Generates values from Normal distribution . def kerasy . initializers . RandomNormal ( shape , mean = 0 , stddev = 0.05 , dtype = None , seed = None ) In [34]: plotMonoInitializer ( \"random_normal\" ) Random Uniform \u00b6 Generates values from Uniform distribution . def kerasy . initializers . RandomUniform ( shape , minval =- 0.05 , maxval = 0.05 , dtype = None , seed = None ) In [35]: plotMonoInitializer ( \"random_uniform\" ) Truncated Normal \u00b6 Generates values from Truncated Normal distribution , which is derived from a Normal distribution. This distribution leave only the values within the standard deviation from the distribution that follows the Normal distribution . Hint It is recommended to use this distribution for initializing the edge weights between layers. def kerasy . initializers . TruncatedNormal ( shape , mean = 0.0 , stddev = 0.05 , dtype = None , seed = None ) In [36]: plotMonoInitializer ( \"truncated_normal\" ) Variance Scaling \u00b6 Performs scaling according to the size of the weight and generates values from Truncated Normal or Uniform distributions. def kerasy . initializers . VarianceScaling ( shape , scale = 1.0 , mode = 'fan_in' , distribution = 'normal' , dtype = None , seed = None ) In [37]: plotMonoInitializer ( \"variance_scaling\" ) Orthogonal \u00b6 Generates values as an Orthogonal matrix . def kerasy . initializers . Orthogonal ( shape , gain = 1.0 , dtype = None , seed = None ) In [38]: plotMonoInitializer ( \"orthogonal\" ) Reference Exact solutions to the nonlinear dynamics of learning in deeplinear neural networks Identity \u00b6 Generates values as an Identity matrix . def kerasy . initializers . Identity ( shape , dtype = None , gain = 1.0 ) In [39]: plotMonoInitializer ( \"identity\" ) Warning This initialization can be used when values is a 2-dimentional square matrix. Glorot Normal \u00b6 Generates values from the following distribution where $n_{\\text{in}}$: the number of input units $n_{\\text{out}}$: the number of output units $$\\text{Truncated Normal}\\left(0, \\frac{\\sqrt{2}}{\\sqrt{n_{\\text{in}}} + \\sqrt{n_{\\text{out}}}}\\right)$$ Glorot's objective is to validate Each \"activation variances\" in a layer has the same variance. $$\\forall(i,i^{\\prime}), \\mathrm{Var}\\left[z^i\\right] = \\mathrm{Var}\\left[z^{i^{\\prime}}\\right]\\quad (8)$$ Each \"activation variances\" in a layer has the same variance. $$\\forall(i,i^{\\prime}), \\mathrm{Var}\\left[\\frac{\\partial \\text{Cost}}{\\partial s^i}\\right] = \\mathrm{Var}\\left[\\frac{\\partial \\text{Cost}}{\\partial s^{i^{\\prime}}}\\right]\\quad (9)$$ These two conditions transform to: $$ \\begin{aligned} \\forall i, n_i\\mathrm{Var}\\left[W^i\\right] &= 1 & (10)\\\\ \\forall i, n_{i+1}\\mathrm{Var}\\left[W^i\\right] &= 1 & (11) \\end{aligned} $$ As a compromise between these two constraints, He took $$\\forall i, \\mathrm{Var}\\left[W^i\\right] = \\frac{2}{n_i + n_{i+1}} \\quad(12)$$ def kerasy . initializers . GlorotNormal ( shape , dtype = None , seed = None ) In [40]: plotMonoInitializer ( \"glorot_normal\" ) Reference Understanding the difficulty of training deep feedforward neural networks Glorot Uniform \u00b6 Generates values from the following distribution $$\\mathrm{Uni}\\left[-\\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}\\right]$$ The idea is same with the Glorot Normal distribution . def kerasy . initializer . GlorotUniform ( shape , dtype = None , seed = None ) In [41]: plotMonoInitializer ( \"glorot_uniform\" ) Reference Understanding the difficulty of training deep feedforward neural networks He Normal \u00b6 Generates values from the following distribution $$\\text{Truncated Normal}\\left(0, \\frac{\\sqrt{2}}{\\sqrt{n_{\\text{in}}}}\\right)$$ As the Glorot Normal has the restriction that it valids only when the activation function is \"origin-symmetric\" and \"linear near the origin\" , this distribution was invented to deal with the exceptions: ReLU Activation Function Convolutional Layer def kerasy . initializer . HeNormal ( shape , dtype = None , seed = None ) In [42]: plotMonoInitializer ( \"he_normal\" ) Reference Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification LeCun Normal \u00b6 Generates the values from He Normal distribution scaling to $1/\\sqrt{2}$. def kerasy . initializer . LeCunNormal ( shape , dtype = None , seed = None ) In [43]: plotMonoInitializer ( \"lecun_normal\" ) Reference Self-Normalizing Neural Networks Efficient BackProp He Uniform \u00b6 Generates values from the following distribution $$\\mathrm{Uni}\\left[-\\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}}}}\\right]$$ The idea is same with the He Normal distribution . def kerasy . initializer . HeUniform ( shape , dtype = None , seed = None ) In [44]: plotMonoInitializer ( \"he_uniform\" ) Reference Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification LeCun Uniform \u00b6 Generates the values from He Uniform distribution scaling to $1/\\sqrt{2}$. def kerasy . initializer . LeCunUniform ( shape , dtype = None , seed = None ) In [45]: plotMonoInitializer ( \"lecun_uniform\" ) Reference Efficient BackProp Plot function \u00b6 In [1]: import numpy as np import matplotlib.pyplot as plt from kerasy.models import Sequential from kerasy.layers import Input , Dense from kerasy.utils import galleryplot from kerasy.initializers import KerasyInitializerFunctions In [2]: def visualizeInitialKernel ( kernel_name , input_units = 1000 , bins = 30 , ax = None , xmin =- 0.3 , xmax = 0.3 ): if ax is None : fig , ax = plt . subplots () # Create the model. model = Sequential () model . add ( Input ( input_shape = ( input_units ,))) model . add ( Dense ( input_units , kernel_initializer = kernel_name )) model . compile ( optimizer = 'sgd' , loss = \"mean_squared_error\" ) # Get the layer's kernel(weights). freq = np . ravel ( model . layers [ - 1 ] . kernel ) ax . hist ( freq , bins = bins , density = True , color = \"#722f37\" ) ax . set_xlim ( xmin , xmax ) ax . set_title ( kernel_name , fontsize = 16 ) return ax In [3]: def plotMonoInitializer ( kernel_name , input_units = 1000 , bins = 30 , xmin =- 0.3 , xmax = 0.3 ): ax = visualizeInitialKernel ( kernel_name , input_units = input_units , bins = bins , xmin = xmin , xmax = xmax ) ax . set_xlabel ( \"Value\" , fontsize = 14 ), ax . set_ylabel ( \"Frequency\" , fontsize = 14 ) ax . set_title ( f \"Initialize Method: { kernel_name } \" , fontsize = 14 ) plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Initializers"},{"location":"DeepLearning/NeuralNetwork/","text":"Neural Network 2019-08-18(\u65e5) core.py Table of contents Perceptron Multi Layer Perceptron (MLP, Neural Networks) Backpropagation Example Perceptron In neural networks, a perceptron is a neuron, which is a binary classifier to decide whether or not an input, represented by a vector of numbers ($\\mathbf{x}$), belong to some specific class ($0\\ /\\ 1$). Perceptron is represented by the following threshold function: $$ \\begin{aligned} f\\left(\\mathbf{x}\\right) &= h_{\\theta}\\left(\\mathbf{w}\\cdot\\mathbf{x} + b\\right)\\\\ &=\\begin{cases}1 & \\text{if $\\mathbf{w}\\cdot\\mathbf{x} + b > \\theta.$}\\\\0 & \\text{otherwise.}\\end{cases} \\end{aligned} $$ where $\\mathbf{w}$ is a vector of real-valued weights, and $b$ is the bias, which shifts the decision boundary away from the origin and does not depend on any input value. Activation function \u00b6 In the above example, we used a step function (Heaviside function) $h_{\\theta}$ as the activation function. $$ h_{\\theta}(a) =\\begin{cases}1 & \\text{if $a > \\theta.$}\\\\0 & \\text{otherwise.}\\end{cases} $$ However, as this function is indifferentiable , we often use other activation functions in neural networks: Logistic sigmoid function \uff1a $$h(a) = \\frac{1}{1+\\exp(-a)}$$ Hyperbolic tangent function \uff1a $$\\begin{aligned}h(a) &= \\tanh (a)\\\\&=\\frac{e^a-e^{-1}}{e^1+e^{-1}}\\end{aligned}$$ Softmax function \uff1a $$h(\\mathbf{a}) = \\frac{\\exp(a_i)}{\\sum_j \\exp(a_i)}$$ Multi Layer Perceptron Then, we will connect two or more perceptons to make a two-layer perceptron. The two or more layers of perceptrons ( MLP ) are called Neural Networks. $$y = h_2\\left(\\sum_{i=0}^{m} w^{(2)}_ih_1\\left(\\sum_{j=0}^{D}w^{(1)}_{ij} x_j\\right)\\right)$$ Why multi Layers ?? \u00b6 $$ \\begin{aligned} &\\text{single :} &y;&=f\\left(\\sum_i w_i\\underbrace{\\phi_i(\\mathbf{x})}\\right)\\\\ &\\text{multi :} &y;&=h_2\\left(\\sum_{i=0}^{m} w^{(2)}_i \\underbrace{h_1\\left(\\sum_{j=0}^{D}w^{(1)}_{ij} x_j\\right)}\\right) \\end{aligned} $$ By comparing the above two equations, we can see that the \"fixed\" basis function $\\phi_i$ becomes \u201cadaptively fluctuate\u201d . $$\\phi_i(\\mathbf{x})\\rightarrow\\displaystyle h_1\\left(\\sum_{j=0}^{D}w^{(1)}_{ij} x_j\\right)$$ Therefore, it is said that \"If Neural Networks have sufficient layers and neurons, they can compute any function at all.\" Backpropagation In the context of Neural Networks, \"training\" is \"to minimize the error $E_n(\\mathbf{w})$ between the output and the correct answer by changing the weight\", and to achive this it is necessary to calculate $\\frac{\\partial E_n}{w_{ji}}$ for all $i, j$. forward backprop As shown in the figure above, as \"$E_n$ depends on $w_{ji}$ only via $a_j$\" , we can get the following formula. $$ \\frac{\\partial E_n}{\\partial w_{ji}} = \\underbrace{\\frac{\\partial E_n}{\\partial a_j}}_{\\delta_j}\\frac{\\partial a_j}{\\partial w_{ji}}$$ Likewise, as \"$E_n$ depends on $a_i$ only via $a_j$ which receives the output of neuron $j$\", we can get $$ \\delta_i = \\frac{\\partial E_n}{\\partial a_i} = \\sum_j \\frac{\\partial E_n}{\\partial a_j}\\frac{\\partial a_j}{\\partial a_i} = \\sum_j\\delta_j\\frac{\\partial a_j}{\\partial a_i}$$ Since the formula $$a_j = \\sum_i w_{ji} h_i(a_i)\\quad \\left(\\because z_i = h_i(a_i)\\right)$$ holds, we can get $$ \\frac{\\partial a_j}{\\partial a_i} = w_{ji} h_i'(a_i)$$ To summarize the results so far, we can derive the backpropagation formula for $\\delta$ $$ \\delta_i = h_i'(a_i) \\sum_j w_{ji}\\delta_j$$ Using the backpropagation formula, once we go back through the network from output to input, we can compute all $\\frac{\\partial E_n}{w_{ji}}$, and update the weights efficiently. Example In [1]: import numpy as np import matplotlib.pyplot as plt from kerasy.models import Sequential from kerasy.layers import Input , Dense Data \u00b6 In [2]: num_samples = 1000 X = np . linspace ( - 1 , 1 , num_samples ) Y = 3 * X ** 3 + 2 * X ** 2 + X noise = np . random . RandomState ( 0 ) . normal ( scale = 1 / 2 , size = num_samples ) Y_train = Y + noise In [3]: plt . plot ( X , Y , label = \"$y=3x^3+2x^2+x$\" , color = \"red\" ) plt . scatter ( X , Y_train , s = 1 , label = \"data\" , color = \"blue\" , alpha =. 3 ) plt . title ( \"Datasets.\" ), plt . xlabel ( \"$x$\" ), plt . ylabel ( \"$y$\" ) plt . legend () plt . show () Model \u00b6 In [4]: model = Sequential () model . add ( Input ( 1 )) model . add ( Dense ( 3 , activation = \"tanh\" , kernel_initializer = \"random_normal\" , bias_initializer = \"zeros\" )) model . add ( Dense ( 3 , activation = \"tanh\" , kernel_initializer = \"random_normal\" , bias_initializer = \"zeros\" )) model . add ( Dense ( 1 , activation = \"linear\" )) In [5]: model . compile ( optimizer = \"adam\" , loss = \"mse\" ) In [6]: model . summary () ----------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================= input_1 (Input) (None, 1) 0 ----------------------------------------------------------------- dense_1 (Dense) (None, 3) 6 ----------------------------------------------------------------- dense_2 (Dense) (None, 3) 12 ----------------------------------------------------------------- dense_3 (Dense) (None, 1) 4 ================================================================= Total params: 22 Trainable params: 22 Non-trainable params: 0 ----------------------------------------------------------------- In [7]: X = X . reshape ( - 1 , 1 ) Y_train = Y_train . reshape ( - 1 , 1 ) In [8]: model . fit ( X , Y_train , batch_size = 32 , epochs = 100 , verbose =- 1 ) In [9]: Y_pred = model . predict ( X ) In [10]: plt . plot ( X , Y_pred , label = \"Prediction\" , color = \"green\" ) plt . plot ( X , Y , label = \"$y=3x^3+2x^2+x$\" , color = \"red\" ) plt . scatter ( X , Y_train , s = 1 , label = \"data\" , color = \"blue\" , alpha =. 3 ) plt . title ( \"Datasets.\" ), plt . xlabel ( \"$x$\" ), plt . ylabel ( \"$y$\" ) plt . legend () plt . show () /*\u86cd\u5149\u30da\u30f3(pink)*/ .marker-pink { color: #c45a5a; background: linear-gradient(transparent 70%, #ff66ff 60%); font-weight: bold; } /*\u86cd\u5149\u30da\u30f3(blue) & hover info*/ .marker-info { color: #5C7DC4; background: linear-gradient(transparent 70%, #66FFCC 60%); font-weight: bold; position: relative; cursor: pointer; } .marker-info:hover:before { opacity: 1; } .marker-info:before { content: attr(aria-label); opacity: 0; position: absolute; top: 30px; right: -90px; font-size: 14px; width: 300px; padding: 10px; color: #fff; background-color: #555; border-radius: 3px; pointer-events: none; } if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Neural Network"},{"location":"DeepLearning/Optimizers/","text":"Optimizers 2019-08-18(\u65e5) optimizers.py After constructing your deep learning model, you will adjust it's weight parameters to improve it's performance. At this time, optimizers will take the role of deciding \"when\" , \"how\" , and \"how much\" to update the weight parameters. Available optimizers SGD Adagrad RMSprop Adadelta Adam Adamax Nadam learning rate \u00b6 All Kerasy optimizers have the arguments learning_rate ( lr ), which is a hyperparameter used in the training of models that has a small positive value, often in the range between 0.0 and 1.0. The learning rate hyperparameter controls the rate or speed at which the model learns. Let's see an example below. (optimizer is simplest Gradient Decent ) In [1]: import numpy as np import matplotlib.pyplot as plt from kerasy.optimizers import GradientDescent In [2]: # Parameters rnd = np . random . RandomState ( 0 ) xmin , xmax = ( 0 , 1 ) num_samples = 100 num_val_samples = 1000 epochs = 100 $$ y_{\\text{train}} = 2x_{\\text{train}} + 1 + \\underset{\\sim\\mathcal{N}\\left(0,1/10\\right)}{\\text{noise}} $$ In [3]: # Training Data / Initial weights. X_train = np . c_ [ rnd . uniform ( xmin , xmax , size = ( num_samples , 1 )), np . ones ( shape = ( num_samples , 1 )) ] coeff = np . asarray ([ 2 , 1 ]) . reshape ( - 1 , 1 ) y_train = X_train . dot ( coeff ) + rnd . normal ( loc = 0.0 , scale = 1 / 10 , size = ( num_samples , 1 )) Xs = np . c_ [ np . linspace ( xmin , xmax , num_val_samples ) . reshape ( - 1 , 1 ), np . ones ( shape = ( num_val_samples , 1 )) ] w = rnd . randn ( 2 , 1 ) # shape=(2,1) In [4]: fig = plt . figure ( figsize = ( 14 , 4 )) for i , lr in enumerate ([ 0.01 , 0.1 , 0.5 ]): ax = fig . add_subplot ( 1 , 3 , i + 1 ) ax . scatter ( X_train [:, 0 ], y_train , label = \"data\" ) # Initialize the optimizer & weights. opt = GradientDescent ( learning_rate = lr ) w_ = w . copy () for epoch in range ( epochs ): grad = ( 1 / num_samples ) * 2 * X_train . T . dot ( X_train . dot ( w_ ) - y_train ) w_ = opt . get_updates ( grad , w_ , f \"sample { i } \" ) if ( epoch < 10 ): y_pred = Xs . dot ( w_ ) ax . plot ( Xs [:, 0 ], y_pred , color = \"blue\" , alpha = 0.1 * ( epoch + 1 )) # After 100 epochs. y_pred = Xs . dot ( w_ ) ax . plot ( Xs [:, 0 ], y_pred , color = \"red\" , label = \"prediction\" ) ax . set_xlabel ( \"x\" ), ax . set_ylabel ( \"y\" ), ax . set_title ( f \"learning_rate= { lr } \" ) ax . legend () plt . tight_layout () plt . show () From the figure above, we could understand it is important to find a good value for the learning rate for your model on your training datasets. Notation $$\\text{(gradient) }\\ g_{i}^{t} :=\\frac{\\partial E}{\\partial w_{i}}\\left(\\mathbf{w}^{t-1}\\right)$$ SGD class kerasy . optimizers . SGD ( learning_rate = 0.01 , momentum = 0. , nesterov = False , ** kwargs ) SGD(Stochastic Gradient Decent) is basically a Gradient Descent method. $$ \\begin{aligned} v_{i}^{t} &= \\text{momentum}\\ast v_{i}^{t-1} - \\text{lr}\\ast g_{i}^{t} \\quad \\left(v_i^0=\\mathbf{0}\\right)\\\\ w_{i}^{t+1} &= \\begin{cases} w_{i}^{t} + v_{i}^{t} & (\\text{vanilla momentum})\\\\ w_{i}^{t} + \\text{momentum}\\ast v - \\text{lr}\\ast g_{i}^{t} & (\\text{nesterov momentum})\\\\ \\end{cases} \\end{aligned} $$ Argments learning_rate : The learning rate. momentum : Accelerates gradient descent in the relevant direction and dampens oscillations. nesterov : Whether to apply Nesterov momentum. Reference A mean field view of the landscape of two-layer neural networks On the importance of initialization and momentum in deep learning Adagrad class kerasy . optimizers . Adagrad ( learning_rate = 0.01 , ** kwargs ) Adagrad is an optimizer with parameter-specific learning rates , which are adapted relative to how frequently a parameter gets updated during training. \"The more updates a parameter receives, the smaller the updates.\" This idea sounds great, but it also has the problem that once you updates through a steep slope, the learning rate for that axis will decrease for good. $$ \\begin{aligned} v_{i}^{t} &=v_{i}^{t-1}+\\left(g_{i}^{t}\\right)^{2} \\quad \\left(v_i^0=\\mathbf{0}\\right)\\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{\\sqrt{v_{i}^{t}+\\varepsilon}} \\left(g_{i}^{t}\\right) \\end{aligned} $$ Argments learning_rate : The learning rate. Reference Adaptive Subgradient Methods for Online Learning and Stochastic Optimization RMSprop class kerasy . optimizers . RMSprop ( learning_rate = 0.001 , rho = 0.9 , ** kwargs ) If you use Adagrad and training takes many iterations, the learning rate becomes smaller and smaller, and eventually parameter will not be updated. Therefore, RMSprop memorizes the past gradients not completely, but forgeting gradually. $$ \\begin{aligned} v_{i}^{t} &=\\gamma v_{i}^{t-1}+(1-\\gamma)\\left(g_{i}^{t}\\right)^{2} & \\left(v_i^0=0\\right)\\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{\\sqrt{v_{i}^{t}+\\varepsilon}} \\left(g_{i}^{t}\\right) \\end{aligned} $$ Comparing $v_i^t$ update rule of Adagrad and RMSprop is as follows: $$ \\begin{cases} \\begin{aligned} v_{i}^{t}&=\\left(g_{i}^{t}\\right)^{2}+\\left(g_{i}^{t-1}\\right)^{2}+\\left(g_{i}^{t-2}\\right)^{2}+\\cdots & \\text{(Adagrad)}\\\\ v_{i}^{t}&=(1-\\gamma) \\sum_{l=1}^{t} \\gamma^{t-l}\\left(g_{i}^{l}\\right)^{2} & \\text{(RMSprop)} \\end{aligned} \\end{cases} $$ Argments learning_rate : The learning rate. rho : Discounting factor for the history/coming gradient. Reference rmsprop: Divide the gradient by a running average of its recent magnitude Adadelta class kerasy . optimizers . Adadelta ( learning_rate = 1.0 , rho = 0.95 , ** kwargs ) Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. $$ \\begin{aligned} v_{i}^{t} &=\\gamma v_{i}^{t-1}+(1-\\gamma)\\left(g_{i}^{t}\\right)^{2} & \\left(v_i^0=0\\right) \\\\ s_{i}^{t} &=\\gamma s_{i}^{t-1}+(1-\\gamma)\\left(\\Delta w_{i}^{t-1}\\right)^{2} & \\left(s_i^0=0\\right) \\\\ \\Delta w_{i}^{t} &=-\\frac{\\sqrt{s_{i}^{t}+\\epsilon}}{\\sqrt{v_{i}^{t}+\\epsilon}} g_{i}^{t} \\\\ w_{i}^{t+1} &=w_{i}^{t}+\\Delta w_{i}^{t} \\end{aligned} $$ Argments learning_rate : The learning rate. rho : The decay rate. Reference Adadelta - an adaptive learning rate method Adam class kerasy . optimizers . Adadelta ( learning_rate = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , amsgrad = False , ** kwargs ) Adam is a combination of the following two methods. Momentum using the first power of the gradient\uff08$m$\uff09 Adagrad using the square of the gradient\uff08$v$\uff09 $$ \\begin{aligned} m_{i}^{t} &=\\beta_{1} m_{i}^{t-1}+\\left(1-\\beta_{1}\\right) g_{i}^{t} & \\left(m_i^0=0\\right)\\\\ v_{i}^{t} &=\\beta_{2} v_{i}^{t-1}+\\left(1-\\beta_{2}\\right)\\left(g_{i}^{t}\\right)^{2} & \\left(v_i^0=0\\right)\\\\ \\hat{m}_{i}^{t} &=\\frac{m_{i}^{t}}{1-\\beta_{1}^{t}} \\\\ \\hat{v}_{i}^{t} &=\\frac{v_{i}^{t}}{1-\\beta_{2}^{t}} \\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{\\sqrt{\\hat{v}_{i}^{t}}+\\varepsilon}\\hat{m}_{i}^{t} \\end{aligned} $$ Argments learning_rate : The learning rate. beta_1 : The exponential decay rate for the 1st moment estimates. beta_2 : The exponential decay rate for the 2nd moment estimates. amsgrad : Whether to apply AMSGrad variant of this algorithm. Reference Adam - A Method for Stochastic Optimization On the Convergence of Adam and Beyond Adamax class kerasy . optimizers . Adamax ( learning_rate = 0.002 , beta_1 = 0.9 , beta_2 = 0.999 , ** kwargs ) Adamax is a variant of Adam based on the infinity norm ($\\beta_2\\rightarrow\\beta_2^p, p\\rightarrow\\infty$). Since it becomes unstable as the value of $p$ becomes large, $p = 1,2$ that is, the $L_1$ norm and $L_2$ norm are generally preferred, but $L_\\infty$ is also known to show stable behavior. $$ \\begin{aligned} v_i^{t} &=\\beta_{2}^{\\infty} v_i^{t-1}+\\left(1-\\beta_{2}^{\\infty}\\right)\\left|g_i^{t}\\right|^{\\infty} \\\\ &=\\max \\left(\\beta_{2} \\cdot v_i^{t-1},\\left|g_i^{t}\\right|\\right)\\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{v_i^t}\\hat{m}_{i}^{t} \\end{aligned} $$ Argments learning_rate : The learning rate. beta_1 : The exponential decay rate for the 1st moment estimates. beta_2 : The exponential decay rate for the 2nd moment estimates. Reference Adam - A Method for Stochastic Optimization Nadam class kerasy . optimizers . Nadam ( learning_rate = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , ** kwargs ) Nadam is Adam with Nesterov momentum(Nesterov's Accelerated Gradient Method) , so a combination of the following two methods. \"Nesterov\" Momentum using the first power of the gradient\uff08$m$\uff09 Adagrad using the square of the gradient\uff08$v$\uff09 Momentum Nesterov's accelerated gradient $$\\begin{aligned}\\mathbf{g}_t &\\leftarrow \\nabla_{\\theta_{t-1}}f\\left(\\theta_{t-1}\\right)\\\\ \\mathbf{m}_t&\\leftarrow \\mu\\mathbf{m}_{t-1} + \\mathbf{g}_t\\\\\\theta_{t}&\\leftarrow \\theta_{t-1} - \\eta\\mathbf{m}_t\\end{aligned}$$ $$\\begin{aligned}\\mathbf{g}_t &\\leftarrow \\nabla_{\\theta_{t-1}}f\\left(\\theta_{t-1}-\\eta\\mu\\mathbf{m}_{t-1}\\right)\\\\\\mathbf{m}_t &\\leftarrow \\mu\\mathbf{m}_{t-1} + \\mathbf{g}_t\\\\\\theta_{t}&\\leftarrow \\theta_{t-1} - \\eta\\mathbf{m}_t \\end{aligned}$$ Argments learning_rate : The learning rate. beta_1 : The exponential decay rate for the 1st moment estimates. beta_2 : The exponential decay rate for the 2nd moment estimates. Reference Nadam report On the importance of initialization and momentum in deep learning if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Optimizers"},{"location":"MachineLearning/Cluster/","text":"#### Purpose Clustering algorithms are attractive for the task of class identification in spatial databases. Notebook Example Notebook: Kerasy.examples.cluster.ipynb DBSCAN class kerasy . ML . cluster . DBSCAN ( eps = 0.5 , min_samples = 5 ) Reference A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise","title":"Cluster"},{"location":"MachineLearning/Cluster/#dbscan","text":"class kerasy . ML . cluster . DBSCAN ( eps = 0.5 , min_samples = 5 ) Reference A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise","title":"DBSCAN"},{"location":"MachineLearning/Decomposition/","text":"#### Purpose The purpose of decomposition (dimensionality reduction, feature extraction) is to find the \"best\" principal subspace where data are expressed with much less dimensions, but keep the information as much as possible. Notebook Example Notebook: Kerasy.examples.decomposition.ipynb PCA class kerasy . ML . decomposition . PCA ( n_components = None ) Consider a dataset ${\\mathbf{x}_n}$. If we consider a $D$-dimensional vector $\\mathbf{u}_1$, each data point $\\mathbf{x}_n\\in\\mathbb{R}^{D}$ is projected onto a scalar value $\\mathbf{u}_1^T\\mathbf{x}_n$. In that (one-dimensional) space, Mean: $$\\tilde{\\mathbf{x}} = \\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$$ Variance: $$\\frac{1}{N}\\sum_{n=1}^N\\left{\\mathbf{u} 1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\tilde{\\mathbf{x}}\\right}^2 = \\mathbf{u}_1^T\\mathbf{Su}_1\\quad\\mathbf{S}= \\frac{1}{N}\\sum {n=1}^N\\left(\\mathbf{x}-\\tilde{\\mathbf{x}}\\right)\\left(\\mathbf{x}-\\tilde{\\mathbf{x}}\\right)^T$$ We now maximize the projected variance $\\mathbf{u}_1^T\\mathbf{Su}_1\\quad\\mathbf{S}$ with respect to $\\mathbf{u}_1$, because we can think \"Larger variance\" \u2194\ufe0e \"Features are more expressed\" . To avoid $|\\mathbf{u}_1|\\rightarrow\\infty$, we introduce the normalization condition $\\mathbf{u}_1^T\\mathbf{u}_1=1$ using Lagrange multiplier $\\lambda_1$. Then, the object function is given by $$\\begin{aligned} L &= \\mathbf{u}_1^T\\mathbf{Su}_1 + \\lambda_1\\left(1-\\mathbf{u}_1^T\\mathbf{u}_1\\right)\\ \\frac{\\partial L}{\\partial \\mathbf{u}_1} &= 2\\mathbf{S}\\mathbf{u}_1 - 2\\lambda_1\\mathbf{u}_1 = 0\\ \\therefore\\mathbf{u}_1^T\\mathbf{Su}_1 &= \\lambda_1 \\quad (\\because \\text{left-multiplied $\\mathbf{u}_1^T$}) \\end{aligned}$$ The variance will be a maximum when we set $\\mathbf{u}_1$ eaual to the eigenvector having the largest eigenvalue $\\lambda_1$. This eigenvector is known as the first principal component. We can define additional principal components in the same way. Summary If we consider an M-dimensional projection space, the optimal linear projection is defined by the M eigenvectors u1,...,uM of the data covariance matrix S corresponding to the M largest eigenvalues \u03bb1,...,\u03bbM. Kernel PCA class kerasy . ML . decomposition . KernelPCA ( n_components = None , kernel = \"gaussian\" , ** kernelargs ) The main idea is same with PCA, but we will perform it in the feature space , which implicitly defines a nonlinear principal component model in the original data space. If we assume that the projected data set has zero mean , The covariance matrix in feature space is given by $$ \\mathbf{C} = \\frac{1}{N}\\sum_{n=1}^N\\phi(\\mathbf{x}_n)\\phi(\\mathbf{x}_n)^T $$ and its eigenvector expansion is defined by $$\\begin{aligned} \\mathbf{C}\\mathbf{v} i &= \\lambda_i\\mathbf{v}_i\\ \\frac{1}{N}\\sum {n=1}^N\\phi(\\mathbf{x}_n)\\underset{\\text{scalar}}{\\left{\\phi(\\mathbf{x}_n)^T\\mathbf{v}_i\\right}} &= \\lambda_i\\mathbf{v}_i\\quad (\\ast)\\ \\end{aligned}$$ From this equation, we see that the vector $\\mathbf{v}_i$ is given by a linear combination of the $\\phi(\\mathbf{x}_n)$ $$\\mathbf{v} i = \\sum {n=1}^N a_{in}\\phi(\\mathbf{x}_n)$$ Substituting this expansion back into the eigenvector equation $(\\ast)$, we obtain $$\\begin{aligned} \\frac{1}{N}\\sum_{n=1}^N\\phi(\\mathbf{x_n})\\phi(\\mathbf{x} n)^T\\sum {m=1}^Na_{im}\\phi(\\mathbf{x} m) &= \\lambda_i\\sum {n=1}^Na_{in}\\phi(\\mathbf{x} n)\\ \\frac{1}{N}\\sum {n=1}^Nk(\\mathbf{x} l,\\mathbf{x}_n)\\sum {m=1}^Na_{i,m}k(\\mathbf{x} n,\\mathbf{x}_m) &= \\lambda_i\\sum {n=1}^Na_{in}k(\\mathbf{x}_l,\\mathbf{x}_n)\\quad(\\because\\text{multiplied $\\phi(\\mathbf{x}_l)^T$})\\ \\mathbf{K}^2\\mathbf{a}_i &= \\lambda_iN\\mathbf{Ka}_i\\quad (\\because\\text{written in matrix notation})\\ \\end{aligned}$$ We can find solutions for $\\mathbf{a}_i$ by solving the following eigenvalue problem. The variance in feature space will be a maximum when we set $\\mathbf{a}_1$ eaual to the eigenvector having the largest eigenvalue $\\lambda_i$. $$\\mathbf{Ka}_i = \\lambda_iN\\mathbf{a}_i$$ Having solved the eigenvector problem, the projection of a point $\\mathbf{x}$ onto eigenvector $i$ is given by $$y_i(\\mathbf{x}) = \\phi(\\mathbf{x})^T\\mathbf{v} i = \\sum {n=1}^Na_{in}\\phi(\\mathbf{x})^T\\phi(\\mathbf{x} n) = \\sum {n=1}^Na_{in}k(\\mathbf{x},\\mathbf{x}_n)$$ If projected data set doesn't have zero mean , the projected data points after centralizing are given by $$\\tilde{\\phi(\\mathbf{x} n)} = \\phi(\\mathbf{x}_n)-\\frac{1}{N}\\sum {l=1}^N\\phi(\\mathbf{x}_l)$$ and the corresponding elements of the Gram matrix are given by $$\\tilde{\\mathbf{K}} = \\mathbf{K} - \\mathbf{1_NK} - \\mathbf{K1_N} +\\mathbf{1_NK1_N}$$ where $\\mathbf{1}_N$ denotes the $N\\times N$ matrix in which every element takes the value $1/N$. Warning I don't understand clearly how to deal with new data point xn. SNE Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between datapoints into conditional probabilities . The similarity of datapoint $\\mathbf{x} j$ to datapoint $\\mathbf{x}_i$ is conditional probability $p {j|i}$, that $\\mathbf{x}_i$ would pick $\\mathbf{x}_j$ as its neighbor if neighbors were picked in propotion to their probability density under Gaussian centered at $\\mathbf{x}_i$ . Mathmatically, the conditional probability $p_{j|i}$ is given by $$p_{j|i} = \\frac{\\exp\\left(-|\\mathbf{x} i-\\mathbf{x}_j|^2/2\\boldsymbol{\\sigma}_i^2\\right)}{\\sum {k\\neq i}\\exp\\left(-|\\mathbf{x}_i-\\mathbf{x}_k|^2/2\\boldsymbol{\\sigma}_i^2\\right)}$$ Because we are only interested in modeling pairwise similarities, we set the value of $p_{i|i}$ to zero. For the low-dimensional counterparts $\\mathbf{y} i$ and $\\mathbf{y}_j$ of the high-dimensional data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$, it is possible to compute a similar conditional probability, which we denote by $q {j|i}$ $$q_{j|i} = \\frac{\\exp\\left(-|\\mathbf{y} i-\\mathbf{y}_j|^2\\right)}{\\sum {k\\neq i}\\exp\\left(-|\\mathbf{y}_i-\\mathbf{y}_k|^2\\right)}$$ Again, we set $q_{i|i} = 0$, and the variance of the Gaussian to $\\frac{1}{\\sqrt{2}}$ Then, the cost function is given by $$C = \\sum_i\\mathrm{KL}(P_i|Q_i) = \\sum_i\\sum_j p_{j|i}\\log\\frac{p_{j|i}}{q_{j|i}}$$ in which $P_i$ represents the conditional probability distribution over all other data points given data points $\\mathbf{x}_i$ The remaining parameter to be selected is the variance $\\boldsymbol{\\sigma_i}$. It is defined to produce a $P_i$ with a fixed perplexity that is specified by the user. The perplexity is defined as $$Perp(P_i) = 2^{H(P_i)},\\quad H(P_i) = -\\sum_jp_{j|i}\\log_2p_{j|i}$$ The minimization of the cost function is performed using a gradient descent method. The gradient has a surprisingly simple form $$\\frac{\\partial C}{\\partial \\mathbf{y} i} = 2\\sum_j\\left(p {j|i} - q_{j|i} + p_{i|j} - q_{i|j}\\right)\\left(\\mathbf{y}_i-\\mathbf{y}_j\\right)$$ tSNE class kerasy . ML . decomposition . tSNE ( initial_momentum = 0.5 , final_momoentum = 0.8 , eta = 500 , min_gain = 0.1 , tol = 1e-5 , prec_max_iter = 50 ) As an alternative to minimizing the sum of the Kullback-Leibler divergence between a joint probability distribution $p_{j|i}$ and $q_{j|i}$, it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution: $$C = KL\\left(P|Q\\right) = \\sum_i\\sum_jp_{ij}\\log\\frac{p_{ij}}{q_{ij}}$$ In t-SNE, - Employ a Student t-distribution with one degree of freedom (which is the same as a Cauchy distribution) as the heavy-tailed distribution in the low-dimensional map. Using this distribution, the joint probabilities $q_{ij}$ are defined as $$q_{ij} = \\frac{\\left(1 + |\\mathbf{y} i-\\mathbf{y}_j|^2\\right)^{-1}}{\\sum {k\\neq l}\\left(1 + |\\mathbf{y} k-\\mathbf{y}_l|^2\\right)^{-1}}$$ - If there is an outlier $\\mathbf{x}_i$, low-dimensional map point $\\mathbf{y}_i$ has very little effect on the cost function, so set $$p {ij} = \\frac{p_{j|i} + p_{i|j}}{2n}$$ to ensure that $\\sum_jp_{ij}>\\frac{1}{2n}$ for all datapoints $\\mathbf{x}_i$ The gradient is given by $$\\frac{\\partial C}{\\partial \\mathbf{y} i} = 4\\sum_j\\left(p {ij} - q_{ij}\\right) \\left(\\mathbf{y}_i - \\mathbf{y}_j\\right)\\left(1 + |\\mathbf{y}_i - \\mathbf{y}_j|^2\\right)^{-1}$$ Reference Visualizing Data using t-SNE Pattern Recognition and Machine Learning by Christopher Bishop UMAP class kerasy . ML . decomposition . UMAP ( metric = \"euclidean\" , metric_kwds = None , min_dist = 0.1 , a = None , b = None , random_state = None , sigma_iter = 20 , sigma_tol = 1e-5 , sigma_lower = 0 , sigma_upper = 1e3 ) UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. Reference UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction tSNE vs. UMAP tSNE UMAP pure Machine Learning semi-empirical algorithm based on solid mathematical principles Preserves Only Local Structure Preserves Global Structure probability in high-dimensional space $$p_{j\\mid i} = \\frac{\\exp\\left(-|x_i-x_j|^2/2\\sigma_i^2\\right)}{\\sum_{k\\neq i}\\exp\\left(-|x_i-x_k|^2/2\\sigma_i^2\\right)}$$ $$p_{i\\mid j} = \\exp\\left(-\\frac{d(x_i,x_j)-\\rho_i}{\\sigma_i}\\right), \\quad \\rho_i = \\min_{j\\neq i}\\left{d(x_i,x_j)\\right}$$ joint probability in high-dimensional space $$p_{ij}=\\frac{p_{i\\mid j}+p_{j\\mid i}}{2N}$$ $$p_{ij} = p_{i\\mid j} + p_{j\\mid i} - p_{i\\mid j}p_{j\\mid i}$$ $$\\text{Perplexity} = 2^{-\\sum_j p_{j\\mid i}\\log_2p_{j\\mid i}}$$ number of nearest neighbors $$k = 2^{\\sum_jp_{i\\mid j}}$$ probability in low-dimensional space $$q_{ij}=\\frac{\\left(1 + |y_i-y_j|^2\\right)^{-1}}{\\sum_{k\\neq i}\\left(1 + |y_i-y_k|^2\\right)^{-1}}$$ $$q_{ij} = \\left(1 + a(y_i-y_j)^{2b}\\right)^{-1}$$ loss function $\\mathrm{KL}(P_i|Q_i) = \\sum_i\\sum_jp_{j\\mid i}\\log\\frac{p_{j\\mid i}}{q_{j\\mid i}}$ $$\\mathrm{CE}(X,Y) = \\sum_i\\sum_j\\left[p_{ij}(X)\\log\\left(\\frac{p_{ij}(X)}{q_{ij}(Y)}\\right) + \\left(1-p_{ij}(X)\\right)\\log\\left(\\frac{1-p_{ij}(X)}{1-q_{ij}(Y)}\\right)\\right]$$ Optimization Gradient Descent Stochastic Gradient Descent Gradients $$\\frac{\\partial\\mathrm{KL}}{\\partial y_i} = 4\\sum_j(p_{ij}-q_{ij})(y_i-y_j)\\left(1 + |y_i-y_j|^2\\right)^{-1}$$ $$\\frac{\\partial\\mathrm{CE}}{\\partial y_i} = \\sum_j\\left \\frac{2abd_{ij}^{2(b-1)}P(X)}{1 + ad_{ij}^{2b}}-\\frac{2b(1-P(X))}{d_{ij}^2\\left(1 + ad_{ij}^{2b}\\right)}\\right $$ Initial low-dimensional coordinates Random Normal Initialization Graph Laplacian","title":"Decomposition"},{"location":"MachineLearning/Decomposition/#pca","text":"class kerasy . ML . decomposition . PCA ( n_components = None ) Consider a dataset ${\\mathbf{x}_n}$. If we consider a $D$-dimensional vector $\\mathbf{u}_1$, each data point $\\mathbf{x}_n\\in\\mathbb{R}^{D}$ is projected onto a scalar value $\\mathbf{u}_1^T\\mathbf{x}_n$. In that (one-dimensional) space, Mean: $$\\tilde{\\mathbf{x}} = \\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$$ Variance: $$\\frac{1}{N}\\sum_{n=1}^N\\left{\\mathbf{u} 1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\tilde{\\mathbf{x}}\\right}^2 = \\mathbf{u}_1^T\\mathbf{Su}_1\\quad\\mathbf{S}= \\frac{1}{N}\\sum {n=1}^N\\left(\\mathbf{x}-\\tilde{\\mathbf{x}}\\right)\\left(\\mathbf{x}-\\tilde{\\mathbf{x}}\\right)^T$$ We now maximize the projected variance $\\mathbf{u}_1^T\\mathbf{Su}_1\\quad\\mathbf{S}$ with respect to $\\mathbf{u}_1$, because we can think \"Larger variance\" \u2194\ufe0e \"Features are more expressed\" . To avoid $|\\mathbf{u}_1|\\rightarrow\\infty$, we introduce the normalization condition $\\mathbf{u}_1^T\\mathbf{u}_1=1$ using Lagrange multiplier $\\lambda_1$. Then, the object function is given by $$\\begin{aligned} L &= \\mathbf{u}_1^T\\mathbf{Su}_1 + \\lambda_1\\left(1-\\mathbf{u}_1^T\\mathbf{u}_1\\right)\\ \\frac{\\partial L}{\\partial \\mathbf{u}_1} &= 2\\mathbf{S}\\mathbf{u}_1 - 2\\lambda_1\\mathbf{u}_1 = 0\\ \\therefore\\mathbf{u}_1^T\\mathbf{Su}_1 &= \\lambda_1 \\quad (\\because \\text{left-multiplied $\\mathbf{u}_1^T$}) \\end{aligned}$$ The variance will be a maximum when we set $\\mathbf{u}_1$ eaual to the eigenvector having the largest eigenvalue $\\lambda_1$. This eigenvector is known as the first principal component. We can define additional principal components in the same way. Summary If we consider an M-dimensional projection space, the optimal linear projection is defined by the M eigenvectors u1,...,uM of the data covariance matrix S corresponding to the M largest eigenvalues \u03bb1,...,\u03bbM.","title":"PCA"},{"location":"MachineLearning/Decomposition/#kernel-pca","text":"class kerasy . ML . decomposition . KernelPCA ( n_components = None , kernel = \"gaussian\" , ** kernelargs ) The main idea is same with PCA, but we will perform it in the feature space , which implicitly defines a nonlinear principal component model in the original data space. If we assume that the projected data set has zero mean , The covariance matrix in feature space is given by $$ \\mathbf{C} = \\frac{1}{N}\\sum_{n=1}^N\\phi(\\mathbf{x}_n)\\phi(\\mathbf{x}_n)^T $$ and its eigenvector expansion is defined by $$\\begin{aligned} \\mathbf{C}\\mathbf{v} i &= \\lambda_i\\mathbf{v}_i\\ \\frac{1}{N}\\sum {n=1}^N\\phi(\\mathbf{x}_n)\\underset{\\text{scalar}}{\\left{\\phi(\\mathbf{x}_n)^T\\mathbf{v}_i\\right}} &= \\lambda_i\\mathbf{v}_i\\quad (\\ast)\\ \\end{aligned}$$ From this equation, we see that the vector $\\mathbf{v}_i$ is given by a linear combination of the $\\phi(\\mathbf{x}_n)$ $$\\mathbf{v} i = \\sum {n=1}^N a_{in}\\phi(\\mathbf{x}_n)$$ Substituting this expansion back into the eigenvector equation $(\\ast)$, we obtain $$\\begin{aligned} \\frac{1}{N}\\sum_{n=1}^N\\phi(\\mathbf{x_n})\\phi(\\mathbf{x} n)^T\\sum {m=1}^Na_{im}\\phi(\\mathbf{x} m) &= \\lambda_i\\sum {n=1}^Na_{in}\\phi(\\mathbf{x} n)\\ \\frac{1}{N}\\sum {n=1}^Nk(\\mathbf{x} l,\\mathbf{x}_n)\\sum {m=1}^Na_{i,m}k(\\mathbf{x} n,\\mathbf{x}_m) &= \\lambda_i\\sum {n=1}^Na_{in}k(\\mathbf{x}_l,\\mathbf{x}_n)\\quad(\\because\\text{multiplied $\\phi(\\mathbf{x}_l)^T$})\\ \\mathbf{K}^2\\mathbf{a}_i &= \\lambda_iN\\mathbf{Ka}_i\\quad (\\because\\text{written in matrix notation})\\ \\end{aligned}$$ We can find solutions for $\\mathbf{a}_i$ by solving the following eigenvalue problem. The variance in feature space will be a maximum when we set $\\mathbf{a}_1$ eaual to the eigenvector having the largest eigenvalue $\\lambda_i$. $$\\mathbf{Ka}_i = \\lambda_iN\\mathbf{a}_i$$ Having solved the eigenvector problem, the projection of a point $\\mathbf{x}$ onto eigenvector $i$ is given by $$y_i(\\mathbf{x}) = \\phi(\\mathbf{x})^T\\mathbf{v} i = \\sum {n=1}^Na_{in}\\phi(\\mathbf{x})^T\\phi(\\mathbf{x} n) = \\sum {n=1}^Na_{in}k(\\mathbf{x},\\mathbf{x}_n)$$ If projected data set doesn't have zero mean , the projected data points after centralizing are given by $$\\tilde{\\phi(\\mathbf{x} n)} = \\phi(\\mathbf{x}_n)-\\frac{1}{N}\\sum {l=1}^N\\phi(\\mathbf{x}_l)$$ and the corresponding elements of the Gram matrix are given by $$\\tilde{\\mathbf{K}} = \\mathbf{K} - \\mathbf{1_NK} - \\mathbf{K1_N} +\\mathbf{1_NK1_N}$$ where $\\mathbf{1}_N$ denotes the $N\\times N$ matrix in which every element takes the value $1/N$. Warning I don't understand clearly how to deal with new data point xn.","title":"Kernel PCA"},{"location":"MachineLearning/Decomposition/#sne","text":"Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between datapoints into conditional probabilities . The similarity of datapoint $\\mathbf{x} j$ to datapoint $\\mathbf{x}_i$ is conditional probability $p {j|i}$, that $\\mathbf{x}_i$ would pick $\\mathbf{x}_j$ as its neighbor if neighbors were picked in propotion to their probability density under Gaussian centered at $\\mathbf{x}_i$ . Mathmatically, the conditional probability $p_{j|i}$ is given by $$p_{j|i} = \\frac{\\exp\\left(-|\\mathbf{x} i-\\mathbf{x}_j|^2/2\\boldsymbol{\\sigma}_i^2\\right)}{\\sum {k\\neq i}\\exp\\left(-|\\mathbf{x}_i-\\mathbf{x}_k|^2/2\\boldsymbol{\\sigma}_i^2\\right)}$$ Because we are only interested in modeling pairwise similarities, we set the value of $p_{i|i}$ to zero. For the low-dimensional counterparts $\\mathbf{y} i$ and $\\mathbf{y}_j$ of the high-dimensional data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$, it is possible to compute a similar conditional probability, which we denote by $q {j|i}$ $$q_{j|i} = \\frac{\\exp\\left(-|\\mathbf{y} i-\\mathbf{y}_j|^2\\right)}{\\sum {k\\neq i}\\exp\\left(-|\\mathbf{y}_i-\\mathbf{y}_k|^2\\right)}$$ Again, we set $q_{i|i} = 0$, and the variance of the Gaussian to $\\frac{1}{\\sqrt{2}}$ Then, the cost function is given by $$C = \\sum_i\\mathrm{KL}(P_i|Q_i) = \\sum_i\\sum_j p_{j|i}\\log\\frac{p_{j|i}}{q_{j|i}}$$ in which $P_i$ represents the conditional probability distribution over all other data points given data points $\\mathbf{x}_i$ The remaining parameter to be selected is the variance $\\boldsymbol{\\sigma_i}$. It is defined to produce a $P_i$ with a fixed perplexity that is specified by the user. The perplexity is defined as $$Perp(P_i) = 2^{H(P_i)},\\quad H(P_i) = -\\sum_jp_{j|i}\\log_2p_{j|i}$$ The minimization of the cost function is performed using a gradient descent method. The gradient has a surprisingly simple form $$\\frac{\\partial C}{\\partial \\mathbf{y} i} = 2\\sum_j\\left(p {j|i} - q_{j|i} + p_{i|j} - q_{i|j}\\right)\\left(\\mathbf{y}_i-\\mathbf{y}_j\\right)$$","title":"SNE"},{"location":"MachineLearning/Decomposition/#tsne","text":"class kerasy . ML . decomposition . tSNE ( initial_momentum = 0.5 , final_momoentum = 0.8 , eta = 500 , min_gain = 0.1 , tol = 1e-5 , prec_max_iter = 50 ) As an alternative to minimizing the sum of the Kullback-Leibler divergence between a joint probability distribution $p_{j|i}$ and $q_{j|i}$, it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution: $$C = KL\\left(P|Q\\right) = \\sum_i\\sum_jp_{ij}\\log\\frac{p_{ij}}{q_{ij}}$$ In t-SNE, - Employ a Student t-distribution with one degree of freedom (which is the same as a Cauchy distribution) as the heavy-tailed distribution in the low-dimensional map. Using this distribution, the joint probabilities $q_{ij}$ are defined as $$q_{ij} = \\frac{\\left(1 + |\\mathbf{y} i-\\mathbf{y}_j|^2\\right)^{-1}}{\\sum {k\\neq l}\\left(1 + |\\mathbf{y} k-\\mathbf{y}_l|^2\\right)^{-1}}$$ - If there is an outlier $\\mathbf{x}_i$, low-dimensional map point $\\mathbf{y}_i$ has very little effect on the cost function, so set $$p {ij} = \\frac{p_{j|i} + p_{i|j}}{2n}$$ to ensure that $\\sum_jp_{ij}>\\frac{1}{2n}$ for all datapoints $\\mathbf{x}_i$ The gradient is given by $$\\frac{\\partial C}{\\partial \\mathbf{y} i} = 4\\sum_j\\left(p {ij} - q_{ij}\\right) \\left(\\mathbf{y}_i - \\mathbf{y}_j\\right)\\left(1 + |\\mathbf{y}_i - \\mathbf{y}_j|^2\\right)^{-1}$$ Reference Visualizing Data using t-SNE Pattern Recognition and Machine Learning by Christopher Bishop","title":"tSNE"},{"location":"MachineLearning/Decomposition/#umap","text":"class kerasy . ML . decomposition . UMAP ( metric = \"euclidean\" , metric_kwds = None , min_dist = 0.1 , a = None , b = None , random_state = None , sigma_iter = 20 , sigma_tol = 1e-5 , sigma_lower = 0 , sigma_upper = 1e3 ) UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. Reference UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction","title":"UMAP"},{"location":"MachineLearning/Decomposition/#tsne-vs-umap","text":"tSNE UMAP pure Machine Learning semi-empirical algorithm based on solid mathematical principles Preserves Only Local Structure Preserves Global Structure probability in high-dimensional space $$p_{j\\mid i} = \\frac{\\exp\\left(-|x_i-x_j|^2/2\\sigma_i^2\\right)}{\\sum_{k\\neq i}\\exp\\left(-|x_i-x_k|^2/2\\sigma_i^2\\right)}$$ $$p_{i\\mid j} = \\exp\\left(-\\frac{d(x_i,x_j)-\\rho_i}{\\sigma_i}\\right), \\quad \\rho_i = \\min_{j\\neq i}\\left{d(x_i,x_j)\\right}$$ joint probability in high-dimensional space $$p_{ij}=\\frac{p_{i\\mid j}+p_{j\\mid i}}{2N}$$ $$p_{ij} = p_{i\\mid j} + p_{j\\mid i} - p_{i\\mid j}p_{j\\mid i}$$ $$\\text{Perplexity} = 2^{-\\sum_j p_{j\\mid i}\\log_2p_{j\\mid i}}$$ number of nearest neighbors $$k = 2^{\\sum_jp_{i\\mid j}}$$ probability in low-dimensional space $$q_{ij}=\\frac{\\left(1 + |y_i-y_j|^2\\right)^{-1}}{\\sum_{k\\neq i}\\left(1 + |y_i-y_k|^2\\right)^{-1}}$$ $$q_{ij} = \\left(1 + a(y_i-y_j)^{2b}\\right)^{-1}$$ loss function $\\mathrm{KL}(P_i|Q_i) = \\sum_i\\sum_jp_{j\\mid i}\\log\\frac{p_{j\\mid i}}{q_{j\\mid i}}$ $$\\mathrm{CE}(X,Y) = \\sum_i\\sum_j\\left[p_{ij}(X)\\log\\left(\\frac{p_{ij}(X)}{q_{ij}(Y)}\\right) + \\left(1-p_{ij}(X)\\right)\\log\\left(\\frac{1-p_{ij}(X)}{1-q_{ij}(Y)}\\right)\\right]$$ Optimization Gradient Descent Stochastic Gradient Descent Gradients $$\\frac{\\partial\\mathrm{KL}}{\\partial y_i} = 4\\sum_j(p_{ij}-q_{ij})(y_i-y_j)\\left(1 + |y_i-y_j|^2\\right)^{-1}$$ $$\\frac{\\partial\\mathrm{CE}}{\\partial y_i} = \\sum_j\\left \\frac{2abd_{ij}^{2(b-1)}P(X)}{1 + ad_{ij}^{2b}}-\\frac{2b(1-P(X))}{d_{ij}^2\\left(1 + ad_{ij}^{2b}\\right)}\\right $$ Initial low-dimensional coordinates Random Normal Initialization Graph Laplacian","title":"tSNE vs. UMAP"},{"location":"MachineLearning/EM%20algorithm/","text":"Notebook Example Notebook: Kerasy.examples.EM.ipynb EM algorithm class kerasy . ML . EM . BaseEMmodel ( K , random_state = None ) KMeans class kerasy . ML . EM . KMeans ( K , random_state = None ) Gaussian Mixture model class kerasy . ML . EM . MixedGaussian ( K , random_state = None ) Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"EM algorithm"},{"location":"MachineLearning/EM%20algorithm/#em-algorithm","text":"class kerasy . ML . EM . BaseEMmodel ( K , random_state = None )","title":"EM algorithm"},{"location":"MachineLearning/EM%20algorithm/#kmeans","text":"class kerasy . ML . EM . KMeans ( K , random_state = None )","title":"KMeans"},{"location":"MachineLearning/EM%20algorithm/#gaussian-mixture-model","text":"class kerasy . ML . EM . MixedGaussian ( K , random_state = None ) Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Gaussian Mixture model"},{"location":"MachineLearning/HMM/","text":"Notebook Example Notebook: Kerasy.examples.HMM.ipynb The hidden Markov model (HMM) is a very powerful statistical method of characterizing the observed data samples of a discrete-time series. It's joint probability distribution over both latent and observed variables is then given by $$ p(\\mathbf{X}, \\mathbf{Z} | \\boldsymbol{\\theta})=p\\left(\\mathbf{z} {1} | \\boldsymbol{\\pi}\\right)\\left[\\prod {n=2}^{N} p\\left(\\mathbf{z} {n} | \\mathbf{z} {n-1}, \\mathbf{A}\\right)\\right] \\prod_{m=1}^{N} p\\left(\\mathbf{x} {m} | \\mathbf{z} {m}, \\boldsymbol{\\phi}\\right)\\qquad (13.10) $$ where $\\mathbf{Z}=\\left{z_1,\\ldots,z_N\\right}$ are the discrete latent variables, $\\mathbf{X}=\\left{x_1,\\ldots,x_N\\right}$ are the corresponding observations, and $\\boldsymbol{\\theta} = \\left{\\boldsymbol{\\pi},\\mathbf{A},\\boldsymbol{\\phi}\\right}$ denotes the set of parameters governing the model. Name Probability Conditional Distribution initial state $$\\pi_{k} \\equiv p\\left(z_{1 k}=1\\right)$$ $$p\\left(\\mathbf{z} {1} \\mid \\boldsymbol{\\pi}\\right)=\\prod {k=1}^{K} \\pi_{k}^{z_{1 k}}\\quad (13.8)$$ transition probability $$A_{j k} \\equiv p\\left(z_{n k}=1\\mid z_{n-1, j}=1\\right)$$ $$p\\left(\\mathbf{z} {n} \\mid \\mathbf{z} {n-1}, \\mathbf{A}\\right)=\\prod_{k=1}^{K} \\prod_{j=1}^{K} A_{j k}^{z_{n-1, j} z_{n k}}\\quad (13.7)$$ emission probability depend on your model. $$p\\left(\\mathbf{x} n\\mid\\mathbf{z}_n,\\boldsymbol{\\phi}\\right) = \\prod {k=1}^Kp\\left(\\mathbf{x} n\\mid\\phi_k\\right)^{z {nk}}\\quad (13.9)$$ If we have observed a data set $\\mathbf{X} = \\left{x_1,\\ldots,x_N\\right}$, we can determine the parameters of an HMM using maximum likelihood. The likelihood function is given by $$ p(\\mathbf{X} | \\boldsymbol{\\theta})=\\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} | \\boldsymbol{\\theta})\\qquad (13.11) $$ However, $\\sum_{\\mathbf{Z}}$ leads to complex expressions, so we turn to the expectation maximization (EM) algorithm . The EM algorithm starts with some initial selection for the model parameters, which we denote by $\\boldsymbol{\\theta}^{\\text{old}}$. In the E step, we take these parameter values and find the posterior distribution of the latent variables $p\\left(\\mathbf{Z} | \\mathbf{X}, \\boldsymbol{\\theta}^{\\text {old }}\\right)$. We then, use this posterior distribution to evaluate the expectation of the logarithm of the complete-data likelihood function , as a function of the parameters $\\boldsymbol{\\theta}$, to give the function $Q\\left(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{\\text{old}}\\right)$ defined by $$ Q\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text {old }}\\right)=\\sum_{\\mathbf{Z}} p\\left(\\mathbf{Z} | \\mathbf{X}, \\boldsymbol{\\theta}^{\\text {old }}\\right) \\ln p\\left(\\mathbf{X},\\mathbf{Z} | \\boldsymbol{\\theta}\\right)\\qquad (13.12) $$ At this point, it is convenient to introduce some notation. We shall use $\\gamma(\\mathbf{z} n)$ to denote the marginal posterior distribution of a latent variable $\\mathbf{z}_n$ , and $\\xi\\left(z {n-1, j}, z_{n k}\\right)$ to denote the joint posterior distribution of two successive latent variables , so that $$ \\begin{aligned} \\gamma\\left(\\mathbf{z} {n}\\right) &=p\\left(\\mathbf{z} {n} | \\mathbf{X}, \\boldsymbol{\\theta}^{\\text {old }}\\right) &(13.13)\\ \\xi\\left(\\mathbf{z} {n-1}, \\mathbf{z} {n}\\right) &=p\\left(\\mathbf{z} {n-1}, \\mathbf{z} {n} | \\mathbf{X}, \\boldsymbol{\\theta}^{\\text {old }}\\right) &(13.14) \\end{aligned} $$ If we substitute the joint distribution $p\\left(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta}\\right)$ given by $(13.10)$ into $(13.12)$, and make use of the definition of $\\gamma$ and $\\xi$, we obtain $$ \\begin{aligned} Q\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}\\right)=& \\sum_{k=1}^{K} \\gamma\\left(z_{1 k}\\right) \\ln \\pi_{k}+\\sum_{n=2}^{N} \\sum_{j=1}^{K} \\sum_{k=1}^{K} \\xi\\left(z_{n-1, j}, z_{n k}\\right) \\ln A_{j k} \\ &+\\sum_{n=1}^{N} \\sum_{k=1}^{K}\\gamma\\left(z_{n k}\\right) \\sum_{i=1}^Dx_{ni} \\ln \\phi_{i k} \\end{aligned}\\qquad (13.17) $$ The goal of the E step will be to evaluate the quantities $\\gamma\\left(\\mathbf{z} n\\right)$ and $\\xi\\left(\\mathbf{z} {n-1},\\mathbf{z}_n\\right)$ efficiently. (\u2192 forward-backward algorithm.) In the M step, we maximize $Q\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}\\right)$ with respect to the parameters $\\boldsymbol{\\theta} = \\left{\\boldsymbol{\\pi},\\mathbf{A},\\boldsymbol{\\phi}\\right}$ in which we treat $\\gamma\\left(\\mathbf{z} n\\right)$ and $\\xi\\left(\\mathbf{z} {n-1},\\mathbf{z} n\\right)$ as constant. Maximization with respect to $\\boldsymbol{\\pi}$ and $\\mathbf{A}$ is easily achieved using appropriate Lagrange multipliers with the results $$ \\begin{aligned} \\pi {k}&= \\frac{\\gamma\\left(z_{1 k}\\right)}{\\sum_{j=1}^{K} \\gamma\\left(z_{1 j}\\right)} & (13.18)\\ A_{j k}&= \\frac{\\sum_{n=2}^{N} \\xi\\left(z_{n-1, j}, z_{n k}\\right)}{\\sum_{l=1}^{K} \\sum_{n=2}^{N} \\xi\\left(z_{n-1, j}, z_{n l}\\right)} & (13.19)\\ \\end{aligned} $$ To maximize $Q\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}\\right)$ with respect to $\\phi_k$, we notice that only the final term in $(13.17)$ depends on $\\phi_k$, and the quantities $\\gamma\\left(z_{nk}\\right)$ are playing the role of the responsibilities. If the parameters $\\phi_k$ are independent for the different components, then this term decouples into a sum of terms one for each value $k$, each of which can be maximized independently. We are then simply maximizing the weighted log likelihood function for the emission density $p\\left(\\mathbf{x}|\\phi_k\\right)$ with weights $\\gamma\\left(z_{nk}\\right)$. Bernoulli HMM class kerasy . ML . HMM . BernoulliHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"ite\" , random_state = None ) For the case of discrete Bernoulli observed variables, the conditional distribution of the observations takes the form $$p\\left(x|\\mathbf{z}\\right) = \\prod_{k=1}^{K}\\left(\\theta_k^x\\left(1-\\theta_k\\right)^{1-x}\\right)^{z_k}$$ and the corresponding M-step equations are given by $$\\theta_k = \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_n}{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)}$$ Multinomial HMM class kerasy . ML . HMM . MultinomialHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"ite\" , random_state = None ) For the case of discrete multinomial observed variables, the conditional distribution of the observations takes the form $$p\\left(\\mathbf{x}|\\mathbf{z}\\right) = \\prod_{i=1}^D\\prod_{k=1}^{K}\\mu_{ik}^{x_iz_k}\\quad (13.22)$$ \u203b When $k$ is $2$ and, the multinomial distribution is the Bernoulli distribution. and the corresponding M-step equations are given by $$\\mu_{ik} = \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_{ni}}{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)}\\qquad (13.23)$$ Binomial HMM class kerasy . ML . HMM . BinomialHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"ite\" , random_state = None ) For the case of discrete binomial observed variables, the conditional distribution of the observations takes the form $$p\\left(x|n,\\mathbf{z}\\right) = \\prod_{k=1}^{K}\\left(\\begin{array}{l}n \\ x\\end{array}\\right) \\left(\\theta_k^x\\left(1-\\theta_k\\right)^{n-x}\\right)^{z_k}$$ \u203b A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. and the corresponding M-step equations are given by $$\\theta_k = \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_n}{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)n_n}$$ Gaussian HMM class kerasy . ML . HMM . GaussianHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"itmc\" , random_state = None , covariance_type = \"diag\" , min_covariance = 1e-3 ) Gaussian emission densities we have $p(\\mathbf{x}|\\boldsymbol{\\phi}_k) = \\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu_k},\\boldsymbol{\\Sigma_k}\\right)$, and maximization of the function $Q\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}\\right)$ then gives $$ \\begin{aligned} \\boldsymbol{\\mu_k} &= \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_n}{\\sum_{n=1}^{N}\\gamma\\left(z_{nk}\\right)} &(13.20)\\ \\boldsymbol{\\Sigma_k} &=\\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)\\left(\\mathbf{x} n-\\boldsymbol{\\mu}_k\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_k\\right)^T}{\\sum {n=1}^N\\gamma\\left(z_{nk}\\right)} &(13.21) \\end{aligned} $$ Covariance type definition Name example parameter size spherical Each state uses a single variance value that applies to all features. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma^{(k)} & & & 0 \\ & \\sigma^{(k)} & & \\ & & \\ddots & \\ 0 & & & \\sigma^{(k)} \\end{array}\\right)$$ nh * 1 diag Each state uses a diagonal covariance matrix. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma^{(k)} {1} & & & 0 \\ & \\sigma^{(k)} {2} & & \\ & & \\ddots & \\ 0 & & & \\sigma^{(k)}_{\\text{nf}} \\end{array}\\right)$$ nh * nf full Each state uses a full (i.e. unrestricted) covariance matrix. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma^{(k)} {1} & \\sigma^{(k)} {2} & \\cdots & \\sigma^{(k)} {\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} \\ \\sigma^{(k)} {2} & \\sigma^{(k)} {3} & \\cdots & \\sigma^{(k)} {\\frac{\\left(\\text{nf}-2\\right)\\cdot\\left(\\text{nf}-1\\right)}{2} + 1}\\ \\vdots & \\vdots & \\ddots & \\vdots\\ \\sigma^{(k)} {\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} & & & \\sigma^{(k)} {\\frac{\\text{nf}\\ast\\left(\\text{nf}+1\\right)}{2}} \\end{array}\\right)$$ nh * nf * ( nf + 1) // 2, tied All states use the same full covariance matrix. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma_{1} & \\sigma_{2} & \\cdots & \\sigma_{\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} \\ \\sigma_{2} & \\sigma_{3} & \\cdots & \\sigma_{\\frac{\\left(\\text{nf}-2\\right)\\cdot\\left(\\text{nf}-1\\right)}{2} + 1}\\ \\vdots & \\vdots & \\ddots & \\vdots\\ \\sigma_{\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} & & & \\sigma_{\\frac{\\text{nf}\\ast\\left(\\text{nf}+1\\right)}{2}} \\end{array}\\right)$$ nf * ( nf + 1) // 2 conversion function conversion functions are described at kerasy/utils/np_utils.py . def compress_based_on_covariance_type_from_tied_shape ( tied_cv , covariance_type , n_gaussian ): \"\"\" Create all the covariance matrices from a given template. \"\"\" def decompress_based_on_covariance_type ( covars , covariance_type = 'full' , n_gaussian = 1 , n_features = 1 ): \"\"\" Create the correct shape of covariance matris from each feature based on the covariance type. \"\"\" Gaussian Mixture HMM class kerasy . ML . HMM . GaussianMixtureHMM () Correspondence between code and formula code index formula log_cond_prob $$[n][k] = p\\left(\\mathbf{x} n\\mid z {nk}\\right)$$ $$p\\left(\\mathbf{X}\\mid\\mathbf{Z}\\right)$$ log_prob constant. $$\\begin{aligned}p\\left(\\mathbf{X}\\right) &=\\sum_{\\mathbf{z} n}\\alpha(\\mathbf{z}_n)\\beta(\\mathbf{z}_n)\\quad \\text{for all $\\mathbf{z}_n$}\\&=\\sum {\\mathbf{z}_N}\\alpha(\\mathbf{z}_N)\\end{aligned}$$ log_alpha $$[n][k] = \\alpha\\left(z_{nk}\\right)$$ $$\\alpha\\left(\\mathbf{z}_n\\right) \\equiv p\\left(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n,\\mathbf{z}_n\\right)$$ log_beta $$[n][k] = \\beta\\left(z_{nk}\\right)$$ $$\\beta\\left(\\mathbf{z} n\\right)\\equiv\\left(\\mathbf{x} {n+1},\\ldots,\\mathbf{x}_N\\mid\\mathbf{z}_n\\right)$$ posterior_prob $$[n][k] = \\gamma\\left(z_{nk}\\right)$$ $$\\begin{aligned}\\gamma\\left(\\mathbf{z}_n\\right) &= p\\left(\\mathbf{z}_n\\mid\\mathbf{X}\\right)= \\frac{p\\left(\\mathbf{z}_n,\\mathbf{X}\\right)}{p\\left(\\mathbf{X}\\right)} \\&= \\frac{\\alpha\\left(\\mathbf{z}_n\\right)\\beta\\left(\\mathbf{z}_n\\right)}{p\\left(\\mathbf{X}\\right)}\\end{aligned}$$ log_xi_sum $$[k][j]= \\sum_{n=2}^N\\xi\\left(z_{n-1,k},z_{n,j}\\right)$$ $$\\begin{aligned}\\xi\\left(\\mathbf{z} {n-1},\\mathbf{z}_n\\right) &= p\\left(\\mathbf{z} {n-1},\\mathbf{z} n\\mid\\mathbf{X}\\right) \\&= \\frac{\\alpha\\left(\\mathbf{z} {n-1}\\right)p\\left(\\mathbf{x} n\\mid\\mathbf{z}_n\\right)p\\left(\\mathbf{z}_n\\mid\\mathbf{z} {n-1}\\right)\\beta\\left(\\mathbf{z}_n\\right)}{p\\left(\\mathbf{X}\\right)}\\end{aligned}$$ Reference Pattern Recognition and Machine Learning by Christopher Bishop Spoken Language Processing: A Guide to Theory, Algorithm, and System Development","title":"HMM"},{"location":"MachineLearning/HMM/#bernoulli-hmm","text":"class kerasy . ML . HMM . BernoulliHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"ite\" , random_state = None ) For the case of discrete Bernoulli observed variables, the conditional distribution of the observations takes the form $$p\\left(x|\\mathbf{z}\\right) = \\prod_{k=1}^{K}\\left(\\theta_k^x\\left(1-\\theta_k\\right)^{1-x}\\right)^{z_k}$$ and the corresponding M-step equations are given by $$\\theta_k = \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_n}{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)}$$","title":"Bernoulli HMM"},{"location":"MachineLearning/HMM/#multinomial-hmm","text":"class kerasy . ML . HMM . MultinomialHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"ite\" , random_state = None ) For the case of discrete multinomial observed variables, the conditional distribution of the observations takes the form $$p\\left(\\mathbf{x}|\\mathbf{z}\\right) = \\prod_{i=1}^D\\prod_{k=1}^{K}\\mu_{ik}^{x_iz_k}\\quad (13.22)$$ \u203b When $k$ is $2$ and, the multinomial distribution is the Bernoulli distribution. and the corresponding M-step equations are given by $$\\mu_{ik} = \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_{ni}}{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)}\\qquad (13.23)$$","title":"Multinomial HMM"},{"location":"MachineLearning/HMM/#binomial-hmm","text":"class kerasy . ML . HMM . BinomialHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"ite\" , random_state = None ) For the case of discrete binomial observed variables, the conditional distribution of the observations takes the form $$p\\left(x|n,\\mathbf{z}\\right) = \\prod_{k=1}^{K}\\left(\\begin{array}{l}n \\ x\\end{array}\\right) \\left(\\theta_k^x\\left(1-\\theta_k\\right)^{n-x}\\right)^{z_k}$$ \u203b A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. and the corresponding M-step equations are given by $$\\theta_k = \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_n}{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)n_n}$$","title":"Binomial HMM"},{"location":"MachineLearning/HMM/#gaussian-hmm","text":"class kerasy . ML . HMM . GaussianHMM ( n_hstates = 3 , init = \"random\" , algorithm = \"viterbi\" , up_params = \"itmc\" , random_state = None , covariance_type = \"diag\" , min_covariance = 1e-3 ) Gaussian emission densities we have $p(\\mathbf{x}|\\boldsymbol{\\phi}_k) = \\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu_k},\\boldsymbol{\\Sigma_k}\\right)$, and maximization of the function $Q\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\mathrm{old}}\\right)$ then gives $$ \\begin{aligned} \\boldsymbol{\\mu_k} &= \\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)x_n}{\\sum_{n=1}^{N}\\gamma\\left(z_{nk}\\right)} &(13.20)\\ \\boldsymbol{\\Sigma_k} &=\\frac{\\sum_{n=1}^N\\gamma\\left(z_{nk}\\right)\\left(\\mathbf{x} n-\\boldsymbol{\\mu}_k\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_k\\right)^T}{\\sum {n=1}^N\\gamma\\left(z_{nk}\\right)} &(13.21) \\end{aligned} $$","title":"Gaussian HMM"},{"location":"MachineLearning/HMM/#definition","text":"Name example parameter size spherical Each state uses a single variance value that applies to all features. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma^{(k)} & & & 0 \\ & \\sigma^{(k)} & & \\ & & \\ddots & \\ 0 & & & \\sigma^{(k)} \\end{array}\\right)$$ nh * 1 diag Each state uses a diagonal covariance matrix. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma^{(k)} {1} & & & 0 \\ & \\sigma^{(k)} {2} & & \\ & & \\ddots & \\ 0 & & & \\sigma^{(k)}_{\\text{nf}} \\end{array}\\right)$$ nh * nf full Each state uses a full (i.e. unrestricted) covariance matrix. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma^{(k)} {1} & \\sigma^{(k)} {2} & \\cdots & \\sigma^{(k)} {\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} \\ \\sigma^{(k)} {2} & \\sigma^{(k)} {3} & \\cdots & \\sigma^{(k)} {\\frac{\\left(\\text{nf}-2\\right)\\cdot\\left(\\text{nf}-1\\right)}{2} + 1}\\ \\vdots & \\vdots & \\ddots & \\vdots\\ \\sigma^{(k)} {\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} & & & \\sigma^{(k)} {\\frac{\\text{nf}\\ast\\left(\\text{nf}+1\\right)}{2}} \\end{array}\\right)$$ nh * nf * ( nf + 1) // 2, tied All states use the same full covariance matrix. $$\\boldsymbol{\\Sigma_k} = \\left(\\begin{array}{cccc} \\sigma_{1} & \\sigma_{2} & \\cdots & \\sigma_{\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} \\ \\sigma_{2} & \\sigma_{3} & \\cdots & \\sigma_{\\frac{\\left(\\text{nf}-2\\right)\\cdot\\left(\\text{nf}-1\\right)}{2} + 1}\\ \\vdots & \\vdots & \\ddots & \\vdots\\ \\sigma_{\\frac{\\left(\\text{nf}-1\\right)\\cdot\\text{nf}}{2} + 1} & & & \\sigma_{\\frac{\\text{nf}\\ast\\left(\\text{nf}+1\\right)}{2}} \\end{array}\\right)$$ nf * ( nf + 1) // 2","title":"definition"},{"location":"MachineLearning/HMM/#conversion-function","text":"conversion functions are described at kerasy/utils/np_utils.py . def compress_based_on_covariance_type_from_tied_shape ( tied_cv , covariance_type , n_gaussian ): \"\"\" Create all the covariance matrices from a given template. \"\"\" def decompress_based_on_covariance_type ( covars , covariance_type = 'full' , n_gaussian = 1 , n_features = 1 ): \"\"\" Create the correct shape of covariance matris from each feature based on the covariance type. \"\"\"","title":"conversion function"},{"location":"MachineLearning/HMM/#gaussian-mixture-hmm","text":"class kerasy . ML . HMM . GaussianMixtureHMM ()","title":"Gaussian Mixture HMM"},{"location":"MachineLearning/HMM/#correspondence-between-code-and-formula","text":"code index formula log_cond_prob $$[n][k] = p\\left(\\mathbf{x} n\\mid z {nk}\\right)$$ $$p\\left(\\mathbf{X}\\mid\\mathbf{Z}\\right)$$ log_prob constant. $$\\begin{aligned}p\\left(\\mathbf{X}\\right) &=\\sum_{\\mathbf{z} n}\\alpha(\\mathbf{z}_n)\\beta(\\mathbf{z}_n)\\quad \\text{for all $\\mathbf{z}_n$}\\&=\\sum {\\mathbf{z}_N}\\alpha(\\mathbf{z}_N)\\end{aligned}$$ log_alpha $$[n][k] = \\alpha\\left(z_{nk}\\right)$$ $$\\alpha\\left(\\mathbf{z}_n\\right) \\equiv p\\left(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n,\\mathbf{z}_n\\right)$$ log_beta $$[n][k] = \\beta\\left(z_{nk}\\right)$$ $$\\beta\\left(\\mathbf{z} n\\right)\\equiv\\left(\\mathbf{x} {n+1},\\ldots,\\mathbf{x}_N\\mid\\mathbf{z}_n\\right)$$ posterior_prob $$[n][k] = \\gamma\\left(z_{nk}\\right)$$ $$\\begin{aligned}\\gamma\\left(\\mathbf{z}_n\\right) &= p\\left(\\mathbf{z}_n\\mid\\mathbf{X}\\right)= \\frac{p\\left(\\mathbf{z}_n,\\mathbf{X}\\right)}{p\\left(\\mathbf{X}\\right)} \\&= \\frac{\\alpha\\left(\\mathbf{z}_n\\right)\\beta\\left(\\mathbf{z}_n\\right)}{p\\left(\\mathbf{X}\\right)}\\end{aligned}$$ log_xi_sum $$[k][j]= \\sum_{n=2}^N\\xi\\left(z_{n-1,k},z_{n,j}\\right)$$ $$\\begin{aligned}\\xi\\left(\\mathbf{z} {n-1},\\mathbf{z}_n\\right) &= p\\left(\\mathbf{z} {n-1},\\mathbf{z} n\\mid\\mathbf{X}\\right) \\&= \\frac{\\alpha\\left(\\mathbf{z} {n-1}\\right)p\\left(\\mathbf{x} n\\mid\\mathbf{z}_n\\right)p\\left(\\mathbf{z}_n\\mid\\mathbf{z} {n-1}\\right)\\beta\\left(\\mathbf{z}_n\\right)}{p\\left(\\mathbf{X}\\right)}\\end{aligned}$$ Reference Pattern Recognition and Machine Learning by Christopher Bishop Spoken Language Processing: A Guide to Theory, Algorithm, and System Development","title":"Correspondence between code and formula"},{"location":"MachineLearning/Linear%20Regression/","text":"Notebook Example Notebook: Kerasy.examples.linear.ipynb Linear Regression class kerasy . ML . linear . LinearRegression ( basis = \"none\" , ** basisargs ) The simplest model for regression is $$y(\\mathbf{x},\\mathbf{w}) = w_0 + w_1x_1 + \\cdots + w_Dx_D$$ However, because of the linearity for the input variables $x_i$, this model has a poor expressive ability. Therefore, we extend it by considering linear combinations of fixed nonlinear functions of the input variables, of the form $$y(\\mathbf{x},\\mathbf{w}) = \\sum_{j=0}^{M-1}w_j\\phi_j(\\mathbf{x}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})$$ In terms of the target variable $t$, we assume that it is given by a $y(\\mathbf{x},\\mathbf{w})$ (deterministic) with additive Gaussian noise so that $$p(t|\\mathbf{x},\\mathbf{w},\\beta) = \\mathcal{N}\\left(t|y(\\mathbf{x},\\mathbf{w}),\\beta^{-1}\\right)$$ Therefore, if we observe data, we obtain the following expression for the likelihood function $$ \\begin{aligned} p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta) = \\prod_{n=1}^N\\mathcal{N}\\left(t_n|\\mathbf{w}^T\\phi(\\mathbf{x} n),\\beta^{-1}\\right)\\ \\begin{cases} \\begin{aligned} \\ln p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta) &=\\sum {n=1}^N\\ln\\mathcal{N}\\left(t_n|\\mathbf{w}^T\\phi(\\mathbf{x} n),\\beta^{-1}\\right)\\ &= \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln(2\\pi) - \\beta E_D(\\mathbf{w})\\ E_D(\\mathbf{w}) &= \\frac{1}{2}\\sum {n=1}^N\\left{t_n - \\mathbf{w}^T\\phi(\\mathbf{x}_n)\\right}^2 \\end{aligned} \\end{cases} \\end{aligned} $$ As we obtain such a likelihood, it is easy to maximize them respect to $\\mathbf{w}$ and $\\beta$. (Setting the gradient to zero.) $$ \\begin{aligned} \\nabla\\ln p(\\mathbf{t}|\\mathbf{w},\\beta) &= \\sum_{n=1}^N\\left{t_n - \\mathbf{w}^T\\phi(\\mathbf{x} n)\\right}\\phi(\\mathbf{x}_n)^T = 0\\ 0 &= \\sum {n=1}^Nt_n\\phi(\\mathbf{x} n)^T - \\mathbf{w}^T\\left(\\sum {n=1}^N\\phi(\\mathbf{x} n)\\phi(\\mathbf{x}_n)^T\\right)\\ \\therefore\\mathbf{w} {\\text{ML}} &= \\left(\\Phi^T\\Phi\\right)^{-1}\\Phi^T\\mathbf{t}\\ \\frac{1}{\\beta_{\\text{ML}}} &= \\frac{1}{N}\\sum_{n=1}^N\\left{t_n -\\mathbf{w}_{\\text{ML}}^T\\phi(\\mathbf{x}_n)\\right}^2 \\end{aligned} $$ $\\Phi$ is an $N\\times M$ matrix, called the design matrix , whose elements are given by $\\Phi_{nj} = \\phi_j(\\mathbf{x}_n)$ The quantity $\\Phi^{\\dagger}\\equiv\\left(\\Phi^T\\Phi\\right)^{-1}\\Phi^T$ is known as the Moore-Penrose pseudo-inverse of the matrix $\\Phi$ Linear Regression (Ridge, L2) class kerasy . ML . linear . LinearRegressionRidge ( lambda_ , basis = \"none\" , ** basisargs ) As you see at the notebook , it is likely to over-fit especially when model is too complicated. ($M\\rightarrow\\text{Large}$) For avoiding it, we introduce the idea of adding a regularization term to an error function. $$E_D(\\mathbf{w}) + \\lambda E_{\\mathbf{w}}(\\mathbf{w})$$ where $\\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\\mathbf{w})$ and the regularization term $E_{\\mathbf{w}}(\\mathbf{w})$. One of the simplest forms of regularizer is given by $$E_{\\mathbf{w}}(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}$$ Then, the total error function and Maximum likelihood estimated $\\mathbf{w}$ is expressed as $$ \\frac{1}{2}\\sum_{n=1}^N\\left{t_n- \\mathbf{w}^T\\phi(\\mathbf{x} n)\\right}^2 + \\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\\ \\mathbf{w} {\\text{ML}} = \\left(\\lambda\\mathbf{I} + \\Phi^T\\Phi\\right)^{-1}\\Phi^T\\mathbf{t} $$ Linear Regression (LASSO, L1) class kerasy . ML . linear . LinearRegressionLASSO ( lambda_ , basis = \"none\" , ** basisargs ) Consider the regularizer given by $$E_{\\mathbf{w}}(\\mathbf{w}) = |\\mathbf{w}|$$ This function is not differentiable, so we use ADMM(Alternating Direction Method of Multipliers) . In this algorithm, we use Extended Lagrange multiplier . $$\\begin{aligned} L_{\\rho}\\left(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\alpha}\\right) &= \\frac{1}{2}|\\mathbf{y} - \\mathbf{Xw}|^2 + \\lambda|\\mathbf{z}| + \\boldsymbol{\\alpha}^T\\left(\\mathbf{w}-\\mathbf{z}\\right) + \\frac{\\rho}{2}|\\mathbf{w}-\\mathbf{z}|^2\\ \\text{subject to }\\mathbf{w} &= \\mathbf{z} \\end{aligned}$$ We minimize $L_{\\rho}$ respect to $\\mathbf{w}$ and $\\mathbf{z}$ repeatedly . $$ \\begin{cases} \\begin{aligned} \\mathbf{w}&\\leftarrow\\underset{\\mathbf{w}}{\\text{argmin}}L_{\\rho}\\left(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\alpha}\\right)\\text{ with fixed with $\\mathbf{z},\\boldsymbol{\\alpha}$}\\ &=\\left(\\mathbf{X}^T\\mathbf{X} + \\rho\\mathbf{I}\\right)^{-1}\\left(\\mathbf{X}^T\\mathbf{y} - \\boldsymbol{\\alpha} + \\rho\\mathbf{z}\\right)\\ \\mathbf{z}&\\leftarrow\\underset{\\mathbf{z}}{\\text{argmin}}L_{\\rho}\\left(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\alpha}\\right)\\text{ with fixed with $\\mathbf{w},\\boldsymbol{\\alpha}$}\\ &= \\text{prox} {\\frac{\\lambda}{\\rho}|\\ast|}\\left(w_i + \\frac{\\alpha_i}{\\rho}\\right)\\ \\boldsymbol{\\alpha}&\\leftarrow\\boldsymbol{\\alpha} + \\rho\\left(\\mathbf{w}-\\mathbf{z}\\right) \\end{aligned} \\end{cases}\\ \\text{prox} {c|\\ast|}(z_0) := \\underset{z}{\\text{argmin}}\\left{c|z| + \\frac{1}{2}\\left(z-z_0\\right)^2\\right} = \\begin{cases}z_0-c & (c < z_0)\\0 & (-c\\leq z_0 \\leq c)\\z_0 + c & (z_0 < -c)\\end{cases} $$ A more general regularizer is sometimes used, for which the regularized error takes the form $$\\frac{1}{2}\\sum_{n=1}^N\\left{t_n-\\mathbf{w}^T\\phi(\\mathbf{x} n)\\right}^2 + \\frac{\\lambda}{2}\\sum {j=1}^M|w_j|^q$$ Bayesian Linear Regression class kerasy . ML . linear . BayesianLinearRegression ( alpha = 1 , beta = 25 , basis = \"none\" , ** basisargs ) We turn to a Bayesian treatment of linear regression, which will avoid the over-fitting problem of maximum likelihood, and which will also lead to automatic methods of determining model complexity using the training data alone. (Not requiring Hold-out methods ) We assume that the prior distribution of $\\mathbf{w}$ is given by a Gaussian distribution of the form $$p(\\mathbf{w}|\\alpha) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{0},\\alpha^{-1}\\mathbf{I}\\right)$$ As I mentioned before, likelihood is given by $$p(t|\\mathbf{x},\\mathbf{w},\\beta) = \\mathcal{N}\\left(t|y(\\mathbf{x},\\mathbf{w}),\\beta^{-1}\\right)\\ p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta) = \\prod_{n=1}^N\\mathcal{N}\\left(t_n|\\mathbf{w}^T\\phi(\\mathbf{x}_n),\\beta^{-1}\\right)$$ From the Bayes' theorem, posterior distribution is proportional to the product of prior distribution and likelihood . Therefore, posterior distribution is described as $$p(\\mathbf{w}|\\mathbf{t}) = \\mathcal{N}\\left(\\mathbf{m}_N,\\mathbf{S}_N\\right)\\ \\begin{cases}\\mathbf{m}_N &= \\beta\\mathbf{S}_N\\Phi^T\\mathbf{t}\\\\mathbf{S}_N^{-1} &= \\alpha\\mathbf{I} + \\beta\\Phi^T\\Phi\\end{cases}$$ Finally, the predictive distribution defined by: $$\\begin{aligned} p(t|\\mathbf{t},\\alpha,\\beta) &= \\int p(t|\\mathbf{w},\\beta)p(\\mathbf{w}|\\mathbf{t},\\alpha,\\beta)d\\mathbf{w}\\ &= \\mathcal{N}\\left(t|\\mathbf{m}_N^T\\phi(\\mathbf{x}),\\sigma^2_N(\\mathbf{x})\\right)\\ \\sigma^2_N(\\mathbf{x}) &= \\frac{1}{\\beta} + \\phi(\\mathbf{x})^T\\mathbf{S}\\phi(\\mathbf{x}) \\end{aligned}$$ Reference If you want to know the derivation process (calculation process) in detail, please visit here . Evidence Approximation Bayesian Regression class kerasy . ML . linear . EvidenceApproxBayesianRegression ( alpha = 1 , beta = 1 , basis = \"none\" , ** basisargs ) In a fully Bayesian treatment of the linear basis function model, we would introduce prior distributions over the hyperparameters $\\alpha$, and $\\beta$. $$ \\begin{aligned} p(t|\\mathbf{t},\\mathbf{X},\\mathbf{x},\\alpha,\\beta) &= \\int p(t|\\mathbf{w},\\mathbf{x},\\beta)p(\\mathbf{w}|\\mathbf{t},\\mathbf{X},\\alpha,\\beta)d\\mathbf{w}\\ &\\Rightarrow \\int p(t|\\mathbf{w},\\mathbf{x},\\beta)p(\\mathbf{w}|\\mathbf{t},\\mathbf{X},\\alpha,\\beta)p(\\alpha,\\beta|\\mathbf{t},\\mathbf{X})d\\mathbf{w}d\\alpha d\\beta \\quad (\\ast) \\end{aligned} $$ Marginalize with respect to these hyperparameters as well as with respect to the parameters $\\mathbf{w}$ to make predictions. As, the complete marginalization over all of these variables is analytically intractable , We will maximize $(\\ast)$ in line with the framework of empirical Bayes (or type 2 maximum likelihood , generalized maximum likelihood , evidence approximation ) In the framework, we repeat the following process. Obtain the marginal likelihood function by first integrating over the parameters $\\mathbf{w}$ $$p(\\mathbf{t}|\\alpha,\\beta) = \\int p(\\mathbf{t}|\\mathbf{w},\\mathbf{X},\\beta)p(\\mathbf{w}|\\alpha)d\\mathbf{w}$$ Maximize $p(\\mathbf{t}|\\alpha,\\beta)$ with respect to $\\alpha$ and $\\beta$. $$ \\begin{cases} \\begin{aligned} \\gamma &= \\sum_{i}\\frac{\\lambda_i}{\\lambda_i + \\alpha^{\\text{old}}}\\ \\alpha^{\\text{new}} &= \\frac{\\gamma}{\\mathbf{m} N^T\\mathbf{m}_N}\\ \\frac{1}{\\beta^{\\text{new}}} &= \\frac{1}{N-\\gamma}\\sum {n=1}^N\\left{t_n - \\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\right}^2 \\end{aligned} \\end{cases} $$ Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Linear Regression"},{"location":"MachineLearning/Linear%20Regression/#linear-regression","text":"class kerasy . ML . linear . LinearRegression ( basis = \"none\" , ** basisargs ) The simplest model for regression is $$y(\\mathbf{x},\\mathbf{w}) = w_0 + w_1x_1 + \\cdots + w_Dx_D$$ However, because of the linearity for the input variables $x_i$, this model has a poor expressive ability. Therefore, we extend it by considering linear combinations of fixed nonlinear functions of the input variables, of the form $$y(\\mathbf{x},\\mathbf{w}) = \\sum_{j=0}^{M-1}w_j\\phi_j(\\mathbf{x}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})$$ In terms of the target variable $t$, we assume that it is given by a $y(\\mathbf{x},\\mathbf{w})$ (deterministic) with additive Gaussian noise so that $$p(t|\\mathbf{x},\\mathbf{w},\\beta) = \\mathcal{N}\\left(t|y(\\mathbf{x},\\mathbf{w}),\\beta^{-1}\\right)$$ Therefore, if we observe data, we obtain the following expression for the likelihood function $$ \\begin{aligned} p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta) = \\prod_{n=1}^N\\mathcal{N}\\left(t_n|\\mathbf{w}^T\\phi(\\mathbf{x} n),\\beta^{-1}\\right)\\ \\begin{cases} \\begin{aligned} \\ln p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta) &=\\sum {n=1}^N\\ln\\mathcal{N}\\left(t_n|\\mathbf{w}^T\\phi(\\mathbf{x} n),\\beta^{-1}\\right)\\ &= \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln(2\\pi) - \\beta E_D(\\mathbf{w})\\ E_D(\\mathbf{w}) &= \\frac{1}{2}\\sum {n=1}^N\\left{t_n - \\mathbf{w}^T\\phi(\\mathbf{x}_n)\\right}^2 \\end{aligned} \\end{cases} \\end{aligned} $$ As we obtain such a likelihood, it is easy to maximize them respect to $\\mathbf{w}$ and $\\beta$. (Setting the gradient to zero.) $$ \\begin{aligned} \\nabla\\ln p(\\mathbf{t}|\\mathbf{w},\\beta) &= \\sum_{n=1}^N\\left{t_n - \\mathbf{w}^T\\phi(\\mathbf{x} n)\\right}\\phi(\\mathbf{x}_n)^T = 0\\ 0 &= \\sum {n=1}^Nt_n\\phi(\\mathbf{x} n)^T - \\mathbf{w}^T\\left(\\sum {n=1}^N\\phi(\\mathbf{x} n)\\phi(\\mathbf{x}_n)^T\\right)\\ \\therefore\\mathbf{w} {\\text{ML}} &= \\left(\\Phi^T\\Phi\\right)^{-1}\\Phi^T\\mathbf{t}\\ \\frac{1}{\\beta_{\\text{ML}}} &= \\frac{1}{N}\\sum_{n=1}^N\\left{t_n -\\mathbf{w}_{\\text{ML}}^T\\phi(\\mathbf{x}_n)\\right}^2 \\end{aligned} $$ $\\Phi$ is an $N\\times M$ matrix, called the design matrix , whose elements are given by $\\Phi_{nj} = \\phi_j(\\mathbf{x}_n)$ The quantity $\\Phi^{\\dagger}\\equiv\\left(\\Phi^T\\Phi\\right)^{-1}\\Phi^T$ is known as the Moore-Penrose pseudo-inverse of the matrix $\\Phi$","title":"Linear Regression"},{"location":"MachineLearning/Linear%20Regression/#linear-regression-ridge-l2","text":"class kerasy . ML . linear . LinearRegressionRidge ( lambda_ , basis = \"none\" , ** basisargs ) As you see at the notebook , it is likely to over-fit especially when model is too complicated. ($M\\rightarrow\\text{Large}$) For avoiding it, we introduce the idea of adding a regularization term to an error function. $$E_D(\\mathbf{w}) + \\lambda E_{\\mathbf{w}}(\\mathbf{w})$$ where $\\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\\mathbf{w})$ and the regularization term $E_{\\mathbf{w}}(\\mathbf{w})$. One of the simplest forms of regularizer is given by $$E_{\\mathbf{w}}(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}$$ Then, the total error function and Maximum likelihood estimated $\\mathbf{w}$ is expressed as $$ \\frac{1}{2}\\sum_{n=1}^N\\left{t_n- \\mathbf{w}^T\\phi(\\mathbf{x} n)\\right}^2 + \\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\\ \\mathbf{w} {\\text{ML}} = \\left(\\lambda\\mathbf{I} + \\Phi^T\\Phi\\right)^{-1}\\Phi^T\\mathbf{t} $$","title":"Linear Regression (Ridge, L2)"},{"location":"MachineLearning/Linear%20Regression/#linear-regression-lasso-l1","text":"class kerasy . ML . linear . LinearRegressionLASSO ( lambda_ , basis = \"none\" , ** basisargs ) Consider the regularizer given by $$E_{\\mathbf{w}}(\\mathbf{w}) = |\\mathbf{w}|$$ This function is not differentiable, so we use ADMM(Alternating Direction Method of Multipliers) . In this algorithm, we use Extended Lagrange multiplier . $$\\begin{aligned} L_{\\rho}\\left(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\alpha}\\right) &= \\frac{1}{2}|\\mathbf{y} - \\mathbf{Xw}|^2 + \\lambda|\\mathbf{z}| + \\boldsymbol{\\alpha}^T\\left(\\mathbf{w}-\\mathbf{z}\\right) + \\frac{\\rho}{2}|\\mathbf{w}-\\mathbf{z}|^2\\ \\text{subject to }\\mathbf{w} &= \\mathbf{z} \\end{aligned}$$ We minimize $L_{\\rho}$ respect to $\\mathbf{w}$ and $\\mathbf{z}$ repeatedly . $$ \\begin{cases} \\begin{aligned} \\mathbf{w}&\\leftarrow\\underset{\\mathbf{w}}{\\text{argmin}}L_{\\rho}\\left(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\alpha}\\right)\\text{ with fixed with $\\mathbf{z},\\boldsymbol{\\alpha}$}\\ &=\\left(\\mathbf{X}^T\\mathbf{X} + \\rho\\mathbf{I}\\right)^{-1}\\left(\\mathbf{X}^T\\mathbf{y} - \\boldsymbol{\\alpha} + \\rho\\mathbf{z}\\right)\\ \\mathbf{z}&\\leftarrow\\underset{\\mathbf{z}}{\\text{argmin}}L_{\\rho}\\left(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\alpha}\\right)\\text{ with fixed with $\\mathbf{w},\\boldsymbol{\\alpha}$}\\ &= \\text{prox} {\\frac{\\lambda}{\\rho}|\\ast|}\\left(w_i + \\frac{\\alpha_i}{\\rho}\\right)\\ \\boldsymbol{\\alpha}&\\leftarrow\\boldsymbol{\\alpha} + \\rho\\left(\\mathbf{w}-\\mathbf{z}\\right) \\end{aligned} \\end{cases}\\ \\text{prox} {c|\\ast|}(z_0) := \\underset{z}{\\text{argmin}}\\left{c|z| + \\frac{1}{2}\\left(z-z_0\\right)^2\\right} = \\begin{cases}z_0-c & (c < z_0)\\0 & (-c\\leq z_0 \\leq c)\\z_0 + c & (z_0 < -c)\\end{cases} $$ A more general regularizer is sometimes used, for which the regularized error takes the form $$\\frac{1}{2}\\sum_{n=1}^N\\left{t_n-\\mathbf{w}^T\\phi(\\mathbf{x} n)\\right}^2 + \\frac{\\lambda}{2}\\sum {j=1}^M|w_j|^q$$","title":"Linear Regression (LASSO, L1)"},{"location":"MachineLearning/Linear%20Regression/#bayesian-linear-regression","text":"class kerasy . ML . linear . BayesianLinearRegression ( alpha = 1 , beta = 25 , basis = \"none\" , ** basisargs ) We turn to a Bayesian treatment of linear regression, which will avoid the over-fitting problem of maximum likelihood, and which will also lead to automatic methods of determining model complexity using the training data alone. (Not requiring Hold-out methods ) We assume that the prior distribution of $\\mathbf{w}$ is given by a Gaussian distribution of the form $$p(\\mathbf{w}|\\alpha) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{0},\\alpha^{-1}\\mathbf{I}\\right)$$ As I mentioned before, likelihood is given by $$p(t|\\mathbf{x},\\mathbf{w},\\beta) = \\mathcal{N}\\left(t|y(\\mathbf{x},\\mathbf{w}),\\beta^{-1}\\right)\\ p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta) = \\prod_{n=1}^N\\mathcal{N}\\left(t_n|\\mathbf{w}^T\\phi(\\mathbf{x}_n),\\beta^{-1}\\right)$$ From the Bayes' theorem, posterior distribution is proportional to the product of prior distribution and likelihood . Therefore, posterior distribution is described as $$p(\\mathbf{w}|\\mathbf{t}) = \\mathcal{N}\\left(\\mathbf{m}_N,\\mathbf{S}_N\\right)\\ \\begin{cases}\\mathbf{m}_N &= \\beta\\mathbf{S}_N\\Phi^T\\mathbf{t}\\\\mathbf{S}_N^{-1} &= \\alpha\\mathbf{I} + \\beta\\Phi^T\\Phi\\end{cases}$$ Finally, the predictive distribution defined by: $$\\begin{aligned} p(t|\\mathbf{t},\\alpha,\\beta) &= \\int p(t|\\mathbf{w},\\beta)p(\\mathbf{w}|\\mathbf{t},\\alpha,\\beta)d\\mathbf{w}\\ &= \\mathcal{N}\\left(t|\\mathbf{m}_N^T\\phi(\\mathbf{x}),\\sigma^2_N(\\mathbf{x})\\right)\\ \\sigma^2_N(\\mathbf{x}) &= \\frac{1}{\\beta} + \\phi(\\mathbf{x})^T\\mathbf{S}\\phi(\\mathbf{x}) \\end{aligned}$$ Reference If you want to know the derivation process (calculation process) in detail, please visit here .","title":"Bayesian Linear Regression"},{"location":"MachineLearning/Linear%20Regression/#evidence-approximation-bayesian-regression","text":"class kerasy . ML . linear . EvidenceApproxBayesianRegression ( alpha = 1 , beta = 1 , basis = \"none\" , ** basisargs ) In a fully Bayesian treatment of the linear basis function model, we would introduce prior distributions over the hyperparameters $\\alpha$, and $\\beta$. $$ \\begin{aligned} p(t|\\mathbf{t},\\mathbf{X},\\mathbf{x},\\alpha,\\beta) &= \\int p(t|\\mathbf{w},\\mathbf{x},\\beta)p(\\mathbf{w}|\\mathbf{t},\\mathbf{X},\\alpha,\\beta)d\\mathbf{w}\\ &\\Rightarrow \\int p(t|\\mathbf{w},\\mathbf{x},\\beta)p(\\mathbf{w}|\\mathbf{t},\\mathbf{X},\\alpha,\\beta)p(\\alpha,\\beta|\\mathbf{t},\\mathbf{X})d\\mathbf{w}d\\alpha d\\beta \\quad (\\ast) \\end{aligned} $$ Marginalize with respect to these hyperparameters as well as with respect to the parameters $\\mathbf{w}$ to make predictions. As, the complete marginalization over all of these variables is analytically intractable , We will maximize $(\\ast)$ in line with the framework of empirical Bayes (or type 2 maximum likelihood , generalized maximum likelihood , evidence approximation ) In the framework, we repeat the following process. Obtain the marginal likelihood function by first integrating over the parameters $\\mathbf{w}$ $$p(\\mathbf{t}|\\alpha,\\beta) = \\int p(\\mathbf{t}|\\mathbf{w},\\mathbf{X},\\beta)p(\\mathbf{w}|\\alpha)d\\mathbf{w}$$ Maximize $p(\\mathbf{t}|\\alpha,\\beta)$ with respect to $\\alpha$ and $\\beta$. $$ \\begin{cases} \\begin{aligned} \\gamma &= \\sum_{i}\\frac{\\lambda_i}{\\lambda_i + \\alpha^{\\text{old}}}\\ \\alpha^{\\text{new}} &= \\frac{\\gamma}{\\mathbf{m} N^T\\mathbf{m}_N}\\ \\frac{1}{\\beta^{\\text{new}}} &= \\frac{1}{N-\\gamma}\\sum {n=1}^N\\left{t_n - \\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\right}^2 \\end{aligned} \\end{cases} $$ Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Evidence Approximation Bayesian Regression"},{"location":"MachineLearning/Support%20Vector%20Machine/","text":"Notebook Example Notebook: Kerasy.examples.svm.ipynb Hard SVC class kerasy . ML . svm . hardSVC ( kernel = \"gaussian\" , ** kernelargs ) Start from the \"two-class classification\" problem using \"linear models\" , which \"separate the training data set linearly\" . Under the assumption, we can denote The training data: $$D = \\left{(t_n,\\mathbf{x}_n)\\in \\left{-1, 1\\right} \\times \\mathbb{R}^m | n = 1,\\ldots, N\\right}$$ linear models: $$y(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x}) + b$$ For all training data points: $$t_ny(\\mathbf{x}_n) > 0$$ Margin is given by: $$\\frac{1}{|\\mathbf{w}|}$$ \u203b If there are any uncertain points, please check here . Points In support vector machines, the decision boundary is chosen to be the one for which the margin is maximized. From this discussion above, the optimization problem (Minimizing) is given by $$L\\left(\\mathbf{w},b,\\mathbf{a}\\right) = \\frac{1}{2}|\\mathbf{w}|^2 - \\sum_{n=1}^Na_n\\left{t_n\\left(\\mathbf{w}^T\\phi(\\mathbf{x}_n) + b\\right) -1 \\right}$$ where $\\mathbf{a} = \\left(a_1,\\ldots,a_N\\right)^T$ are the Lagrange multipliers $a_n\\gg0$. Warning Note the minus sign in front of the Lagrange multiplier term. Check HERE . To solve this optimization problem, set the derivatives of $L\\left(\\mathbf{w},b,\\mathbf{a}\\right)$ with respect to $\\mathbf{w}$ and $b$ equal to zero. $$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{w}} &= \\mathbf{w}-\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x} n) =0 &\\therefore \\mathbf{w}^{\\ast}=\\sum {n=1}^N a_nt_n\\phi(\\mathbf{x} n)\\ \\frac{\\partial L}{\\partial b} &= -\\sum {n=1}^N a_nt_n = 0 &\\therefore\\sum_{n=1}^Na_n^{\\ast}t_n = 0\\ \\end{aligned} $$ Eliminating $\\mathbf{w}$ and $b$ from $L\\left(\\mathbf{w},b,\\mathbf{a}\\right)$ using these conditions then gives the dual representation of the maximum margin problem in which we maximize $$ \\begin{aligned} \\tilde{L}\\left(\\mathbf{a}\\right) &= \\frac{1}{2}|\\mathbf{w}|^2 - \\mathbf{w}^T\\sum_{n=1}^Na_nt_n\\phi(\\mathbf{x} n) - b\\sum {n=1}^Na_nt_n + \\sum_{n=1}^Na_n\\ &= -\\frac{1}{2}|\\mathbf{w}|^2 - b\\cdot0 + \\sum_{n=1}^Na_n\\ &= \\sum_{n=1}^Na_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^Na_na_mt_nt_mk\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)\\quad k\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)=\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\end{aligned} $$ with respect to $\\mathbf{a}$ subject to the constraints $$ \\begin{aligned} a_n & \\geq 0,\\quad n=1,\\ldots,N\\ \\sum_{n=1}^Na_nt_n &= 0 \\end{aligned} $$ This takes the form of a quadratic programming problems , and there are some well-known algorithms to solve them. (ex. SMO; Sequential Minimal Optimization ) Once optimal $\\mathbf{a}$ are calculated, the decision boundary is given by $$y(\\mathbf{x}) = \\mathbf{w}^{\\ast T}\\phi(\\mathbf{x})+b = \\sum_{n=1}^N a_nt_nk(\\mathbf{x},\\mathbf{x}_n) + b$$ In terms of bias parameter $b$, we can calculate it by noting that any support vector $\\mathbf{x}_n$ satisfies $t_ny(\\mathbf{x}_n) = 1$. $$ \\begin{aligned} &t_n\\left(\\sum_{m\\in\\mathcal{S}}a_mt_mk(\\mathbf{x} n,\\mathbf{x}_m) + b\\right) = 1\\ &\\therefore b^{\\ast}= \\frac{1}{N {\\mathcal{S}}}\\sum_{n\\in\\mathcal{S}}\\left(t_n-\\sum_{n\\in\\mathcal{S}}a_mt_mk(\\mathbf{x}_n,\\mathbf{x}_m)\\right) \\end{aligned} $$ Points To minimize the numerical error, it is more appropriate to average these equation over all support vectors instead of choosing an arbitrarily support vector. Soft SVC class kerasy . ML . svm . SVC ( kernel = \"gaussian\" , C = 10 , ** kernelargs ) In the Hard SVC , we have assumed that the training data points are linearly separable in the feature space $\\phi(\\mathbf{x})$ . However, in practice, this property may cause the bad effect on the generalization performance . Therefore, we introduce slack variables, $\\xi_n\\gg0$ to allow some of the training points to be misclassified. $$ t_ny(\\mathbf{x}_n) \\geq 1 \\Longrightarrow t_ny(\\mathbf{x}_n) \\geq 1 - \\xi_n $$ Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary $$C\\sum_{n=1}^N\\xi_n + \\frac{1}{2}|\\mathbf{w}|$$ $C$ controls the trade-off between minimizing training errors and controlling model complexity. The corresponding Lagrangian is given by $$L(\\mathbf{w},b,\\boldsymbol{\\xi},\\mathbf{a},\\boldsymbol{\\mu}) = \\frac{1}{2}|\\mathbf{w}|^2 + C\\sum_{n=1}^N\\xi_n - \\sum_{n=1}^{N}a_n\\left{t_n\\left(\\mathbf{w}^T\\phi(\\mathbf{x} n) + b\\right) - 1 + \\xi_n\\right} - \\sum {n=1}^N\\mu_n\\xi_n$$ Same as before, we set the derivatives of $L\\left(\\mathbf{w},b,\\mathbf{a}\\right)$ with respect to $\\mathbf{w}, b$ and ${\\xi_n}$ equal to zero. $$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{w}} &= \\mathbf{w}-\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x} n) =0 &\\therefore \\mathbf{w}^{\\ast}=\\sum {n=1}^N a_nt_n\\phi(\\mathbf{x} n)\\ \\frac{\\partial L}{\\partial b} &= -\\sum {n=1}^N a_nt_n = 0 &\\therefore\\sum_{n=1}^Na_n^{\\ast}t_n = 0\\ \\frac{\\partial L}{\\partial \\xi_n} &= C-a_n-\\mu_n &\\therefore a_n = C-\\mu_n \\end{aligned} $$ Eliminating them from $L$ using these conditions then gives the dual representation of the maximum margin problem in which we maximize $$ \\begin{aligned} \\tilde{L}\\left(\\mathbf{a}\\right) &= \\frac{1}{2}|\\mathbf{w}|^2 +C\\sum_{n=1}^N\\xi_n - |\\mathbf{w}|^2 - b\\sum_{n=1}^Na_nt_n + \\sum_{n=1}^Na_n - \\sum_{n=1}^Na_n\\xi_n - \\sum_{n=1}^N\\mu_n\\xi_n\\ &= -\\frac{1}{2}|\\mathbf{w}|^2 - b\\cdot0 + \\sum_{n=1}^Na_n + \\sum_{n=1}^N\\left(C - a_n - \\mu_n\\right)\\xi_n\\ &= \\sum_{n=1}^Na_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^Na_na_mt_nt_mk\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)\\quad k\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)=\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\end{aligned} $$ with respect to $\\mathbf{a}$ subject to the constraints $$ \\begin{aligned} &a_n \\geq 0,\\mu_n\\geq 0 \\quad n=1,\\ldots,N\\ &\\therefore 0\\leq a_n \\leq C\\ &\\sum_{n=1}^Na_nt_n = 0 \\end{aligned} $$ In this case, As shown in the table below, we consider only the data points having $0 < a_n < C$ to calculate the biasparameter $b$ $a_n$ $\\mu_n$ $\\xi_n$ Meaning $a_n=0$ They do not contribute to the predictive model. $0 < a_n < C$ $\\mu_n>0$ $\\xi_n=0$ satisfy the $t_ny(\\mathbf{x}_n) = 1$ $a_n=C$ $\\mu_n=0$ $\\xi_n>0$ lie inside the margin and can either be correctly classified if $\\xi\\ll1$ or misclassified if $\\xi_n>1$ Multiclass SVMs class kerasy . ML . svm . MultipleSVM ( kernel = \"gaussian\" , C = 10 , ** kernelargs ) The support vector machine is fundamentally a two-class classifier, but in practice, we often face the multiple class problems. In that cases, we have to combine the multiple two-class SVMs. Various methods have therefore been proposed for combining, and one commonly used approach (this method is also used in Kerasy) is to construct $K$ separate SVMs, in which the $k^{\\text{th}}$ model $y_k(\\mathbf{x})$ is trained to distinguish whether $\\mathbf{x}$ belongs to class $k$ or not, and final decision is given by $$y(\\mathbf{x}) = \\underset{k}{\\max}y_k(\\mathbf{x})$$ This is known as the one-versus-the-rest approach. Warning This heuristic approach suffers from some problems: There is no guarantee that the real-valued quantities yk(x) for different classifiers will have appropriate scales. The training sets are imbalanced. Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Support Vector Machine"},{"location":"MachineLearning/Support%20Vector%20Machine/#hard-svc","text":"class kerasy . ML . svm . hardSVC ( kernel = \"gaussian\" , ** kernelargs ) Start from the \"two-class classification\" problem using \"linear models\" , which \"separate the training data set linearly\" . Under the assumption, we can denote The training data: $$D = \\left{(t_n,\\mathbf{x}_n)\\in \\left{-1, 1\\right} \\times \\mathbb{R}^m | n = 1,\\ldots, N\\right}$$ linear models: $$y(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x}) + b$$ For all training data points: $$t_ny(\\mathbf{x}_n) > 0$$ Margin is given by: $$\\frac{1}{|\\mathbf{w}|}$$ \u203b If there are any uncertain points, please check here . Points In support vector machines, the decision boundary is chosen to be the one for which the margin is maximized. From this discussion above, the optimization problem (Minimizing) is given by $$L\\left(\\mathbf{w},b,\\mathbf{a}\\right) = \\frac{1}{2}|\\mathbf{w}|^2 - \\sum_{n=1}^Na_n\\left{t_n\\left(\\mathbf{w}^T\\phi(\\mathbf{x}_n) + b\\right) -1 \\right}$$ where $\\mathbf{a} = \\left(a_1,\\ldots,a_N\\right)^T$ are the Lagrange multipliers $a_n\\gg0$. Warning Note the minus sign in front of the Lagrange multiplier term. Check HERE . To solve this optimization problem, set the derivatives of $L\\left(\\mathbf{w},b,\\mathbf{a}\\right)$ with respect to $\\mathbf{w}$ and $b$ equal to zero. $$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{w}} &= \\mathbf{w}-\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x} n) =0 &\\therefore \\mathbf{w}^{\\ast}=\\sum {n=1}^N a_nt_n\\phi(\\mathbf{x} n)\\ \\frac{\\partial L}{\\partial b} &= -\\sum {n=1}^N a_nt_n = 0 &\\therefore\\sum_{n=1}^Na_n^{\\ast}t_n = 0\\ \\end{aligned} $$ Eliminating $\\mathbf{w}$ and $b$ from $L\\left(\\mathbf{w},b,\\mathbf{a}\\right)$ using these conditions then gives the dual representation of the maximum margin problem in which we maximize $$ \\begin{aligned} \\tilde{L}\\left(\\mathbf{a}\\right) &= \\frac{1}{2}|\\mathbf{w}|^2 - \\mathbf{w}^T\\sum_{n=1}^Na_nt_n\\phi(\\mathbf{x} n) - b\\sum {n=1}^Na_nt_n + \\sum_{n=1}^Na_n\\ &= -\\frac{1}{2}|\\mathbf{w}|^2 - b\\cdot0 + \\sum_{n=1}^Na_n\\ &= \\sum_{n=1}^Na_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^Na_na_mt_nt_mk\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)\\quad k\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)=\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\end{aligned} $$ with respect to $\\mathbf{a}$ subject to the constraints $$ \\begin{aligned} a_n & \\geq 0,\\quad n=1,\\ldots,N\\ \\sum_{n=1}^Na_nt_n &= 0 \\end{aligned} $$ This takes the form of a quadratic programming problems , and there are some well-known algorithms to solve them. (ex. SMO; Sequential Minimal Optimization ) Once optimal $\\mathbf{a}$ are calculated, the decision boundary is given by $$y(\\mathbf{x}) = \\mathbf{w}^{\\ast T}\\phi(\\mathbf{x})+b = \\sum_{n=1}^N a_nt_nk(\\mathbf{x},\\mathbf{x}_n) + b$$ In terms of bias parameter $b$, we can calculate it by noting that any support vector $\\mathbf{x}_n$ satisfies $t_ny(\\mathbf{x}_n) = 1$. $$ \\begin{aligned} &t_n\\left(\\sum_{m\\in\\mathcal{S}}a_mt_mk(\\mathbf{x} n,\\mathbf{x}_m) + b\\right) = 1\\ &\\therefore b^{\\ast}= \\frac{1}{N {\\mathcal{S}}}\\sum_{n\\in\\mathcal{S}}\\left(t_n-\\sum_{n\\in\\mathcal{S}}a_mt_mk(\\mathbf{x}_n,\\mathbf{x}_m)\\right) \\end{aligned} $$ Points To minimize the numerical error, it is more appropriate to average these equation over all support vectors instead of choosing an arbitrarily support vector.","title":"Hard SVC"},{"location":"MachineLearning/Support%20Vector%20Machine/#soft-svc","text":"class kerasy . ML . svm . SVC ( kernel = \"gaussian\" , C = 10 , ** kernelargs ) In the Hard SVC , we have assumed that the training data points are linearly separable in the feature space $\\phi(\\mathbf{x})$ . However, in practice, this property may cause the bad effect on the generalization performance . Therefore, we introduce slack variables, $\\xi_n\\gg0$ to allow some of the training points to be misclassified. $$ t_ny(\\mathbf{x}_n) \\geq 1 \\Longrightarrow t_ny(\\mathbf{x}_n) \\geq 1 - \\xi_n $$ Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary $$C\\sum_{n=1}^N\\xi_n + \\frac{1}{2}|\\mathbf{w}|$$ $C$ controls the trade-off between minimizing training errors and controlling model complexity. The corresponding Lagrangian is given by $$L(\\mathbf{w},b,\\boldsymbol{\\xi},\\mathbf{a},\\boldsymbol{\\mu}) = \\frac{1}{2}|\\mathbf{w}|^2 + C\\sum_{n=1}^N\\xi_n - \\sum_{n=1}^{N}a_n\\left{t_n\\left(\\mathbf{w}^T\\phi(\\mathbf{x} n) + b\\right) - 1 + \\xi_n\\right} - \\sum {n=1}^N\\mu_n\\xi_n$$ Same as before, we set the derivatives of $L\\left(\\mathbf{w},b,\\mathbf{a}\\right)$ with respect to $\\mathbf{w}, b$ and ${\\xi_n}$ equal to zero. $$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{w}} &= \\mathbf{w}-\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x} n) =0 &\\therefore \\mathbf{w}^{\\ast}=\\sum {n=1}^N a_nt_n\\phi(\\mathbf{x} n)\\ \\frac{\\partial L}{\\partial b} &= -\\sum {n=1}^N a_nt_n = 0 &\\therefore\\sum_{n=1}^Na_n^{\\ast}t_n = 0\\ \\frac{\\partial L}{\\partial \\xi_n} &= C-a_n-\\mu_n &\\therefore a_n = C-\\mu_n \\end{aligned} $$ Eliminating them from $L$ using these conditions then gives the dual representation of the maximum margin problem in which we maximize $$ \\begin{aligned} \\tilde{L}\\left(\\mathbf{a}\\right) &= \\frac{1}{2}|\\mathbf{w}|^2 +C\\sum_{n=1}^N\\xi_n - |\\mathbf{w}|^2 - b\\sum_{n=1}^Na_nt_n + \\sum_{n=1}^Na_n - \\sum_{n=1}^Na_n\\xi_n - \\sum_{n=1}^N\\mu_n\\xi_n\\ &= -\\frac{1}{2}|\\mathbf{w}|^2 - b\\cdot0 + \\sum_{n=1}^Na_n + \\sum_{n=1}^N\\left(C - a_n - \\mu_n\\right)\\xi_n\\ &= \\sum_{n=1}^Na_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^Na_na_mt_nt_mk\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)\\quad k\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)=\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\end{aligned} $$ with respect to $\\mathbf{a}$ subject to the constraints $$ \\begin{aligned} &a_n \\geq 0,\\mu_n\\geq 0 \\quad n=1,\\ldots,N\\ &\\therefore 0\\leq a_n \\leq C\\ &\\sum_{n=1}^Na_nt_n = 0 \\end{aligned} $$ In this case, As shown in the table below, we consider only the data points having $0 < a_n < C$ to calculate the biasparameter $b$ $a_n$ $\\mu_n$ $\\xi_n$ Meaning $a_n=0$ They do not contribute to the predictive model. $0 < a_n < C$ $\\mu_n>0$ $\\xi_n=0$ satisfy the $t_ny(\\mathbf{x}_n) = 1$ $a_n=C$ $\\mu_n=0$ $\\xi_n>0$ lie inside the margin and can either be correctly classified if $\\xi\\ll1$ or misclassified if $\\xi_n>1$","title":"Soft SVC"},{"location":"MachineLearning/Support%20Vector%20Machine/#multiclass-svms","text":"class kerasy . ML . svm . MultipleSVM ( kernel = \"gaussian\" , C = 10 , ** kernelargs ) The support vector machine is fundamentally a two-class classifier, but in practice, we often face the multiple class problems. In that cases, we have to combine the multiple two-class SVMs. Various methods have therefore been proposed for combining, and one commonly used approach (this method is also used in Kerasy) is to construct $K$ separate SVMs, in which the $k^{\\text{th}}$ model $y_k(\\mathbf{x})$ is trained to distinguish whether $\\mathbf{x}$ belongs to class $k$ or not, and final decision is given by $$y(\\mathbf{x}) = \\underset{k}{\\max}y_k(\\mathbf{x})$$ This is known as the one-versus-the-rest approach. Warning This heuristic approach suffers from some problems: There is no guarantee that the real-valued quantities yk(x) for different classifiers will have appropriate scales. The training sets are imbalanced. Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Multiclass SVMs"},{"location":"MachineLearning/Tree/","text":"Notebook Example Notebook: Kerasy.examples.tree.ipynb Decision Tree class kerasy . ML . tree . DecisionTreeClassifier ( criterion = \"gini\" , max_depth = None , random_state = None ) There are various simple , but widely used, models that work by partitioning the input space into cuboid regions , whose edges are aligned with the axes , and assigning a simple model to each region. In this models, one specific model is responsible for making predictions at any given point in input space. Within each region, there is an independent model to predict the target variable. Regression : simply predict a constant over each region Classification : assign each region to a specific class. In order to learn such a model from a training set, we have to determine the structure of the tree about How many edges ? (When to stop adding nodes?) How to chose the variable and threshold ? One simple approach is minimizing the objective function $C(T)$ $$C(T) = \\sum_{\\tau=1}^{|T|}Q_{\\tau}(T) + \\lambda|T|$$ $\\lambda$ determines the trade-off . $|T|$ is the number of leaf nodes. $Q(T)$ is the loss functions like: Residual Sum-of-Squares (Regression): $$Q_{\\tau}(T) = \\sum_{\\mathbf{x} n\\in\\mathcal{R} {\\tau}}\\left{t_n-y_{\\tau}\\right}^2$$ Cross-Entropy (Classification): $$Q_{\\tau}(T) = \\sum_{k=1}^Kp_{\\tau k}\\ln p_{\\tau k}$$ Gini index (Classification): $$Q_{\\tau}(T) = \\sum_{k=1}^Kp_{\\tau k}\\left(1 - p_{\\tau k}\\right)$$ \u203b $p_{\\tau k}$ is the proportion of data points in region $\\mathcal{R}_{\\tau}$ assigned to class $k$. Strength Human interpretability. Does not require the preprocessing (Scaling, Standardization...) Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Tree"},{"location":"MachineLearning/Tree/#decision-tree","text":"class kerasy . ML . tree . DecisionTreeClassifier ( criterion = \"gini\" , max_depth = None , random_state = None ) There are various simple , but widely used, models that work by partitioning the input space into cuboid regions , whose edges are aligned with the axes , and assigning a simple model to each region. In this models, one specific model is responsible for making predictions at any given point in input space. Within each region, there is an independent model to predict the target variable. Regression : simply predict a constant over each region Classification : assign each region to a specific class. In order to learn such a model from a training set, we have to determine the structure of the tree about How many edges ? (When to stop adding nodes?) How to chose the variable and threshold ? One simple approach is minimizing the objective function $C(T)$ $$C(T) = \\sum_{\\tau=1}^{|T|}Q_{\\tau}(T) + \\lambda|T|$$ $\\lambda$ determines the trade-off . $|T|$ is the number of leaf nodes. $Q(T)$ is the loss functions like: Residual Sum-of-Squares (Regression): $$Q_{\\tau}(T) = \\sum_{\\mathbf{x} n\\in\\mathcal{R} {\\tau}}\\left{t_n-y_{\\tau}\\right}^2$$ Cross-Entropy (Classification): $$Q_{\\tau}(T) = \\sum_{k=1}^Kp_{\\tau k}\\ln p_{\\tau k}$$ Gini index (Classification): $$Q_{\\tau}(T) = \\sum_{k=1}^Kp_{\\tau k}\\left(1 - p_{\\tau k}\\right)$$ \u203b $p_{\\tau k}$ is the proportion of data points in region $\\mathcal{R}_{\\tau}$ assigned to class $k$. Strength Human interpretability. Does not require the preprocessing (Scaling, Standardization...) Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Decision Tree"},{"location":"MachineLearning/sampling/","text":"Notebook Example Notebook: Kerasy.examples.sampling.ipynb Rejection Sampling class kerasy . ML . sampling . RejectionSampler ( p , q , domain , k = 1 , ** qargs ) Metropolis-Hastings class kerasy . ML . sampling . MHSampler ( p , q , ** qargs ) GibbsMsphereSampler class kerasy . ML . sampling . GibbsMsphereSampler ( M , r = 1 ) Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"Sampling"},{"location":"MachineLearning/sampling/#rejection-sampling","text":"class kerasy . ML . sampling . RejectionSampler ( p , q , domain , k = 1 , ** qargs )","title":"Rejection Sampling"},{"location":"MachineLearning/sampling/#metropolis-hastings","text":"class kerasy . ML . sampling . MHSampler ( p , q , ** qargs )","title":"Metropolis-Hastings"},{"location":"MachineLearning/sampling/#gibbsmspheresampler","text":"class kerasy . ML . sampling . GibbsMsphereSampler ( M , r = 1 ) Reference Pattern Recognition and Machine Learning by Christopher Bishop","title":"GibbsMsphereSampler"}]}