{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Kerasy Kerasy: The Python Deep Learning library \"Keras\" is a high-level neural networks API, written in Python and capable of running on top of TensorFlow , CNTK , or Theano . It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Danger This is not Keras which is well known deep learning framework, so if you want to enjoy deep learning, visit Keras . Why I made Kerasy I study deep learning, and machine learning theory at the University, and have practical experience in the internships, hackathons, and competitions. Through those experiences, I felt I wanted to go further, next step, building my original model , which is basically different from the conventional one (NN, CNN, RNN, ..., back propagation series.) On the other hand, I thought it is extremely important to understand the conventional one properly, so by building the framework on my own, I tried to confirm their understanding. As it is too hard to building the deep learning framework from scratch, I decided to imitate the Keras , one of the most popular frameworks. Future prospects I am particularly interested in \"synthetic biology\" (redesigning, or remodeling organisms by engineering them to have new abilities) and \"neuroscience\" (Back-engineering the flexibility of brain structure concerned with mechanism of memory, pattern recognition, and so on) . Therefore, I want to study deeply about them and apply them for Human augmentation purpose to create the more wonderful world.","title":"Home"},{"location":"#welcome-to-kerasy","text":"Kerasy: The Python Deep Learning library \"Keras\" is a high-level neural networks API, written in Python and capable of running on top of TensorFlow , CNTK , or Theano . It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Danger This is not Keras which is well known deep learning framework, so if you want to enjoy deep learning, visit Keras .","title":"Welcome to Kerasy"},{"location":"#why-i-made-kerasy","text":"I study deep learning, and machine learning theory at the University, and have practical experience in the internships, hackathons, and competitions. Through those experiences, I felt I wanted to go further, next step, building my original model , which is basically different from the conventional one (NN, CNN, RNN, ..., back propagation series.) On the other hand, I thought it is extremely important to understand the conventional one properly, so by building the framework on my own, I tried to confirm their understanding. As it is too hard to building the deep learning framework from scratch, I decided to imitate the Keras , one of the most popular frameworks.","title":"Why I made Kerasy"},{"location":"#future-prospects","text":"I am particularly interested in \"synthetic biology\" (redesigning, or remodeling organisms by engineering them to have new abilities) and \"neuroscience\" (Back-engineering the flexibility of brain structure concerned with mechanism of memory, pattern recognition, and so on) . Therefore, I want to study deeply about them and apply them for Human augmentation purpose to create the more wonderful world.","title":"Future prospects"},{"location":"DeepLearning/CNN/","text":"CNN 2019-08-30(Fri) DeepLearning CNN\u306f\u3001\u753b\u50cf\u8a8d\u8b58\u3084\u97f3\u58f0\u8a8d\u8b58\u306a\u3069\u306e\u5206\u91ce\u3067\u9ad8\u3044\u7cbe\u5ea6\u3092\u8a87\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e00\u3064\u3067\u3059\u3002 Convolution Layer\uff08\u7573\u307f\u8fbc\u307f\u5c64\uff09 \u00b6 \u3053\u3053\u3067\u306f\u3001 \u7573\u307f\u8fbc\u307f\u6f14\u7b97\u306e\u4e2d\u8eab \u3068 \u8aa4\u5dee\u9006\u4f1d\u64ad\u306e\u5f0f\u306e\u5c0e\u51fa \u306e\u307f\u3092\u884c\u3044\u307e\u3059\u3002 \u300c\u7573\u307f\u8fbc\u307f\u6f14\u7b97\u306b\u3088\u3063\u3066\u3069\u306e\u3088\u3046\u306a\u6027\u80fd\u304c\u671f\u5f85\u3067\u304d\u308b\u304b\u300d \u306a\u3069\u306b\u3064\u3044\u3066\u306f\u3001 \u3053\u3053 \u3067\u5b9f\u6f14\u3092\u8e0f\u307e\u3048\u3066\u793a\u3057\u3066\u3044\u307e\u3059\u3002 Mono Multi forward \u00b6 $$ \\begin{cases} \\begin{aligned} a_{i,j,c'}^{{k+1}} &= \\sum_c\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}w_{m,n,c,c'}^{k+1}z_{i+m,j+n,c}^{k} + b_{c'}^{k+1}\\\\ z_{i,j,c'}^{{k}} &= h^{k}\\left(a_{i,j,c'}^{{k}}\\right) \\end{aligned} \\end{cases} $$ backprop \u00b6 \u753b\u50cf\u3092\u8a73\u7d30\u306b\u898b\u308b\u3002 $w_{m,n,c,c'}^k, b_{c'}^k$ \u00b6 $$ \\begin{aligned} \\frac{\\partial E}{\\partial w_{m,n,c,c'}^{k+1}} &= \\sum_{i}\\sum_{j}\\frac{\\partial E}{\\partial a_{i,j,c'}^{k+1}}\\frac{\\partial a_{i,j,c'}^{k+1}}{\\partial w_{m,n,c,c'}^{k+1}}\\\\ &= \\sum_{i}\\sum_{j}\\frac{\\partial E}{\\partial a_{i,j,c'}^{k+1}}z_{i+m,j+n,c}^{k}\\\\ &= \\sum_{i}\\sum_{j}\\delta_{i,j,c'}^{k+1}\\cdot z_{i+m,j+n,c}^{k}\\\\ \\frac{\\partial E}{\\partial b_{c'}^{k+1}} &= \\sum_{i}\\sum_{j}\\delta_{i,j,c'}^{k+1} \\end{aligned} $$ $\\delta_{i,j,c}^k$ \u00b6 $$ \\begin{aligned} \\delta_{i,j,c}^{k} &= \\frac{\\partial E}{\\partial a_{i,j,c}^{k}} \\\\ &= \\sum_{c'}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}\\left(\\frac{\\partial E}{\\partial a_{i-m,j-n,c'}^{k+1}}\\right)\\left(\\frac{\\partial a_{i-m,j-n,c'}^{k+1}}{\\partial a_{i,j,c}^k}\\right)\\\\ &= \\sum_{c'}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1} \\left(\\delta_{i-m,j-n,c'}^{k+1}\\right)\\left(w_{m,n,c,c'}^{k+1}h'\\left(a_{i,j,c}^k\\right)\\right) \\\\ &= h'\\left(a_{i,j,c}^k\\right)\\sum_{c'}\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1} \\delta_{i-m,j-n,c'}^{k+1}\\cdot w_{m,n,c,c'}^{k+1} \\end{aligned} $$ Pooling Layer\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\uff09 \u00b6 Pooling Layer\u306b\u306f\u3001 Max \u30fb Avg \u30fb Global-Max \u30fb Global-Avg \u306a\u3069\u306e\u7a2e\u985e\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u3053\u3067\u306f\u7c21\u5358\u306e\u305f\u3081\u306b Max-Pooling\u306e\u9806\u4f1d\u64ad\u30fb\u9006\u4f1d\u64ad\u8a08\u7b97 \u306e\u306b\u3064\u3044\u3066\u306e\u307f\u7d39\u4ecb\u3057\u3001\u305d\u308c\u4ee5\u5916\u306b\u95a2\u3057\u3066\u306f \u3053\u3053 \u3067\u5b9f\u6f14\u3092\u8e0f\u307e\u3048\u3066\u793a\u3057\u3066\u3044\u307e\u3059\u3002 forward \u00b6 backprop \u00b6 MNIST \u00b6 MNIST\u306f\u3001\u753b\u50cf\u51e6\u7406\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3088\u304f\u4f7f\u308f\u308c\u308b\u30bf\u30b9\u30af\u3067\u3001\u4ee5\u4e0b\u306e\u624b\u66f8\u304d\u6570\u5b57\uff08$0\\sim9$\uff09\u3092\u30e2\u30c7\u30eb\u306b\u8a8d\u8b58\u3055\u305b\u308b\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u7c21\u5358\u306e\u305f\u3081\u306b\u3001\u6570\u5b57\u306e $0$ \u3092\u9069\u5f53\u306b\u5b66\u7fd2\u3055\u305b\u307e\u3059\u3002 \u203b \u30e2\u30c7\u30eb\u304c\u521d\u3081\u304b\u3089 $0$ \u3060\u3068\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3068\u5b66\u7fd2\u306e\u6d41\u308c\u304c\u898b\u3089\u308c\u306a\u3044\u306e\u3067\u3001\u4fbf\u5b9c\u4e0a\u3001 \u6700\u3082\u4e88\u6e2c\u78ba\u7387\u306e\u4f4e\u3044\u3082\u306e\u3092\u6b63\u89e3\u30e9\u30d9\u30eb \u3068\u3057\u307e\u3059\u3002 0 1 2 3 4 5 6 7 8 9 In [1]: import cv2 import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm In [2]: from kerasy.layers.convolutional import Conv2D from kerasy.layers.pool import MaxPooling2D from kerasy.layers.core import Input , Dense , Flatten from kerasy.engine.sequential import Sequential In [3]: cv_path = \"/Users/iwasakishuto/Github/portfolio/Kerasy/doc/theme/img/MNIST-sample/0.png\" image = np . expand_dims ( cv2 . imread ( cv_path , 0 ), axis = 2 ) / 255 In [4]: plt . imshow ( cv2 . cvtColor ( cv2 . imread ( cv_path ), cv2 . COLOR_BGR2RGB )) plt . xticks ([]), plt . yticks ([]), plt . title ( \"Sample Training Data.\" ) plt . show () In [5]: model = Sequential () model . add ( Input ( input_shape = ( 28 , 28 , 1 ))) model . add ( Conv2D ( filters = 32 , kernel_size = ( 3 , 3 ), activation = 'relu' )) model . add ( Conv2D ( filters = 64 , kernel_size = ( 3 , 3 ), activation = 'relu' )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 128 , activation = 'tanh' )) model . add ( Dense ( 10 , activation = 'softmax' )) In [6]: model . compile ( optimizer = 'sgd' , loss = \"categorical_crossentropy\" ) In [7]: for layer in model . layers : print ( f \"== {layer.name} ==\" ) print ( f \"output shape: {layer.output_shape} \" ) print ( f \"kernel shape: {layer._losses.get('kernel', np.zeros(shape=())).shape}\" ) print ( f \"bias shape : {layer._losses.get('bias', np.zeros(shape=())).shape}\" ) == input_1 == output shape: (28, 28, 1) kernel shape: () bias shape : () == conv2d_1 == output shape: (26, 26, 32) kernel shape: (3, 3, 1, 32) bias shape : (32,) == conv2d_2 == output shape: (24, 24, 64) kernel shape: (3, 3, 32, 64) bias shape : (64,) == maxpooling2d_1 == output shape: (12, 12, 64) kernel shape: () bias shape : () == flatten_1 == output shape: (9216,) kernel shape: () bias shape : () == dense_1 == output shape: (128,) kernel shape: (128, 9216) bias shape : (128, 1) == dense_2 == output shape: (10,) kernel shape: (10, 128) bias shape : (10, 1) In [8]: # #=== Train Only Dense Layer === # model.layers[1].trainable = False # model.layers[2].trainable = False In [9]: # #=== Train Only Convolutional Layer === model . layers [ - 1 ] . trainable = False model . layers [ - 2 ] . trainable = False In [10]: x_train = np . expand_dims ( image , axis = 0 ) In [11]: original_pred = model . predict ( x_train ) print ( f \"original prediction: {np.argmax(original_pred)} \\n {original_pred} \" ) original prediction: 8 [[0.09992267 0.10031274 0.10041982 0.09772118 0.09986997 0.10006854 0.10004782 0.09852888 0.10165813 0.10145026]] In [12]: ans_label = np . argmin ( original_pred ) print ( f \"ans_label: {ans_label} \" ) ans_label: 3 In [13]: y_true = np . zeros ( shape = ( 10 ,)) y_true [ ans_label ] = 1 In [14]: preds = [] for _ in range ( 100 ): y_pred = model . forward ( x_train [ 0 ]) model . backprop ( y_true , y_pred ) pred = np . argmax ( y_pred ) model . updates ( 1 ) preds . append ([ y_pred [ ans_label ], pred ]) if ans_label == pred : break In [15]: prob , label = np . array ( preds ) . T In [16]: counts = np . arange ( len ( prob )) plt . plot ( counts , prob , color = \"black\" , alpha = 0.3 ) for l in np . unique ( label ): ix = np . where ( label == l ) plt . scatter ( counts [ ix ], prob [ ix ], color = cm . hsv ( float ( l ) / 10 ), label = l ) plt . legend (), plt . grid () plt . show () In [17]: y_pred Out[17]: array([0.09997384, 0.0999851 , 0.09999513, 0.10001753, 0.10000801, 0.10000499, 0.10001656, 0.10000084, 0.09998462, 0.10001338]) In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"CNN"},{"location":"DeepLearning/ComputationalGraph/","text":"Computational Graph 2019-08-23(Fri) DeepLearning \u8a08\u7b97\u30b0\u30e9\u30d5(Computational Graph) \u00b6 \u8a08\u7b97\u306e\u904e\u7a0b\u3092\u30b0\u30e9\u30d5\u306b\u3088\u3063\u3066\u8868\u73fe\u3057\u305f\u3082\u306e\u3002 \u30ce\u30fc\u30c9\uff1a \u30aa\u30da\u30b3\u30fc\u30c9 \u30a8\u30c3\u30b8\uff1a \u30aa\u30da\u30e9\u30f3\u30c9 \u9014\u4e2d\u306e\u8a08\u7b97\u7d50\u679c\u3092\u5168\u3066\u4fdd\u6301\u3067\u304d\u308b\u3002 \u5c40\u6240\u7684\u306a\u8a08\u7b97\u3092\u4f1d\u64ad\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u6700\u7d42\u7684\u306a\u8a08\u7b97\u7d50\u679c\u3092\u5f97\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308a\u3001\u305d\u308c\u306b\u3088\u3063\u3066 \u554f\u984c\u3092\u5358\u7d14\u5316\u3057\u3066\u7406\u89e3\u3067\u304d\u308b\u3002 \u7279\u306b\u5fae\u5206\u306e\u8a08\u7b97\u306f\u300c\u8a08\u7b97\u306e\u5c40\u6240\u5316\u300d\uff1d \u300c\u9023\u9396\u5f8b(chain rule)\u300d \u3067\u3042\u308a\u3001\u76f8\u6027\u304c\u826f\u3044\u3002 \u9023\u9396\u5f8b\u3068\u8a08\u7b97\u30b0\u30e9\u30d5 \u00b6 \u9023\u9396\u5f8b(chain rule) \u00b6 \u3053\u3053\u3067\u306f\u4f8b\u3068\u3057\u3066 $z=(x+y)^2$ \u3068\u3044\u3046\u5f0f\u3092\u6271\u3046\u3002\u3053\u306e\u5f0f\u306f\u3001\u6b21\u306e $2$ \u5f0f\u304b\u3089\u69cb\u6210\u3055\u308c\u3066\u3044\u308b\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 $$ \\begin{cases} z = t^2\\\\ t = x+y \\end{cases} $$ \u3053\u306e\u6642\u3001\u9023\u9396\u5f8b\u304b\u3089 \u300c$x$ \u306b\u95a2\u3059\u308b $z$ \u306e\u5fae\u5206 $\\frac{\\partial z}{\\partial x}$\u300d \u306f \u300c$t$ \u306b\u95a2\u3059\u308b $z$ \u306e\u5fae\u5206 $\\frac{\\partial z}{\\partial t}$\u300d \u3068 \u300c$x$ \u306b\u95a2\u3059\u308b $t$ \u306e\u5fae\u5206 $\\frac{\\partial t}{\\partial x}$\u300d \u306e\u7a4d\u306b\u3088\u3063\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3002 $$ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\times \\frac{\\partial t}{\\partial x} $$ \u8a08\u7b97\u30b0\u30e9\u30d5 \u00b6 \u4e0a\u3067\u6570\u5f0f\u7684\u306b\u8003\u3048\u305f\u9023\u9396\u5f8b\u306e\u8a08\u7b97\u3092\u8a08\u7b97\u30b0\u30e9\u30d5\u3067\u8868\u3059\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3002 \u3053\u308c\u306b\u3088\u3063\u3066\u3001\u5c40\u6240\u7684\u306a\u5fae\u5206\u8a08\u7b97\u306e\u7a4d\u306b\u3088\u3063\u3066\u5408\u6210\u95a2\u6570\u306e\u5fae\u5206\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u8996\u899a\u7684\u306b\u7406\u89e3\u3067\u304d\u308b\u3002 \u8272\u3005\u306a\u30ce\u30fc\u30c9\u306b\u304a\u3051\u308b\u9006\u4f1d\u64ad \u00b6 \u3053\u3053\u304b\u3089\u3001\u69d8\u3005\u306a\u30aa\u30da\u30b3\u30fc\u30c9\u306b\u304a\u3051\u308b\u9006\u4f1d\u64ad\u3092\u8003\u3048\u308b\u3002 \u52a0\u7b97\u30ce\u30fc\u30c9 \u4e57\u7b97\u30ce\u30fc\u30c9 \u9006\u6570\u30ce\u30fc\u30c9 exp\u30ce\u30fc\u30c9 dot\u30ce\u30fc\u30c9 \u52a0\u7b97\u30ce\u30fc\u30c9 $z=x+y$ \u3068\u3044\u3046\u6570\u5f0f\u3092\u8003\u3048\u3068\u3001\u3053\u306e\u5f0f\u306e\u5fae\u5206\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u308b\u3002 $$ \\begin{cases} \\begin{aligned} \\frac{\\partial z}{\\partial x} = 1\\\\ \\frac{\\partial z}{\\partial y} = 1\\\\ \\end{aligned} \\end{cases} $$ \u3057\u305f\u304c\u3063\u3066\u3001\u52a0\u7b97\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\u3002 \u3064\u307e\u308a\u3001\u52a0\u7b97\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f \u300c\u4e0a\u6d41\u304b\u3089\u4f1d\u308f\u3063\u305f\u5fae\u5206\u306e\u5024\u3092\u305d\u306e\u307e\u307e\u6b21\u306e\u30ce\u30fc\u30c9\u3078\u3068\u6d41\u3059\u3060\u3051\u300d \u306b\u306a\u308b\u3002 \u4e57\u7b97\u30ce\u30fc\u30c9 $z=xy$ \u3068\u3044\u3046\u6570\u5f0f\u3092\u8003\u3048\u3068\u3001\u3053\u306e\u5f0f\u306e\u5fae\u5206\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u308b\u3002 $$ \\begin{cases} \\begin{aligned} \\frac{\\partial z}{\\partial x} = y\\\\ \\frac{\\partial z}{\\partial y} = x\\\\ \\end{aligned} \\end{cases} $$ \u3057\u305f\u304c\u3063\u3066\u3001\u4e57\u7b97\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\u3002 \u3064\u307e\u308a\u3001\u4e57\u7b97\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f \u300c\u4e0a\u6d41\u304b\u3089\u4f1d\u308f\u3063\u305f\u5fae\u5206\u306e\u5024\u306b\u3001\u9806\u4f1d\u64ad\u306e\u969b\u306e\u5165\u529b\u4fe1\u53f7\u3092\u300e\u3072\u3063\u304f\u308a\u8fd4\u3057\u305f\u5024\u300f\u3092\u4e57\u7b97\u3057\u3066\u4e0b\u6d41\u306e\u30ce\u30fc\u30c9\u3078\u3068\u6d41\u3059\u300d \u3053\u3068\u306b\u306a\u308b\u3002 \u9006\u6570\u30ce\u30fc\u30c9 $y=\\frac{1}{x}$ \u3068\u3044\u3046\u6570\u5f0f\u3092\u8003\u3048\u3068\u3001\u3053\u306e\u5f0f\u306e\u5fae\u5206\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u308b\u3002 $$ \\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2} = -y^2 $$ \u3057\u305f\u304c\u3063\u3066\u3001\u9006\u6570\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\u3002 \u3064\u307e\u308a\u3001\u9006\u6570\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f \u300c\u4e0a\u6d41\u304b\u3089\u4f1d\u308f\u3063\u305f\u5fae\u5206\u306e\u5024\u306b\u300e\u9806\u4f1d\u64ad\u306e\u51fa\u529b\u306e\u4e8c\u4e57\u306b\u30de\u30a4\u30ca\u30b9\u3092\u4ed8\u3051\u305f\u5024\u300f\u3092\u4e57\u7b97\u3057\u3066\u4e0b\u6d41\u306e\u30ce\u30fc\u30c9\u3078\u3068\u6d41\u3059\u300d \u3053\u3068\u306b\u306a\u308b\u3002 exp\u30ce\u30fc\u30c9 $y=\\exp(x)$ \u3068\u3044\u3046\u6570\u5f0f\u3092\u8003\u3048\u3068\u3001\u3053\u306e\u5f0f\u306e\u5fae\u5206\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u308b\u3002 $$ \\frac{\\partial y}{\\partial x} = \\exp\\left(x\\right) = y $$ \u3057\u305f\u304c\u3063\u3066\u3001exp\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\u3002 \u3064\u307e\u308a\u3001\u9006\u6570\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f \u300c\u4e0a\u6d41\u304b\u3089\u4f1d\u308f\u3063\u305f\u5fae\u5206\u306e\u5024\u306b\u300e\u9806\u4f1d\u64ad\u306e\u51fa\u529b\u300f\u3092\u4e57\u7b97\u3057\u3066\u4e0b\u6d41\u306e\u30ce\u30fc\u30c9\u3078\u3068\u6d41\u3059\u300d \u3053\u3068\u306b\u306a\u308b\u3002 \u3000\u3053\u3053\u307e\u3067\u306e\u30ce\u30fc\u30c9\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001 Softmax \u3084 Cross Entropy Error \u306e\u9006\u4f1d\u64ad\u3082\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 dot\u30ce\u30fc\u30c9 $\\mathbf{Y} = \\mathbf{X}\\cdot\\mathbf{W} + \\mathbf{B}$ \u3068\u3044\u3046\u6570\u5f0f\u3092\u8003\u3048\u308b\u3068\u3001\u3053\u306e\u5f0f\u306e\u5fae\u5206\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3002 $$ \\begin{cases} \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{X}}\\cdot \\mathbf{W}^T\\\\ \\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{X}^T\\cdot\\frac{\\partial L}{\\partial \\mathbf{Y}}\\\\ \\end{aligned} \\end{cases} $$ \u3057\u305f\u304c\u3063\u3066\u3001dot\u30ce\u30fc\u30c9\u306e\u9006\u4f1d\u64ad\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\u3002 \u5929\u4e0b\u308a\u5f0f\u306b\u3053\u306e\u9006\u4f1d\u64ad\u3092\u7406\u89e3\u3057\u3066\u3082\u826f\u3044\u304c\u3001\u884c\u5217\u3092\u5bfe\u8c61\u3068\u3057\u305f\u9006\u4f1d\u64ad\u3092\u6c42\u3081\u308b\u5834\u5408\u306f\u3001\u884c\u5217\u306e\u8981\u7d20\u3054\u3068\u306b\u66f8\u304d\u4e0b\u3059\u3053\u3068\u3067\u3001\u3053\u308c\u307e\u3067\u306e\u30b9\u30ab\u30e9\u5024\u3092\u5bfe\u8c61\u3068\u3057\u305f\u8a08\u7b97\u30b0\u30e9\u30d5\u3068\u540c\u69d8\u306b\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u5b9f\u969b\u306b\u66f8\u304d\u4e0b\u3059\u3068\u3001\u4ee5\u4e0b\u306e\u8a08\u7b97\u30b0\u30e9\u30d5\u306b\u5206\u89e3\u3067\u304d\u308b\u3002 Python\u306b\u3088\u308b\u5b9f\u88c5 \u00b6 \u52a0\u7b97\u30ce\u30fc\u30c9 \u00b6 In [1]: class AddLayer : def __init__ ( self ): pass def forward ( self , x , y ): out = x + y return out def backward ( self , dout ): dx = dout * 1 dy = dout * 1 return dx , dy \u4e57\u7b97\u30ce\u30fc\u30c9 \u00b6 In [2]: class MulLayer : def __init__ ( self ): self . x = None self . y = None def forward ( self , x , y ): self . x = x self . y = y out = x * y return out def backward ( self , dout ): dx = dout * self . y dy = dout * self . x return dx , dy \u9006\u6570\u30ce\u30fc\u30c9 \u00b6 In [3]: class InverseLayer : def __init__ ( self ): self . out = None def forward ( self , x ): out = 1 / x self . out = out return out def backward ( self , dout ): dx = dout * ( - self . out ** 2 ) return dx exp\u30ce\u30fc\u30c9 \u00b6 In [4]: class ExpLayer : def __init__ ( self ): self . out = None def forward ( self , x ): out = np . exp ( x ) self . out = out return out def backward ( self , dout ): dx = dout * self . out return dx dot\u30ce\u30fc\u30c9 \u00b6 In [5]: class DotLayer : def __init__ ( self ): self . X = None self . W = None def forward ( self , X , W ): \"\"\" @param X : shape=(1,a) @param W : shape=(a,b) @return out: shape=(1,b) \"\"\" self . X = X self . W = W out = np . dot ( X , W ) # shape=(1,b) return out def backward ( self , dout ): \"\"\" @param dout: shape=(1,b) @return dX : shape=(1,a) @return dW : shape=(a,b) \"\"\" dX = np . dot ( dout , self . W . T ) dW = np . dot ( self . X . T , dout ) return dX , dW \u53c2\u8003 \u00b6 In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Computational Graph"},{"location":"DeepLearning/Initializers/","text":"Initializers 2019-08-28(Wed) DeepLearning \u521d\u671f\u5316(Initializers) \u00b6 \uff08\u7de8\u96c6\u4e2d\uff09 In [2]: import numpy as np import matplotlib.pyplot as plt % matplotlib inline \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u91cd\u307f\u306f\u3001 \u5c0f\u3055\u306a\u4e71\u6570\u5024\u3067\u521d\u671f\u5316\u3055\u308c\u308b \u3053\u3068\u304c\u307b\u3068\u3093\u3069\u3067\u3059\u304c\u3001\u30e2\u30c7\u30eb\u306b\u5408\u308f\u305b\u3066\u9069\u5207\u306a\u4e71\u6570\u3092\u7528\u3044\u3066\u521d\u671f\u5316\u3092\u884c\u3048\u3070 \u5b66\u7fd2\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u9ad8\u901f\u5316\u3059\u308b \u306a\u3069\u306e\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u307e\u3059\u3002 \u307e\u305f\u3001\u7279\u306b\u5c64\u304c\u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u307b\u3069\u521d\u671f\u5316\u306e\u5f71\u97ff\u3092\u5927\u304d\u304f\u53d7\u3051\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u307e\u3059\u3002 \uff08\u521d\u671f\u5206\u5e03\u306e\u30ba\u30ec\u304c\u639b\u3051\u7b97\u3055\u308c\u308b\u305f\u3081\u3001\u6307\u6570\u7684\u306b\u5f71\u97ff\u3092\u53ca\u307c\u3059\u304b\u3089\uff09 \u3053\u3053\u3067\u306f\u3001Kerasy\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u69d8\u3005\u306a\u521d\u671f\u5316\u65b9\u6cd5\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002 # \u624b\u6cd5 Kerasy 1 Zeros kerasy.initializers.Zeros() 2 Ones kerasy.initializers.Ones() 3 Constant kerasy.initializers.Constant(value=0) 4 RandomNormal kerasy.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None) 5 RandomUniform kerasy.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None) 6 TruncatedNormal kerasy.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None) 7 VarianceScaling kerasy.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None) 8 Orthogonal kerasy.initializers.Orthogonal(gain=1.0, seed=None) 9 Identity kerasy.initializers.Identity(gain=1.0) 10 GlorotNormal kerasy.initializers.glorot_normal(seed=None) 11 GlorotUniform kerasy.initializers.glorot_uniform(seed=None) 12 HeNormal kerasy.initializers.he_normal(seed=None) 13 LeCunNormal kerasy.initializers.lecun_normal(seed=None) 14 HeUniform kerasy.initializers.he_uniform(seed=None) 15 LeCunUniform kerasy.initializers.lecun_uniform(seed=None) Zeros \u5168\u3066\u306e\u91cd\u307f\u30920\u3067\u521d\u671f\u5316\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5404\u5c64\u9593\u306e\u91cd\u307f\u30920\u3067\u521d\u671f\u5316\u3059\u308b\u3068\u8aa4\u5dee\u304c\u4f1d\u308f\u3089\u305a\u3001\u3046\u307e\u304f\u5b66\u7fd2\u304c\u3067\u304d\u306a\u3044 \u3053\u3068\u306f\u5bb9\u6613\u306b\u60f3\u50cf\u304c\u3067\u304d\u307e\u3059\u3002 \u4e00\u65b9\u3067\u3001\u30d0\u30a4\u30a2\u30b9\u9805\u30920\u3067\u521d\u671f\u5316\u3059\u308b\u624b\u6cd5\u306f\u826f\u304f\u53d6\u3089\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\u30d0\u30a4\u30a2\u30b9\u9805\u304c0\u3067\u3082\u30d0\u30a4\u30a2\u30b9\u9805\u306b\u8aa4\u5dee\u306f\u4f1d\u308f\u308b\u305f\u3081\u3001\u5b66\u7fd2\u306f\u9032\u307f\u307e\u3059\u3002\u30d0\u30a4\u30a2\u30b9\u9805\u30920\u3067\u521d\u671f\u5316\u3059\u308b\u3053\u3068\u3067\u3001 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u306b\u30d0\u30a4\u30a2\u30b9\u306e\u5f71\u97ff\u3092\u4e0e\u3048\u306b\u304f\u304f\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b \u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002 Ones \u5168\u3066\u306e\u91cd\u307f\u30921\u3067\u521d\u671f\u5316\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 Constant \u5168\u3066\u306e\u91cd\u307f\u3092\u5b9a\u6570\uff08= value \uff09\u3067\u521d\u671f\u5316\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 RandomNormal \u6b63\u898f\u5206\u5e03\u306b\u3057\u305f\u304c\u3063\u3066\u91cd\u307f\u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002 mean : \u6b63\u898f\u5206\u5e03\u306e\u5e73\u5747 stddev : \u6b63\u898f\u5206\u5e03\u306e\u6a19\u6e96\u504f\u5dee seed : \u30b7\u30fc\u30c9\u5024 RandomUniform \u4e00\u69d8\u5206\u5e03\u306b\u3057\u305f\u304c\u3063\u3066\u91cd\u307f\u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002 minval : \u4e71\u6570\u3092\u767a\u751f\u3059\u308b\u7bc4\u56f2\u306e\u4e0b\u9650 maxval : \u4e71\u6570\u3092\u767a\u751f\u3059\u308b\u7bc4\u56f2\u306e\u4e0a\u9650 seed : \u30b7\u30fc\u30c9\u5024 TruncatedNormal \u5207\u65ad\u6b63\u898f\u5206\u5e03 \u3068\u547c\u3070\u308c\u308b\u5206\u5e03\u306b\u5f93\u3063\u3066\u91cd\u307f\u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u6b63\u898f\u5206\u5e03\u3068\u4f3c\u3066\u3044\u307e\u3059\u304c\u3001 \u5e73\u5747\u304b\u3089\u6a19\u6e96\u504f\u5dee\u4ee5\u4e0a\u96e2\u308c\u305f\u5024\u306f\u5207\u308a\u6368\u3066\u3089\u308c\u308b \u3068\u3044\u3046\u7279\u5fb4\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u5206\u5e03\u3092\u7528\u3044\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u91cd\u307f\u3092\u521d\u671f\u5316\u3059\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u3066\u3044\u307e\u3059\u3002 mean : \u6b63\u898f\u5206\u5e03\u306e\u5e73\u5747 stddev : \u6b63\u898f\u5206\u5e03\u306e\u6a19\u6e96\u504f\u5dee seed : \u30b7\u30fc\u30c9\u5024 VarianceScaling \u91cd\u307f\u306e\u30b5\u30a4\u30ba\u306b\u5408\u308f\u305b\u3066\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3092\u884c\u3044\u3001\u521d\u671f\u5316\u3057\u307e\u3059\u3002 if mode == \"fan_in\" : n = \"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u306e\u6570\" elif mode == \"fan_out\" : n = \"\u51fa\u529b\u30e6\u30cb\u30c3\u30c8\u306e\u6570\" elif mode == \"fan_avg\" : n = \"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u3068\u51fa\u529b\u30e6\u30cb\u30c3\u30c8\u306e\u6570\u306e\u5e73\u5747\" \u307e\u305f\u3001 distribution=\"normal\" \u306e\u6642\uff1a \u5e73\u5747\u30920\u3068\u3057\u3001\u6a19\u6e96\u504f\u5dee\u3092 stddev = sqrt(scale / n) \u3068\u3057\u305f \u5207\u65ad\u6b63\u898f\u5206\u5e03 \u304c\u7528\u3044\u3089\u308c\u307e\u3059\u3002 distribution=\"uniform\" \u306e\u6642\uff1a limit = sqrt(3 * scale / n) \u3068\u3057\u3001[- limit , limit ]\u3092\u7bc4\u56f2\u3068\u3059\u308b \u4e00\u69d8\u5206\u5e03 \u304c\u7528\u3044\u3089\u308c\u307e\u3059\u3002 Orthogonal \u91cd\u307f\u304c\u76f4\u4ea4\u884c\u5217\u3068\u306a\u308b\u3088\u3046\u306b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u306a\u304a\u3001\u5b9f\u88c5\u3067\u306f\u3001$m$ \u884c $n$ \u5217\u306e\u884c\u5217 $A$ \u306b\u5bfe\u3057\u3066\u3001 $$A=U \\Sigma \\overline{V^{T}}$$ $U$: $m\\times m$ \u306e\u30e6\u30cb\u30bf\u30ea\u884c\u5217 $\\Sigma$: $m\\times n$ \u306e\u5b9f\u5bfe\u89d2\u884c\u5217 $V$: $n\\times n$ \u306e\u30e6\u30cb\u30bf\u30ea\u884c\u5217 \u3068\u5206\u89e3\u3067\u304d\u308b \uff08\u7279\u7570\u5024\u5206\u89e3(Singular Value Decomposition)\uff09 \u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u304a\u308a\u3001\u5b9f\u30e6\u30cb\u30bf\u30ea\u884c\u5217\u304c\u76f4\u4ea4\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u304b\u3089\u3001\u3053\u306e\u5206\u89e3\u3092\u7528\u3044\u3066\u76f4\u4ea4\u884c\u5217\u3092\u751f\u6210\u3057\u3066\u3044\u307e\u3059\u3002 \u203b \u30e6\u30cb\u30bf\u30ea\u884c\u5217 \u306f\u3001$U^{*} U=U U^{*}=I$ \u304c\u6210\u308a\u7acb\u3064\u884c\u5217\u3067\u3042\u308b\u3002\uff08$U^{*}=\\overline{U}^{\\mathrm{T}}$\uff09 gain : \u76f4\u4ea4\u884c\u5217\u306e\u30b9\u30b1\u30fc\u30eb seed : \u30b7\u30fc\u30c9\u5024 Identity \u91cd\u307f\u304c\u5358\u4f4d\u884c\u5217\u3068\u306a\u308b\u3088\u3046\u306b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u306a\u304a\u3001\u3053\u308c\u306f\u3001\u91cd\u307f\u304c2\u6b21\u6b63\u65b9\u884c\u5217\u306e\u5834\u5408\u306e\u307f\u4f7f\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 GlorotNormal Glorot \u306e\u6b63\u898f\u5206\u5e03 \u306b\u3088\u308b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u3053\u308c\u306f\u3001 \u5e73\u5747: $0$ \u6a19\u6e96\u504f\u5dee: stddev = sqrt(2 / (\"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\" + \"\u51fa\u529b\u30e6\u30cb\u30c3\u30c8\u6570\")) \u3068\u3059\u308b \u6b63\u898f\u5206\u5e03 \u3068\u7b49\u3057\u3044\u3067\u3059\u3002 Glorot \u306e\u6b63\u898f\u5206\u5e03\u306e\u30a2\u30a4\u30c7\u30a2\u306f\u3001 \u6d3b\u6027\u5316\u95a2\u6570\u304c\u539f\u70b9\u5bfe\u79f0\u3067\u3042\u308a\u3001\u539f\u70b9\u4ed8\u8fd1\u3067\u7dda\u5f62\u3067\u3042\u308b\u3068\u3044\u3046\u6761\u4ef6\u4e0b\u3067 \u9806\u4f1d\u64ad\u30fb\u9006\u4f1d\u64ad\u5171\u306b \u5404\u5c64\u306e\u51fa\u529b\u304c\u540c\u3058\u5206\u6563\u3092\u6301\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u76ee\u6307\u3059 \u3068\u3044\u3046\u3082\u306e\u3067\u3001\u305d\u308c\u305e\u308c\u306e\u6761\u4ef6\u306e\u4e2d\u9593\u3092\u53d6\u308b\u3053\u3068\u3067\u3053\u308c\u3092\u5b9f\u73fe\u3057\u3066\u3044\u307e\u3059\u3002\u3088\u3063\u3066\u3001\u5206\u6563\u304c sqrt(2 / (\"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\" + \"\u51fa\u529b\u30e6\u30cb\u30c3\u30c8\u6570\")) \u3068\u306a\u308b \u6b63\u898f\u5206\u5e03 \u3067\u521d\u671f\u5316\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002 GlorotUniform Glorot \u306e\u4e00\u69d8\u5206\u5e03 \u306b\u3088\u308b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u3053\u308c\u306f\u3001 limit=sqrt(6 / (\"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\" + \"\u51fa\u529b\u30e6\u30cb\u30c3\u30c8\u6570\")) \u3068\u3057\u3001[- limit , limit ]\u3092\u7bc4\u56f2\u3068\u3059\u308b \u4e00\u69d8\u5206\u5e03 \u3068\u7b49\u3057\u3044\u3067\u3059\u3002 Glorot \u306e\u4e00\u69d8\u5206\u5e03\u306f\u3001\u5206\u6563\u304c sqrt(2 / (\"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\" + \"\u51fa\u529b\u30e6\u30cb\u30c3\u30c8\u6570\")) \u3068\u306a\u308b\u3088\u3046\u306a\u4e00\u69d8\u5206\u5e03\u3067\u3042\u308a\u3001$[-a,a]$ \u306e\u7bc4\u56f2\u306e\u4e00\u69d8\u5206\u5e03\u306e\u5206\u6563\u304c $a^2/3$ \u3067\u3042\u308b\u3053\u3068\u304b\u3089\u3001\u4e0a\u8a18\u306e\u7bc4\u56f2\u306e \u4e00\u69d8\u5206\u5e03 \u3067\u521d\u671f\u5316\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002 HeNormal He \u306e\u6b63\u898f\u5206\u5e03 \u306b\u3088\u308b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u3053\u308c\u306f\u3001 \u5e73\u5747: $0$ \u6a19\u6e96\u504f\u5dee: stddev = sqrt(2 / \"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\") \u3068\u3059\u308b \u5207\u65ad\u6b63\u898f\u5206\u5e03 \u3068\u7b49\u3057\u3044\u3067\u3059\u3002 Glorot \u306e\u6b63\u898f\u5206\u5e03 \u304c\u6d3b\u6027\u5316\u95a2\u6570\u306b\u5236\u7d04\u3092\u7f6e\u3044\u3066\u304a\u308a\u3001 relu\u95a2\u6570 \u3092\u7528\u3044\u305f\u5834\u5408\u3084\u3001 \u5168\u7d50\u5408\u4ee5\u5916\u306eConvolution \u5c64\u306a\u3069\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u304c\u5bfe\u8c61\u5916\u3067\u3042\u3063\u305f\u305f\u3081\u3001\u305d\u308c\u3089\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306b\u8003\u3048\u3089\u308c\u305f\u306e\u304c He \u306e\u6b63\u898f\u5206\u5e03 \u3067\u3059\u3002 LeCunNormal LeCun\u306e\u6b63\u898f\u5206\u5e03 \u306b\u3088\u308b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u3053\u308c\u306f\u3001 \u5e73\u5747: $0$ \u6a19\u6e96\u504f\u5dee: stddev = sqrt(1 / \"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\") \u3068\u3059\u308b \u5207\u65ad\u6b63\u898f\u5206\u5e03 \u3068\u7b49\u3057\u3044\u3067\u3059\u3002 \u3053\u306e\u5206\u5e03\u306f\u3001HeNormal\u3092 $1/\\sqrt{2}$ \u3067\u30b9\u30b1\u30fc\u30eb\u3057\u305f\u3082\u306e\u3068\u7b49\u4fa1\u3067\u3059\u3002 HeUniform He \u306e\u4e00\u69d8\u5206\u5e03 \u306b\u3088\u308b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u3053\u308c\u306f\u3001 limit=sqrt(6 / \"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\") \u3068\u3057\u3001[- limit , limit ]\u3092\u7bc4\u56f2\u3068\u3059\u308b \u4e00\u69d8\u5206\u5e03 \u3068\u7b49\u3057\u3044\u3067\u3059\u3002 LeCunUniform LeCun \u306e\u4e00\u69d8\u5206\u5e03 \u306b\u3088\u308b\u521d\u671f\u5316\u3092\u884c\u3044\u307e\u3059\u3002 \u3053\u308c\u306f\u3001 limit=sqrt(3 / \"\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u6570\") \u3068\u3057\u3001[- limit , limit ]\u3092\u7bc4\u56f2\u3068\u3059\u308b \u4e00\u69d8\u5206\u5e03 \u3068\u7b49\u3057\u3044\u3067\u3059\u3002 In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Initializers"},{"location":"DeepLearning/NeuralNetwork/","text":"Neural Network 2019-08-18(Sun) DeepLearning Neural Network \u00b6 Table of contents \u00b6 \u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30ed\u30c8\u30f3 (Perceptron) \u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30ed\u30c8\u30f3 (Multilayer Perceptron) \u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5 (Back Propagation) \u5b9f\u88c5 (Implementation) 1. \u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30ed\u30c8\u30f3 (Perceptron) \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f \u591a\u6570\u306e\u7d20\u5b50\uff08\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff09\u306e\u96c6\u307e\u308a \u3067\u8868\u73fe\u3055\u308c\u307e\u3059\u3002\u305d\u3053\u3067\u3001\u307e\u305a\u306f\uff11\u3064\uff11\u3064\u306e\u7d20\u5b50\u3001\u30d1\u30fc\u30bb\u30d7\u30ed\u30c8\u30f3\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002 \u5358\u4e00\u306e\u7d20\u5b50\u306f\u4ee5\u4e0b\u306e\u56f3\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u3001\u5165\u529b\u3092\u53d7\u3051\u53d6\u308a\u3001 \u305d\u306e\u5024\uff08\u306e\u548c\uff09\u304c\u3042\u308b\u95be\u5024 $\\theta$ \u3092\u8d85\u3048\u305f\u3089\u767a\u706b\u3057\u3066\u51fa\u529b $\\theta$ \u4ee5\u4e0b\u3060\u3063\u305f\u3089\u51fa\u529b\u3057\u306a\u3044 \u3068\u3044\u3046\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\u3002 \u3053\u306e\u6027\u8cea\u3092\u6570\u5b66\u7684\u306b\u8a18\u8ff0\u3059\u308b\u3068\u3001 \u91cd\u307f\u30d1\u30e9\u30e1\u30fc\u30bf\uff1a$w_0,w_1,\\ldots,w_m$ \u6d3b\u6027\u5316\u95a2\u6570\uff1a$h$ \u304b\u3089\u69cb\u6210\u3055\u308c\u308b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u95a2\u6570\u3068\u3057\u3066\u66f8\u3051\u307e\u3059\u3002 $$f(x_1,x_2,\\ldots,x_m) = h(w_1x_1+w_2x_2+\\cdots+w_Nx_N+w_0)$$ \u3053\u306e\u6642 $w_0$ \u306f \u30d0\u30a4\u30a2\u30b9\u30d1\u30e9\u30e1\u30fc\u30bf(bias parameter) \u3068\u547c\u3070\u308c\u3001 \u95a2\u6570\u3092\u5e73\u884c\u79fb\u52d5\u3055\u305b\u308b \u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002 \u6d3b\u6027\u5316\u95a2\u6570(activation function) \u00b6 \u6d3b\u6027\u5316\u95a2\u6570\u306f\u3001 \u300c\u3042\u308b\u95be\u5024 $\\theta$ \u3092\u8d85\u3048\u305f\u3089\u767a\u706b\u3057\u3066\u51fa\u529b\u3057\u3001$\\theta$ \u4ee5\u4e0b\u3060\u3063\u305f\u3089\u51fa\u529b\u3057\u306a\u3044\u300d \u3068\u3044\u3046\u795e\u7d4c\u306e\u6027\u8cea\u3092\u8868\u73fe\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\u3067\u3042\u308a\u3001\u6700\u3082\u5358\u7d14\u306a\u3082\u306e\u3068\u3057\u3066\u306f\u4ee5\u4e0b\u3067\u8868\u3055\u308c\u308b \u30d8\u30f4\u30a3\u30b5\u30a4\u30c9\u95a2\u6570(Heaviside function) \u304c\u3042\u308a\u307e\u3059\u3002 $$h(a) = \\left\\{\\begin{array}{cc}0 & (\\theta < 0) \\\\1 & (\\theta > 0)\\end{array}\\right.$$ \u3053\u306e\u6027\u8cea\u306f\u8868\u3057\u305f\u3044\u80fd\u529b\u3092\u5982\u5b9f\u306b\u8868\u3057\u3066\u306f\u3044\u307e\u3059\u304c\u3001 \u4e0d\u9023\u7d9a\u3067\u3042\u308a\u6271\u3044\u3065\u3089\u3044 \u305f\u3081\u3001\u5b9f\u969b\u306f \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 \uff1a $$h(a) = \\frac{1}{1+\\exp(-a)}\\qquad(4.59)$$ \u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8\u95a2\u6570 \uff1a $$\\begin{aligned}h(a) &= \\tanh (a)\\\\&=\\frac{e^a-e^{-1}}{e^1+e^{-1}} &(5.59)\\end{aligned}$$ \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 \uff1a $$h(\\mathbf{a}) = \\frac{\\exp(a_i)}{\\sum_j \\exp(a_i)}\\qquad (4.63)$$ \u306a\u3069\u304c\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u591a\u3044\u3067\u3059\u3002 \u203b Kerasy\u3067\u306f \u3053\u3053 \u3067\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002 In [1]: class Linear (): def forward ( input ): return input def diff ( a ): dhda = a return dhda class Tanh (): def forward ( input ): return np . tanh ( input ) def diff ( a ): dhda = 1 - np . tanh ( a ) ** 2 return dhda ActivationHandler = { 'linear' : Linear , 'tanh' : Tanh , } def ActivationFunc ( activation_func_name ): return ActivationHandler [ activation_func_name ] 2. \u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30ed\u30c8\u30f3 (Multilayer Perceptron) \u305d\u308c\u3067\u306f\u3001\u8907\u6570\u306e\u5358\u5b50\uff08\u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30ed\u30c8\u30f3\uff09\u3092\u7e4b\u3052\u3066\uff12\u5c64\u306e\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3092\u4f5c\u308a\u307e\u3059\u3002\u306a\u304a\u3001 \uff12\u5c64\u4ee5\u4e0a\u306e\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3092 \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u3068\u8a00\u3044\u307e\u3059\u3002 \u4f8b\u3048\u3070\uff11\u5c64\u76ee\u306b $m$ \u500b\u306e\u7d20\u5b50\u3092\u7528\u610f\u3057\u3001\u5165\u529b\u304c $D$ \u6b21\u5143\u306e\u5834\u5408\u3001\u95a2\u6570\u3067\u8868\u3059\u3068 $$y = h_2\\left(\\sum_{i=0}^{m} w^{(2)}_ih_1\\left(\\sum_{j=0}^{D}w^{(1)}_{ij} x_j\\right)\\right)\\qquad(5.9)$$ \u3068\u306a\u308a\u307e\u3059\u3002 \u8a18\u53f7 \u610f\u5473 $w^{(1)}_{ij}$ $1$ \u5c64\u76ee\u306e $i$ \u756a\u76ee\u306e\u7d20\u5b50\u3078\u306e\u5165\u529b $x_j$ \u306e\u91cd\u307f $w^{(2)}_i$ $1$ \u5c64\u76ee\u306e $i$ \u756a\u76ee\u306e\u7d20\u5b50\u304b\u3089 $2$ \u5c64\u76ee\u306e\u7d20\u5b50\u3078\u306e\u5165\u529b\u306e\u91cd\u307f $h_1,h_2$ \u305d\u308c\u305e\u308c\u306e\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570 \u591a\u5c64\u306b\u3059\u308b\u610f\u5473 \u00b6 \u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30ed\u30c8\u30f3\uff1a $$ y=f\\left(\\sum_i w_i\\textcolor{red}{\\phi_i(\\mathbf{x})}\\right) \\qquad(5.1)$$ $2$ \u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff1a $$ y = h_2\\left(\\sum_{i=0}^{m} w^{(2)}_i\\textcolor{red}{h_1\\left(\\sum_{j=0}^{D}w^{(1)}_{ij} x_j\\right)}\\right)\\qquad(5.9)$$ \u3053\u306e\u5f0f\u304b\u3089\u5206\u304b\u308b\u3088\u3046\u306b\u3001\u57fa\u5e95 $\\phi_i(\\mathbf{x})$ \u304c $\\displaystyle h_1\\left(\\sum_{j=0}^{D}w^{(1)}_{ij} x_j\\right)$ \u306b\u7f6e\u304d\u63db\u308f\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u3063\u3066\u3001 \u5358\u4e00\u3060\u3068\u300c\u56fa\u5b9a\u300d\u3055\u308c\u3066\u3044\u305f\u57fa\u5e95\u95a2\u6570\u304c\u3001\u591a\u5c64\u306b\u306a\u308b\u3053\u3068\u3067\u300c\u9069\u5fdc\u7684\u306b\u5909\u52d5\u300d\u3059\u308b \u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u3086\u3048\u306b\u3001 \u5341\u5206\u5927\u304d\u306a $m$ \u3092\u3068\u308a\u3001\u6d3b\u6027\u5316\u95a2\u6570\u304c\u975e\u7dda\u5f62\u306a\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306f\u3001 \u4efb\u610f\u306e\u95a2\u6570\u3092\u4efb\u610f\u306e\u7cbe\u5ea6\u3067\u8fd1\u4f3c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b \u3068\u3044\u3046\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\u3002 3. \u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5 (Back Propagation) \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3064\u3044\u3066\u8a9e\u308b\u4e0a\u3067\u907f\u3051\u3066\u306f\u901a\u308c\u306a\u304f\u3001\u7406\u89e3\u304c\u96e3\u3057\u3044\u306e\u304c \u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\uff08back propagation\uff09 \u3067\u3059\u3002 \u3057\u304b\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\u3068\u306f \u300c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u7fd2\u3055\u305b\u308b\u969b\u306b\u7528\u3044\u3089\u308c\u308b\u52b9\u7387\u7684\u306a\u8a08\u7b97\u65b9\u6cd5\u300d \u306e\u3053\u3068\u3067\u3001\u7c21\u5358\u306b\u8a00\u3063\u3066\u3057\u307e\u3048\u3070 \u300c\u5408\u6210\u95a2\u6570\u306e\u5fae\u5206\u5247\u300d \u3067\u3059\u3002\u307e\u305f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8a13\u7df4\u3068\u306f\u3001 \u300c\uff08\u91cd\u307f\u3092\u5909\u5316\u3055\u305b\u308b\u3053\u3068\u3067\uff09\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u51fa\u529b\u3068\u6b63\u89e3\u306e\u8aa4\u5dee $E_n(\\mathbf{w})$ \u3092\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u300d \u3092\u6307\u3057\u307e\u3059\u3002 \u4ee5\u4e0b\u3001 \u7d20\u5b50 $i$ \u306e\u51fa\u529b\u3092 $z_i$ \u7d20\u5b50 $j$ \u3078\u306e\u5165\u529b\u548c\u3092 $a_j$ \u3068\u3057\u3066\u3001\u8aac\u660e\u3057\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u5b66\u7fd2\u306e\u30d7\u30ed\u30bb\u30b9\u306b\u304a\u3044\u3066\u306f \u300c\u7d20\u5b50 $i$ \u304b\u3089\u7d20\u5b50 $j$ \u3078\u306e\u63a5\u7d9a\u306e\u91cd\u307f $w_{ji}$ \u3068\u3057\u3066\u3001\u5168\u3066\u306e $i,j$ \u306e\u7d44\u5408\u305b\uff08\uff1d\u5168\u30ce\u30fc\u30c9\u306e\u91cd\u307f\uff09\u306b\u5bfe\u3057\u3066$ \\frac{\\partial E_n}{\\partial w_{ji}} $\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u300d \u304c\u76ee\u6a19\u3068\u306a\u308a\u307e\u3059\u3002 \u3059\u308b\u3068\u3001\u4e0a\u306e\u56f3\u306e\u3088\u3046\u306b \u300c$E_n$ \u306f $a_j$ \u3092\u4ecb\u3057\u3066\u306e\u307f $w_{ji}$ \u306b\u4f9d\u5b58\u3059\u308b\u300d\uff08\uff1d $w_{ji}$ \u304c\u5909\u5316\u3059\u308b\u3068 $a_j$ \u304c\u5909\u5316\u3057\u3001\u305d\u308c\u304c $E_n$ \u306b\u5f71\u97ff\u3092\u53ca\u307c\u3059\uff09 \u3053\u3068\u306b\u6ce8\u610f\u3059\u308c\u3070 $$ \\frac{\\partial E_n}{\\partial w_{ji}} = \\frac{\\partial E_n}{\\partial a_j}\\frac{\\partial a_j}{\\partial w_{ji}}\\qquad(5.50)$$ \u3068\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u3053\u3053\u3067 $\\delta_j = \\partial E_n/\\partial a_j$ \u3068\u66f8\u304f\u3053\u3068\u306b\u3057\u3001\u3053\u308c\u3092 \u8aa4\u5dee \u3068\u547c\u3073\u307e\u3059\u3002 \u7d9a\u3044\u3066\u3001$\\delta_i$ \u306b\u3064\u3044\u3066\u8003\u3048\u308b\u3068\u3001$E_n$ \u306f\u7d20\u5b50 $i$ \u306e\u51fa\u529b\u3092\u53d7\u3051\u53d6\u308b\u7d20\u5b50 $j$ \u306e\u5165\u529b $a_j$ \u3092\u4ecb\u3057\u3066 $a_i$ \u306b\u4f9d\u5b58\u3059\u308b\u306e\u3067\u3001\u5408\u6210\u5fae\u5206\u5247\u3088\u308a $$ \\delta_i = \\frac{\\partial E_n}{\\partial a_i} = \\sum_j \\frac{\\partial E_n}{\\partial a_j}\\frac{\\partial a_j}{\\partial a_i} = \\sum_j\\delta_j\\frac{\\partial a_j}{\\partial a_i}\\qquad(5.55)$$ \u3068\u306a\u308a\u307e\u3059\u3002\uff08\u4e0b\u56f3\u53c2\u7167\uff09 \u3055\u3089\u306b\u3001\u7d20\u5b50 $i$ \u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092 $h_i$ \u3068\u3059\u308b\u3068\u3001 $$a_j = \\sum_i w_{ji} h_i(a_i)\\qquad(5.48)$$ \u3060\u3063\u305f\uff08\u7d20\u5b50 $i$ \u3078\u306e\u5165\u529b\u548c\u3092\u6d3b\u6027\u5316\u95a2\u6570\u306b\u901a\u3057\u305d\u308c\u305e\u308c\u306b\u91cd\u307f $w_{ji}$ \u3092\u304b\u3051\u305f\u548c\u3092\u7d20\u5b50 $j$ \u306b\u4f1d\u3048\u308b\uff09\u306e\u3067\u3001 $$ \\frac{\\partial a_j}{\\partial a_i} = w_{ji} h_i'(a_i)$$ \u3068\u306a\u308a\u307e\u3059\u3002 \u3053\u3053\u307e\u3067\u3092\u307e\u3068\u3081\u308b\u3068\u3001\u8aa4\u5dee $\\delta_i$ \u306e\u9006\u4f1d\u64ad\u516c\u5f0f $$ \\delta_i = h_i'(a_i) \\sum_j w_{ji}\\delta_j\\qquad(5.56)$$ \u304c\u5f97\u3089\u308c\u307e\u3059\u3002 \u307e\u305f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u90e8\u306b\u304a\u3051\u308b $\\delta$ \u306e\u5024\u306f\u76f4\u63a5\u8a08\u7b97\u3059\u308b\u4e8b\u304c\u51fa\u6765\u308b\u306e\u3067\u3001\u305d\u3053\u304b\u3089\u9006\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8fbf\u308a\u306a\u304c\u3089\u5404 $\\delta$ \u3092\u8a08\u7b97\u3059\u308b\u4e8b\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002 \u3053\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u9006\u306b\u8fbf\u308b\u904e\u7a0b\u3067\u5404 $w_{ji}$ \u306b\u5bfe\u3059\u308b $W$ \u500b\u306e\u504f\u5c0e\u95a2\u6570\u3092\u4e00\u56de\u306e\u8aa4\u5dee\u4f1d\u64ad\u3067\u6c42\u3081\u3066\u3057\u307e\u3046\u4e8b\u304c\u51fa\u6765\u308b\u306e\u3067\u3001 \u52b9\u7387\u306e\u826f\u3044\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002 \u3010\u307e\u3068\u3081\u3011 \u00b6 \u30c7\u30fc\u30bf $\\mathbf{x}_n$ \u3092\u5165\u529b\u3057\u305f\u6642\u306e\u3001\u5404\u7d20\u5b50\u3078\u306e\u5165\u529b $a_i$ \u51fa\u529b $z_i$ \u3092\u6c42\u3081\u308b\u3002 \uff08\u9806\u4f1d\u64ad\uff09 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u90e8\u306b\u304a\u3051\u308b\u8aa4\u5dee $\\delta_i$ \u3092\u8a08\u7b97\u3059\u308b\u3002 \u9006\u4f1d\u64ad\u516c\u5f0f\u3092\u5229\u7528\u3057\u3066\u5404\u7d20\u5b50\u306b\u304a\u3051\u308b $\\delta_i$ \u3092\u8a08\u7b97\u3059\u308b\u3002 $$\\delta_i = h_i'(a_i) \\sum_j w_{ji}\\delta_j$$ \u4ee5\u4e0b\u3092\u5229\u7528\u3057\u3066\u5fc5\u8981\u306a\u504f\u5fae\u5206\u4fc2\u6570\u3092\u6c42\u3081\u308b\u3002 $$\\frac{\\partial E_n}{\\partial w_{ji}} = \\delta_j z_i$$ 4. \u5b9f\u88c5 (Implementation) In [2]: import numpy as np import matplotlib.pyplot as plt \u521d\u671f\u5316 \u00b6 In [3]: def Zeros ( shape , dtype = None ): return np . zeros ( shape = shape , dtype = dtype ) def RandomNormal ( shape ): return np . random . normal ( loc = 0 , scale = 0.05 , size = shape ) InitializeHandler = { 'zeros' : Zeros , 'random_normal' : RandomNormal , } def Initializer ( initializer_name ): return InitializeHandler [ initializer_name ] \u8aa4\u5dee\u95a2\u6570 \u00b6 In [4]: class mean_squared_error (): def loss ( y_true , y_pred ): return np . mean ( np . square ( y_pred - y_true ), axis =- 1 ) def diff ( y_true , y_pred ): return y_pred - y_true LossHandler = { 'mean_squared_error' : mean_squared_error , } def LossFunc ( loss_func_name ): return LossHandler [ loss_func_name ] \u5404\u5c64 \u00b6 In [5]: class Layers (): NLayers = 1 def __init__ ( self , name , units ): self . name = f \"Layer {Layers.NLayers} . {name} \" self . outdim = units self . w = None Layers . NLayers += 1 def build ( self , indim ): self . indim = indim self . w = np . c_ [ self . kernel_initializer ( shape = ( self . outdim , self . indim )), self . bias_initializer ( shape = ( self . outdim , 1 )) ] class Input ( Layers ): def __init__ ( self , inputdim ): super () . __init__ ( \"inputs\" , inputdim ) class Dense ( Layers ): def __init__ ( self , units , activation = 'linear' , kernel_initializer = 'random_normal' , bias_initializer = 'zeros' ): \"\"\" @param units : (tuple) dimensionality of the (input space, output space). @param activation : (str) Activation function to use. @param kernel_initializer: (str) Initializer for the `kernel` weights matrix. @param bias_initializer : (str) Initializer for the bias vector. \"\"\" super () . __init__ ( \"dense\" , units ) self . kernel_initializer = Initializer ( kernel_initializer ) self . bias_initializer = Initializer ( bias_initializer ) self . h = ActivationFunc ( activation ) self . z = None self . a = None def forward ( self , input ): \"\"\" @param input: shape=(Din,) \"\"\" z_in = np . append ( input , 1 ) # shape=(Din+1,) a = self . w . dot ( z_in ) # (Dout,Din+1) @ (Din+1,) = (Dout,) z_out = self . h . forward ( a ) # shape=(Dout,) self . z = z_in self . a = a return z_out def backprop ( self , dEdz_out ): \"\"\" @param dEdz_out: shape=(Dout,) \"\"\" dEda = self . h . diff ( self . a ) * dEdz_out # \u03b4, shape=(Dout,) dEdz_in = self . w . T . dot ( dEda ) # (Din+1,Dout) @ (Dout,) = (Din+1,) self . update ( dEda ) return dEdz_in [: - 1 ] # shape=(Din,) term of bias is not propagated. def update ( self , delta , ALPHA = 0.01 ): \"\"\" @param delta: shape=(Dout,) \"\"\" dw = np . outer ( delta , self . z ) # (Dout,) \u00d7 (Din+1,) = (Dout,Din+1) self . w -= ALPHA * dw # update. w \u2192 w + ALPHA*dw \u30e2\u30c7\u30eb\uff08\u5404\u5c64\u306e\u30b9\u30bf\u30c3\u30af\uff09 \u00b6 In [6]: class Sequential (): def __init__ ( self , layer = None ): self . layers = [] self . epochs = 0 if layer is not None : self . add ( layer ) self . loss = None self . config = None def add ( self , layer ): \"\"\"Adds a layer instance.\"\"\" self . layers . append ( layer ) def compile ( self , loss , input_shape = None ): \"\"\" Creates the layer weights. \"\"\" self . loss = LossFunc ( loss ) units = [ l . outdim for l in self . layers ] for i , l in enumerate ( self . layers ): if l . name [ - 6 :] == \"inputs\" : continue l . build ( indim = units [ i - 1 ]) def fit ( self , x_train , y_train , epochs = 1000 ): goal_epochs = self . epochs + epochs digit = len ( str ( goal_epochs )) for e in range ( epochs ): for x , y in zip ( x_train , y_train ): out = self . forward ( x ) self . backprop ( y , out ) self . epochs += 1 y_pred = self . predict ( x_train ) mse = np . mean (( y_pred - y_train ) ** 2 ) if self . epochs % 100 == 99 : print ( f '[{self.epochs+1: {digit} d}/{goal_epochs: {digit} d}] mse={mse: {4} f}' ) def forward ( self , input ): out = input for l in self . layers : if l . name [ - 6 :] == \"inputs\" : continue out = l . forward ( out ) return out def backprop ( self , y_true , out ): dEdz_out = self . loss . diff ( y_true , out ) for l in reversed ( self . layers ): if l . name [ - 6 :] == \"inputs\" : continue dEdz_out = l . backprop ( dEdz_out ) def predict ( self , x_train ): if np . ndim ( x_train ) == 1 : return self . forward ( x_train ) else : return np . array ([ self . forward ( x ) for x in x_train ]) \u5b9f\u88c5\u4f8b \u00b6 In [7]: model = Sequential ( Input ( 1 )) model . add ( Dense ( 3 , activation = \"tanh\" , kernel_initializer = \"random_normal\" , bias_initializer = \"zeros\" )) model . add ( Dense ( 3 , activation = \"tanh\" , kernel_initializer = \"random_normal\" , bias_initializer = \"zeros\" )) model . add ( Dense ( 1 , activation = \"tanh\" )) In [8]: print ( \"Units: {} \" . format ([ l . outdim for l in model . layers ])) print ( \"Layers: {} \" . format ([ l . name for l in model . layers ])) print ( \"Weights: {} \" . format ([ l . w for l in model . layers ])) model . compile ( loss = \"mean_squared_error\" ) print ( \"Weights: {} \" . format ([ l . w . shape if l . w is not None else None for l in model . layers ])) Units: [1, 3, 3, 1] Layers: ['Layer1.inputs', 'Layer2.dense', 'Layer3.dense', 'Layer4.dense'] Weights: [None, None, None, None] Weights: [None, (3, 2), (3, 4), (1, 4)] In [9]: N = 1000 func = lambda x : x ** 2 X = np . linspace ( - 1 , 1 , N ) . reshape ( - 1 , 1 ) Y = np . vectorize ( func )( X ) In [10]: model . fit ( X , Y ) [ 100/1000] mse=0.027993 [ 200/1000] mse=0.007164 [ 300/1000] mse=0.001297 [ 400/1000] mse=0.000846 [ 500/1000] mse=0.000842 [ 600/1000] mse=0.000807 [ 700/1000] mse=0.000766 [ 800/1000] mse=0.000728 [ 900/1000] mse=0.000694 [1000/1000] mse=0.000664 In [11]: Y_pred = model . predict ( X ) In [12]: plt . plot ( X , Y_pred , label = \"Neural Network\" , color = \"red\" ) plt . scatter ( X , Y , s = 1 , label = \"data\" , color = \"blue\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . legend () plt . show () /*\u86cd\u5149\u30da\u30f3(pink)*/ .marker-pink { color: #c45a5a; background: linear-gradient(transparent 70%, #ff66ff 60%); font-weight: bold; } /*\u86cd\u5149\u30da\u30f3(blue) & hover info*/ .marker-info { color: #5C7DC4; background: linear-gradient(transparent 70%, #66FFCC 60%); font-weight: bold; position: relative; cursor: pointer; } .marker-info:hover:before { opacity: 1; } .marker-info:before { content: attr(aria-label); opacity: 0; position: absolute; top: 30px; right: -90px; font-size: 14px; width: 300px; padding: 10px; color: #fff; background-color: #555; border-radius: 3px; pointer-events: none; } In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Neural Network"},{"location":"DeepLearning/Optimizers/","text":"Optimizers 2019-08-18(Sun) DeepLearning \u6700\u9069\u5316(Optimization) \u00b6 \u6a5f\u68b0\u5b66\u7fd2\u3067\u306f\u3001\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3057\u3001\u91cd\u307f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8abf\u6574\u3057\u3066\u640d\u5931\u95a2\u6570\u306e\u5024\u3092\u6e1b\u5c11\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u6027\u80fd\u3092\u9ad8\u3081\u307e\u3059\u3002 \u3053\u306e\u6642\u3001 \u300c\u3044\u3064\u300d \u30fb \u300c\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u300d \u30fb \u300c\u3069\u306e\u3050\u3089\u3044\u300d \u91cd\u307f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8abf\u6574\u3059\u308b\u306e\u304b\u3001\u3092\u6c7a\u3081\u308b\u306e\u304c optimizers \u306e\u5f79\u5272\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u6700\u9069\u5316\u306e\u57fa\u672c\u3067\u3042\u308b \u52fe\u914d\u964d\u4e0b\u6cd5(Gradient Decent) \u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u65b9\u6cd5\u3092\u898b\u3066\u3044\u304d\u3001\u305d\u306e\u5f8c\u52fe\u914d\u964d\u4e0b\u6cd5\u306e\u6b20\u70b9\u3092\u4fee\u6b63\u3057\u305f\u69d8\u3005\u306a\u6700\u9069\u5316\u624b\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u52fe\u914d\u964d\u4e0b\u6cd5(Gradient Decent) \u00b6 \u52fe\u914d\u964d\u4e0b\u6cd5\u306f\u975e\u5e38\u306b\u5358\u7d14\u306a\u30a2\u30a4\u30c7\u30a2\u3067\u3001\u640d\u5931\u95a2\u6570\u306e \u52fe\u914d\u30d9\u30af\u30c8\u30eb\uff08\u6700\u3082\u50be\u304d\u304c\u6025\u306a\u65b9\u5411\uff09 \u306b\u6ce8\u76ee\u3057\u3001\u305d\u306e\u9006\u65b9\u5411\u306b\u9032\u3081\u3070\u95a2\u6570\u306e\u6700\u5c0f\u5024\u306b\u305f\u3069\u308a\u7740\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3060\u308d\u3046\u3001\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002 \u66f4\u65b0\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 $$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial E}{\\partial \\mathbf{w}}$$ \u3053\u306e\u6642\u3001$\\eta$ \u306f \u5b66\u7fd2\u7387 \u3068\u547c\u3070\u308c\u3001\u3069\u306e\u7a0b\u5ea6\u52fe\u914d\u3092\u4e0b\u308b\u304b\u3092\u6c7a\u3081\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3059\u3002 $\\eta$ \u3068\u5b66\u7fd2\u306e\u95a2\u4fc2 \u00b6 $\\eta$ \u306e\u5024\u3092\u5909\u5316\u3055\u305b\u305f\u6642\u306b\u3069\u306e\u3088\u3046\u306b\u5024\u304c\u5909\u5316\u3059\u308b\u304b\u3092\u3001\u4e8c\u4e57\u548c\u8aa4\u5dee $E(\\mathbf{w}) = \\left\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\right\\|^2$ \u306b\u5bfe\u3059\u308b\u7dda\u5f62\u56de\u5e30\u306e\u5834\u5408\u3067\u898b\u3066\u3044\u304d\u307e\u3059\u3002 \u3053\u306e\u6642\u3001\u640d\u5931\u95a2\u6570\u306e\u52fe\u914d\u30d9\u30af\u30c8\u30eb\u306f\u4ee5\u4e0b\u306b\u3088\u3063\u3066\u6c42\u307e\u308a\u307e\u3059\u3002 $$ \\begin{aligned} \\frac{\\partial}{\\partial \\mathbf{w}}E(\\mathbf{w}) &= \\frac{\\partial}{\\partial \\mathbf{w}}\\left\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\right\\|^2\\\\ &= \\frac{\\partial}{\\partial \\mathbf{w}}\\left( \\left\\|\\mathbf{y}\\right\\|^2 -2\\mathbf{w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{y} + \\|\\mathbf{Xw}\\|^2\\right)\\\\ &= -2\\mathbf{X}^{\\mathrm{T}}\\mathbf{y} + \\frac{\\partial}{\\partial \\mathbf{w}}\\left(\\left(\\mathbf{Xw}\\right)^{\\mathrm{T}}\\left(\\mathbf{Xw}\\right)\\right)\\\\ &= -2\\mathbf{X}^{\\mathrm{T}}\\mathbf{y} + \\frac{\\partial}{\\partial \\mathbf{w}}\\left(\\mathbf{w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{Xw}\\right)\\\\ &= -2\\mathbf{X}^{\\mathrm{T}}\\mathbf{y} + \\left(\\mathbf{X}^{\\mathrm{T}}\\mathbf{X} + \\left(\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\\right)^{\\mathrm{T}}\\right)\\mathbf{w}\\\\ &= 2\\mathbf{X}^{\\mathrm{T}}\\left(\\mathbf{Xw} - \\mathbf{y}\\right) \\end{aligned} $$ In [1]: import numpy as np import matplotlib.pyplot as plt % matplotlib inline In [2]: #=== \u30d1\u30e9\u30e1\u30fc\u30bf === xmin = 0 xmax = 1 ETAs = [ 0.01 , 0.1 , 0.5 ] EPOCH = 100 # \u30a8\u30dd\u30c3\u30af\u6570 N = 100 # \u30c7\u30fc\u30bf\u6570 M = 3 # \u91cd\u307f\u306e\u30d1\u30e9\u30e1\u30bf\u6570 #=== \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u751f\u6210/\u91cd\u307f\u306e\u521d\u671f\u5316 === X = np . random . uniform ( xmin , xmax , ( N , 1 )) # shape=(N,1) y = 1 + 2 * X + np . random . randn ( N , 1 ) / 10 # shape=(N,1) b = np . ones ( shape = ( N , 1 )) # shape=(N,1) Xb = np . c_ [ X , b ] # shape=(N,2) #=== \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf === X_test = np . c_ [ np . linspace ( xmin , xmax , N ), np . ones ( shape = ( N )) ] # shape=(N,2) Loss = np . zeros ( shape = ( EPOCH )) fig = plt . figure ( figsize = ( 14 , 4 )) for i , eta in enumerate ( ETAs ): ax = fig . add_subplot ( 1 , 3 , i + 1 ) ax . scatter ( X , y , label = \"data\" ) #=== \u91cd\u307f\u306e\u521d\u671f\u5316 === np . random . seed ( 123 ) w = np . random . randn ( 2 , 1 ) # shape=(2,1) for epoch in range ( EPOCH ): grad = ( 1 / N ) * 2 * Xb . T . dot ( Xb . dot ( w ) - y ) w = w - eta * grad # .reshape(-1,1) if ( epoch < 10 ): y_pred = X_test . dot ( w ) ax . plot ( X_test [:, 0 ], y_pred , color = \"blue\" , alpha = 0.1 * ( epoch + 1 )) y_pred = X_test . dot ( w ) ax . plot ( X_test [:, 0 ], y_pred , color = \"red\" , label = \"prediction\" ) ax . set_xlabel ( \"x\" ) ax . set_ylabel ( \"y\" ) ax . set_title ( f \"$\\eta= {eta} $\" ) ax . legend () plt . tight_layout () plt . show () \u4e0a\u306e\u56f3\u304b\u3089\u3001 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3060\u3051\u3067\u306a\u304f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u3063\u3066\u3082\u5b66\u7fd2\u306e\u4ed5\u65b9\u304c\u5927\u304d\u304f\u7570\u306a\u308b \u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u7279\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306a\u3069\u306e \u5c40\u6240\u89e3\u304c\u3044\u304f\u3064\u3082\u5b58\u5728\u3059\u308b\u95a2\u6570 \u306e\u5834\u5408\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u3063\u3066\u6700\u7d42\u7684\u306b\u53d6\u308b\u5024\u304c\u5927\u304d\u304f\u7570\u306a\u308b\u3053\u3068\u306f\u5bb9\u6613\u306b\u60f3\u50cf\u3067\u304d\u307e\u3059\u3002 \u3067\u3059\u304c\u3001 \u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3064\u3044\u3066\u89e6\u308c\u308b\u306e\u306f\u3053\u3053\u307e\u3067\u3068\u3057\u3001\u3053\u3053\u304b\u3089\u306f\u4ed6\u306e\u6700\u9069\u5316\u624b\u6cd5\u306b\u3064\u3044\u3066\u898b\u3066\u3044\u304d\u307e\u3059\u3002 \u305d\u306e\u4ed6\u306e\u6700\u9069\u5316\u624b\u6cd5 \u00b6 # \u624b\u6cd5 Kerasy 1 SGD \uff08 Momentum SGD \uff09 kerasy.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False) 2 Adagrad kerasy.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0) 3 RMSprop kerasy.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) 4 Adadelta kerasy.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0) 5 Adam kerasy.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) 6 Adamax kerasy.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0) 7 Nadam kerasy.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004) SGD SGD(Stochastic Gradient Decent)\u306f\u57fa\u672c\u7684\u306b\u306f\u52fe\u914d\u964d\u4e0b\u6cd5\u3068\u540c\u3058\u3067\u3059\u304c\u3001\u66f4\u65b0\u3059\u308b\u30bf\u30a4\u30df\u30f3\u30b0\u304c\u7570\u306a\u308a\u307e\u3059\u3002 \u5148\u307b\u3069\u306f $1\\mathrm{epoch}$ \u3054\u3068\u306b\u307e\u3068\u3081\u3066\u66f4\u65b0\u3092\u884c\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001SGD\u3067\u306f $1$ \u30b5\u30f3\u30d7\u30eb\u3054\u3068\u3001\u3082\u3057\u304f\u306f\u30df\u30cb\u30d0\u30c3\u30c1\u3054\u3068\u306b\u66f4\u65b0\u3092\u884c\u3044\u307e\u3059\u3002\u3053\u3046\u3059\u308b\u3053\u3068\u3067\u3001\u5c40\u6240\u89e3\u306b\u306f\u307e\u308a\u8fbc\u307f\u3001\u629c\u3051\u51fa\u305b\u306a\u304f\u306a\u3063\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63fa\u3055\u632f\u308b\u3053\u3068\u3067\u3001\u5c40\u6240\u89e3\u304b\u3089\u629c\u3051\u51fa\u3059\u3053\u3068\u304c\u671f\u5f85\u3067\u304d\u307e\u3059\u3002 \u3068\u3053\u308d\u304c\u3001SGD\u306b\u3082\u6b20\u70b9\u304c\u3042\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u95a2\u6570\u306e\u6700\u5c0f\u5024\u3092\u6c42\u3081\u308b\u554f\u984c\u3092\u8003\u3048\u307e\u3059\u3002 $$f(x,y) = \\frac{1}{200}x^2 + y^2$$ \u3053\u306e\u95a2\u6570\u306e\u52fe\u914d\u306f\u4ee5\u4e0b\u3067\u8868\u3055\u308c\u307e\u3059\u3002 In [3]: x = np . arange ( - 5 , 5 , 1 ) y = np . arange ( - 5 , 5 , 1 ) X , Y = np . meshgrid ( x , y ) dX = - 1 / 100 * X # \u2202f/\u2202x = (1/10)x dY = - 2 * Y # \u2202f/\u2202y = 2y plt . quiver ( X , Y , dX - X , dY - Y , label = \"gradient\" ) plt . scatter ( 0 , 0 , color = \"r\" , label = \"Optimal\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . legend () plt . show () \u3053\u306e\u6642\u3001 \u52fe\u914d\u306e\u65b9\u5411\u304c\u672c\u6765\u306e\u6700\u5c0f\u5024\u3092\u5411\u3044\u3066\u3044\u306a\u3044 \u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001 SGD\u306e\u63a2\u7d22\u306f\u30b8\u30b0\u30b6\u30b0\u306b\u52d5\u304f\u3053\u3068\u304c\u4e88\u6e2c\u3055\u308c\u3001\u3053\u308c\u306f\u975e\u52b9\u7387 \u3067\u3059\u3002 \u305d\u3053\u3067\u3001\u3053\u308c\u3092\u6539\u826f\u3057\u305f\u306e\u304c Momentum SGD \u3067\u3059\u3002 Momentum SGD Momentum SGD\u306f\u3001SGD\u306b \u30e2\u30e1\u30f3\u30bf\u30e0(Momentum,\u904b\u52d5\u91cf) \u306e\u6982\u5ff5\u3092\u52a0\u3048\u305f\u624b\u6cd5\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\u3002 $$ \\begin{aligned} \\mathbf{m} &\\leftarrow \\gamma\\mathbf{m} - \\eta\\frac{\\partial E}{\\partial \\mathbf{w}}\\\\ \\mathbf{w} &\\leftarrow \\mathbf{w} + \\mathbf{m} \\end{aligned} $$ \u3053\u306e $\\mathbf{m}$ \u306f\u300c\u901f\u5ea6\u300d\u306b\u5bfe\u5fdc\u3059\u308b\u5024\u3067\u3042\u308a\u3001 \u300c\u7269\u4f53\u304c\u52fe\u914d\u65b9\u5411\u306b\u529b\u3092\u53d7\u3051\u3001\u305d\u306e\u529b\u306b\u3088\u3063\u3066\u7269\u4f53\u306e\u901f\u5ea6\u304c\u52a0\u7b97\u3055\u308c\u308b\u300d \u3068\u8a00\u3046\u7269\u7406\u6cd5\u5247\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u306a\u304a\u3001$\\gamma$ \u306f $0.9$ \u306a\u3069\u306e\u5024\u3092\u3068\u308a\u3001\u3053\u308c\u306f \u300c\u7269\u4f53\u304c\u4f55\u3082\u529b\u3092\u53d7\u3051\u306a\u3044\u3068\u304d\u306b\u306f\u5f90\u3005\u306b\u6e1b\u901f\u3059\u308b\u300d \u6027\u8cea\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 Adagrad AdaGrad\u306e\u57fa\u672c\u306fSGD\u3067\u3059\u304c\u3001\u30d1\u30e9\u30e1\u30fc\u30bf $\\mathbf{w}$ \u306e \u5404\u6210\u5206\u3054\u3068\u306b\u7570\u306a\u308b\u5b66\u7fd2\u7387(learning rate)\u3092\u4e0e\u3048\u308b \u70b9\u304c\u7570\u306a\u308a\u307e\u3059\u3002 \u203b \u3053\u308c\u4ee5\u964d\u8a18\u53f7\u306e\u7c21\u7565\u5316\u306e\u305f\u3081\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u66f8\u304d\u307e\u3059\u3002 $$g_{i}^{t} :=\\frac{\\partial E}{\\partial w_{i}}\\left(\\mathbf{w}^{t-1}\\right)$$ \u3053\u308c\u306f\u3001 \u300c\u7a00\u306b\u3057\u304b\u89b3\u6e2c\u3055\u308c\u306a\u3044\u7279\u5fb4\u306f\u3001\u305d\u306e\u8ef8\u65b9\u5411\u306e\u52fe\u914d\u304c\u307b\u3068\u3093\u3069\u306e\u5834\u5408\u30bc\u30ed\u306b\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001\u5b66\u7fd2\u304c\u4e0a\u624b\u304f\u884c\u304d\u5c4a\u304b\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\u300d \u3068\u3044\u3046\u554f\u984c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3067\u3001\u5404\u8ef8\u65b9\u5411\u306e\u52fe\u914d\u306e\u4e8c\u4e57\u548c\u3092\u8a18\u61b6\u3057\u3066\u304a\u304d\u3001\u5b66\u7fd2\u7387\u3092\u305d\u306e\u5e73\u65b9\u6839\uff08+\u5fae\u5c0f\u91cf $\\varepsilon$\uff09\u3067\u5272\u308b\u3053\u3068\u3067\u3001\u7a00\u306a\u7279\u5fb4\u306b\u5bfe\u3057\u3066\u5b66\u7fd2\u7387\u3092\u9ad8\u3081\u306b\u3057\u307e\u3059\u3002 $$ \\begin{aligned} v_{i}^{t} &=v_{i}^{t-1}+\\left(g_{i}^{t}\\right)^{2} & \\left(v_i^0=0\\right)\\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{\\sqrt{v_{i}^{t}+\\varepsilon}} \\left(g_{i}^{t}\\right) \\end{aligned} $$ \u3057\u304b\u3057\u3053\u306e\u624b\u6cd5\u3067\u306f\u3001\u9006\u306b \u4e00\u5ea6\u52fe\u914d\u306e\u6025\u306a\u5834\u6240\u3092\u901a\u308b\u3068\u3001\u305d\u308c\u4ee5\u964d\u305d\u306e\u8ef8\u306e\u5b66\u7fd2\u7387\u304c\u5c0f\u3055\u304f\u306a\u3063\u3066\u3057\u307e\u3046 \u3068\u3044\u3046\u554f\u984c\u70b9\u3082\u3042\u308a\u307e\u3059\u3002 RMSprop Adagrad\u3067\u306f\u3001 \u66f4\u65b0\u304c\u7e70\u308a\u8fd4\u3055\u308c\u308b\u307b\u3069\u5b9f\u8cea\u7684\u306a\u5b66\u7fd2\u7387\u304c\u5c0f\u3055\u304f\u306a\u308a\u3001\u5024\u306e\u66f4\u65b0\u304c\u6b62\u307e\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002 \u3053\u308c\u3060\u3068\u5b66\u7fd2\u304c\u5341\u5206\u306b\u884c\u304d\u5c4a\u304b\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u3001 \u904e\u53bb\u306e\u52fe\u914d\u3092\u305d\u306e\u307e\u307e\u8a18\u61b6\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u300c\u904e\u53bb\u306e\u7d2f\u7a4d\u3068\u76f4\u8fd1\u306e\u52fe\u914d\u3092 $\\gamma:1-\\gamma$ \u306e\u6bd4\u7387\u3067\u8db3\u3057\u306a\u304c\u3089\u8a18\u61b6\u300d \u3057\u307e\u3059\u3002 $$ \\begin{aligned} v_{i}^{t} &=\\gamma v_{i}^{t-1}+(1-\\gamma)\\left(g_{i}^{t}\\right)^{2} & \\left(v_i^0=0\\right)\\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{\\sqrt{v_{i}^{t}+\\varepsilon}} \\left(g_{i}^{t}\\right) \\end{aligned} $$ \u3059\u308b\u3068\u3001$v_i^t$ \u306e\u66f4\u65b0\u5f0f\u304c $$ \\begin{cases} \\begin{aligned} v_{i}^{t}&=\\left(g_{i}^{t}\\right)^{2}+\\left(g_{i}^{t-1}\\right)^{2}+\\left(g_{i}^{t-2}\\right)^{2}+\\cdots & \\text{(Adagrad)}\\\\ v_{i}^{t}&=(1-\\gamma) \\sum_{l=1}^{t} \\gamma^{t-l}\\left(g_{i}^{l}\\right)^{2} & \\text{(RMSprop)} \\end{aligned} \\end{cases} $$ \u3068\u306a\u308b\u3053\u3068\u304b\u3089\u3001 \u904e\u53bb\u306e\u60c5\u5831\u304c\u6307\u6570\u7684\u306b\u6e1b\u8870\u3057\u306a\u304c\u3089\u7d2f\u7a4d\u3055\u308c\u3066\u3044\u308b \u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002 Adadelta Adadelta\u3082Adagrad\u3092\u6539\u826f\u3057\u305f\u624b\u6cd5\u3067\u3001$w_i^t$ \u304c\u5927\u304d\u304f\u66f4\u65b0\u3055\u308c\u305f\u5834\u5408\u3001\u3055\u3089\u306b\u63a2\u7d22\u8ddd\u96e2\u304c\u5e83\u304c\u308b\u3088\u3046\u306bRMSprop\u306b\u88dc\u6b63\u304c\u5165\u3063\u3066\u3044\u307e\u3059\u3002 $$ \\begin{aligned} v_{i}^{t} &=\\gamma v_{i}^{t-1}+(1-\\gamma)\\left(g_{i}^{t}\\right)^{2} & \\left(v_i^0=0\\right) \\\\ s_{i}^{t} &=\\gamma s_{i}^{t-1}+(1-\\gamma)\\left(\\Delta w_{i}^{t-1}\\right)^{2} & \\left(s_i^0=0\\right) \\\\ \\Delta w_{i}^{t} &=-\\frac{\\sqrt{s_{i}^{t}+\\epsilon}}{\\sqrt{v_{i}^{t}+\\epsilon}} g_{i}^{t} \\\\ w_{i}^{t+1} &=w_{i}^{t}+\\Delta w_{i}^{t} \\end{aligned} $$ \u203b \u306a\u304a\u3001\u5b66\u7fd2\u7387 $\\eta$ \u3092\u3055\u3089\u306b\u52a0\u3048\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002 Adam Adam\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u624b\u6cd5\u3067\u3059\u3002 \u52fe\u914d\u306e1\u4e57\u3092\u5229\u7528\u3057\u305f\u30e2\u30e1\u30f3\u30bf\u30e0\uff08$m$\uff09 \u52fe\u914d\u306e2\u4e57\u3092\u5229\u7528\u3057\u305fAdagrad\uff08$v$\uff09 \u66f4\u65b0\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 $$ \\begin{aligned} m_{i}^{t} &=\\beta_{1} m_{i}^{t-1}+\\left(1-\\beta_{1}\\right) g_{i}^{t} & \\left(m_i^0=0\\right)\\\\ v_{i}^{t} &=\\beta_{2} v_{i}^{t-1}+\\left(1-\\beta_{2}\\right)\\left(g_{i}^{t}\\right)^{2} & \\left(v_i^0=0\\right)\\\\ \\hat{m}_{i}^{t} &=\\frac{m_{i}^{t}}{1-\\beta_{1}^{t}} \\\\ \\hat{v}_{i}^{t} &=\\frac{v_{i}^{t}}{1-\\beta_{2}^{t}} \\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{\\sqrt{\\hat{v}_{i}^{t}}+\\varepsilon}\\hat{m}_{i}^{t} \\end{aligned} $$ Adamax Adam\u306e $v_i^t$ \u306e\u66f4\u65b0\u5f0f\u306b\u73fe\u308c\u308b $\\beta_2$ \u3092 $\\beta_2^p$ \u306e\u5f62\u306b\u62e1\u5f35\u3059\u308b\u3053\u3068\u3092\u8003\u3048\u307e\u3059\u3002 $p$ \u306e\u5024\u304c\u5927\u304d\u304f\u306a\u308b\u3068\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\u305f\u3081\u3001\u4e00\u822c\u306b $p=1,2$ \u3064\u307e\u308a\u3001$l_1$ \u30ce\u30eb\u30e0, $l_2$ \u30ce\u30eb\u30e0\u304c\u597d\u3093\u3067\u4f7f\u308f\u308c\u307e\u3059\u304c\u3001$l_{\\infty}$ \u30ce\u30eb\u30e0\u3082\u5b89\u5b9a\u7684\u306a\u632f\u308b\u821e\u3044\u3092\u898b\u305b\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u307e\u3059\u3002 Adamax\u306f\u3053\u306e $l_{\\infty}$ \u30ce\u30eb\u30e0\u3092\u66f4\u65b0\u5f0f\u306b\u7528\u3044\u305f\u624b\u6cd5\u3067\u3001\u4ee5\u4e0b\u306e\u66f4\u65b0\u5f0f\u3092\u7528\u3044\u3066\u91cd\u307f\u306e\u66f4\u65b0\u3092\u884c\u3044\u307e\u3059\u3002 $$ \\begin{aligned} v_i^{t} &=\\beta_{2}^{\\infty} v_i^{t-1}+\\left(1-\\beta_{2}^{\\infty}\\right)\\left|g_i^{t}\\right|^{\\infty} \\\\ &=\\max \\left(\\beta_{2} \\cdot v_i^{t-1},\\left|g_i^{t}\\right|\\right)\\\\ w_{i}^{t+1} &=w_{i}^{t}-\\frac{\\eta}{v_i^t}\\hat{m}_{i}^{t} \\end{aligned} $$ Nadam Adam\u306f\u3001 \u300c\u52fe\u914d\u306e1\u4e57\u3092\u5229\u7528\u3057\u305f\u30e2\u30e1\u30f3\u30bf\u30e0\uff08$m$\uff09\u300d \u3068 \u300c\u52fe\u914d\u306e2\u4e57\u3092\u5229\u7528\u3057\u305fAdagrad\uff08$v$\uff09\u300d \u306e2\u3064\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u624b\u6cd5\u3067\u3059\u304c\u3001Nadam\u3067\u306f\u3001 \u30e2\u30e1\u30f3\u30bf\u30e0\u6cd5 \u306e\u4ee3\u308f\u308a\u306b\u3001\u305d\u308c\u3092\u62e1\u5f35\u3055\u305b\u305f \u30cd\u30b9\u30c6\u30ed\u30d5\u306e\u52a0\u901f\u52fe\u914d\u6cd5(Nesterov's Accelerated Gradient Method) \u3092\u5229\u7528\u3057\u307e\u3059\u3002 \u30cd\u30b9\u30c6\u30ed\u30d5\u306e\u52a0\u901f\u52fe\u914d\u6cd5 \u306f\u3001\u30e2\u30e1\u30f3\u30bf\u30e0\u6cd5\u3068\u306f\u7570\u306a\u308a\u3001\u6163\u6027\u306b\u3088\u308b\u66f4\u65b0\u306e \u5f8c\u306b \u52fe\u914d\u306b\u3088\u308b\u66f4\u65b0\u304c\u884c\u308f\u308c\u307e\u3059\u3002 Classical Momentum Nesterov's accelerated gradient $$ \\begin{aligned} \\mathbf{g}_t &\\leftarrow \\nabla_{\\theta_{t-1}}f\\left(\\theta_{t-1}\\right)\\\\ \\mathbf{m}_t &\\leftarrow \\mu\\mathbf{m}_{t-1} + \\mathbf{g}_t\\\\ \\theta_{t} &\\leftarrow \\theta_{t-1} - \\eta\\mathbf{m}_t \\end{aligned}$$|$$\\begin{aligned} \\mathbf{g}_t &\\leftarrow \\nabla_{\\theta_{t-1}}f\\left(\\theta_{t-1}-\\eta\\mu\\mathbf{m}_{t-1}\\right)\\\\ \\mathbf{m}_t &\\leftarrow \\mu\\mathbf{m}_{t-1} + \\mathbf{g}_t\\\\ \\theta_{t} &\\leftarrow \\theta_{t-1} - \\eta\\mathbf{m}_t \\end{aligned} $$| In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Optimizers"},{"location":"DeepLearning/TrainOnlyDense/","text":"TrainOnlyDense 2019-08-31(Sat) DeepLearning In [1]: import cv2 import numpy as np import matplotlib.pyplot as plt In [2]: from kerasy.layers.convolutional import Conv2D from kerasy.layers.pool import MaxPooling2D from kerasy.layers.core import Input , Dense , Flatten from kerasy.engine.sequential import Sequential In [3]: cv_path = \"/Users/iwasakishuto/Github/portfolio/Kerasy/doc/theme/img/MNIST-sample/0.png\" In [4]: image = np . expand_dims ( cv2 . imread ( cv_path , 0 ), axis = 2 ) / 255 In [5]: plt . imshow ( cv2 . cvtColor ( cv2 . imread ( cv_path ), cv2 . COLOR_BGR2RGB )) plt . show () In [6]: model = Sequential () model . add ( Input ( input_shape = ( 28 , 28 , 1 ))) model . add ( Conv2D ( filters = 32 , kernel_size = ( 3 , 3 ), activation = 'linear' , padding = \"same\" )) model . add ( Conv2D ( filters = 64 , kernel_size = ( 3 , 3 ), activation = 'linear' , padding = \"same\" )) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 128 , activation = 'tanh' )) model . add ( Dense ( 10 , activation = 'softmax' )) In [7]: model . compile ( optimizer = 'sgd' , loss = \"categorical_crossentropy\" ) In [8]: for layer in model . layers : print ( f \"== {layer.name} ==\" ) print ( f \"output shape: {layer.output_shape} \" ) print ( f \"kernel shape: {layer._losses.get('kernel', np.zeros(shape=())).shape}\" ) print ( f \"bias shape : {layer._losses.get('bias', np.zeros(shape=())).shape}\" ) == input_1 == output shape: (28, 28, 1) kernel shape: () bias shape : () == conv2d_1 == output shape: (28, 28, 32) kernel shape: (3, 3, 1, 32) bias shape : (32,) == conv2d_2 == output shape: (28, 28, 64) kernel shape: (3, 3, 32, 64) bias shape : (64,) == maxpooling2d_1 == output shape: (14, 14, 64) kernel shape: () bias shape : () == flatten_1 == output shape: (12544,) kernel shape: () bias shape : () == dense_1 == output shape: (128,) kernel shape: (128, 12544) bias shape : (128, 1) == dense_2 == output shape: (10,) kernel shape: (10, 128) bias shape : (10, 1) In [9]: #=== Train Only Dense Layer === model . layers [ 1 ] . trainable = False model . layers [ 2 ] . trainable = False In [10]: # #=== Train Only Convolutional Layer === # model.layers[-1].trainable = False # model.layers[-2].trainable = False In [11]: x_train = np . expand_dims ( image , axis = 0 ) In [12]: original_pred = model . predict ( x_train ) print ( f \"original prediction: {np.argmax(original_pred)} \\n {original_pred} \" ) original prediction: 2 [[0.09963766 0.1004707 0.10146399 0.09917924 0.10046313 0.0988132 0.09983802 0.10139949 0.10078803 0.09794654]] In [13]: ans_label = np . argmin ( original_pred ) print ( f \"ans_label: {ans_label} \" ) ans_label: 9 In [14]: y_true = np . zeros ( shape = ( 10 ,)) y_true [ ans_label ] = 1 y_true = y_true . reshape ( 1 , - 1 ) In [15]: model . fit ( x = x_train , y = y_true , epochs = 30 ) [1/30] [2/30] [3/30] [4/30] [5/30] [6/30] [7/30] [8/30] [9/30] [10/30] [11/30] [12/30] [13/30] [14/30] [15/30] [16/30] [17/30] [18/30] [19/30] [20/30] [21/30] [22/30] [23/30] [24/30] [25/30] [26/30] [27/30] [28/30] [29/30] [30/30] In [16]: final_pred = model . predict ( x_train ) print ( f \" Answer: {ans_label} \" ) print ( f \" original prediction: {np.argmax(original_pred)} \\n {original_pred} \" ) print ( f \" final prediction: {np.argmax(final_pred)} \\n {final_pred} \" ) print ( f \" Difference: \\n {np.where((final_pred-original_pred)>0, '+', '-')}\" ) Answer: 9 original prediction: 2 [[0.09963766 0.1004707 0.10146399 0.09917924 0.10046313 0.0988132 0.09983802 0.10139949 0.10078803 0.09794654]] final prediction: 9 [[0.00293264 0.002827 0.00315045 0.00266858 0.00292554 0.00257199 0.00294734 0.00316633 0.00319796 0.97361218]] Difference: [['-' '-' '-' '-' '-' '-' '-' '-' '-' '+']] In [17]: from Functions.TexWriter import CNNbackprop2tex In [18]: kh , kw = ( 3 , 3 ) sh , sw = ( 1 , 1 ) IH , IW = ( 5 , 5 ) OH , OW = ( 3 , 3 ) In [19]: CNNbackprop2tex ( IH , IW , OH , OW , kh , kw , sh , sw , path = \"sample.tex\" ) Your tex file saved at '/Users/iwasakishuto/Github/portfolio/Kerasy/pelican/pelican-src/DeepLearning/sample.tex' Please run the following code. Then, /Users/iwasakishuto/Github/portfolio/Kerasy/pelican/pelican-src/DeepLearning/sample.pdf will be created. ============================== $ platex /Users/iwasakishuto/Github/portfolio/Kerasy/pelican/pelican-src/DeepLearning/sample.tex $ dvipdfmx /Users/iwasakishuto/Github/portfolio/Kerasy/pelican/pelican-src/DeepLearning/sample.dvi ! platex /Users/iwasakishuto/Github/portfolio/Kerasy/pelican/pelican-src/DeepLearning/sample.tex ! dvipdfmx /Users/iwasakishuto/Github/portfolio/Kerasy/pelican/pelican-src/DeepLearning/sample.dvi if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"TrainOnlyDense"},{"location":"MachineLearning/Principal Component Analysis/","text":"Principal Component Analysis 2019-11-26(Tue) MachineLearning Purpose The purpose of decomposition (dimensionality reduction, feature extraction) is to find the \"best\" principal subspace where data are expressed with much less dimensions, but keep the information as much as possible. Notebook Example Notebook: Kerasy.examples.decompositions.ipynb PCA class kerasy . ML . decomposition . PCA ( n_components = None ) Consider a dataset \\(\\{\\mathbf{x}_n\\}\\) . If we consider a \\(D\\) -dimensional vector \\(\\mathbf{u}_1\\) , each data point \\(\\mathbf{x}_n\\in\\mathbb{R}^{D}\\) is projected onto a scalar value \\(\\mathbf{u}_1^T\\mathbf{x}_n\\) . In that (one-dimensional) space, Mean: $$\\tilde{\\mathbf{x}} = \\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$$ Variance: $$\\frac{1}{N}\\sum_{n=1}^N\\left\\{\\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\tilde{\\mathbf{x}}\\right\\}^2 = \\mathbf{u}_1^T\\mathbf{Su}_1\\quad\\mathbf{S}= \\frac{1}{N}\\sum_{n=1}^N\\left(\\mathbf{x}-\\tilde{\\mathbf{x}}\\right)\\left(\\mathbf{x}-\\tilde{\\mathbf{x}}\\right)^T$$ We now maximize the projected variance \\(\\mathbf{u}_1^T\\mathbf{Su}_1\\quad\\mathbf{S}\\) with respect to \\(\\mathbf{u}_1\\) , because we can think \"Larger variance\" \u2194\ufe0e \"Features are more expressed\" . To avoid \\(\\|\\mathbf{u}_1\\|\\rightarrow\\infty\\) , we introduce the normalization condition \\(\\mathbf{u}_1^T\\mathbf{u}_1=1\\) using Lagrange multiplier \\(\\lambda_1\\) . Then, the object function is given by $$\\begin{aligned} L &= \\mathbf{u}_1^T\\mathbf{Su}_1 + \\lambda_1\\left(1-\\mathbf{u}_1^T\\mathbf{u}_1\\right)\\\\ \\frac{\\partial L}{\\partial \\mathbf{u}_1} &= 2\\mathbf{S}\\mathbf{u}_1 - 2\\lambda_1\\mathbf{u}_1 = 0\\\\ \\therefore\\mathbf{u}_1^T\\mathbf{Su}_1 &= \\lambda_1 \\quad (\\because \\text{left-multiplied $\\mathbf{u}_1^T$}) \\end{aligned}$$ The variance will be a maximum when we set \\(\\mathbf{u}_1\\) eaual to the eigenvector having the largest eigenvalue \\(\\lambda_1\\) . This eigenvector is known as the first principal component. We can define additional principal components in the same way. Summary If we consider an M-dimensional projection space, the optimal linear projection is defined by the M eigenvectors u1,...,uM of the data covariance matrix S corresponding to the M largest eigenvalues \u03bb1,...,\u03bbM. Kernel PCA class kerasy . ML . decomposition . KernelPCA ( n_components = None , kernel = \"gaussian\" , ** kernelargs ) The main idea is same with PCA, but we will perform it in the feature space , which implicitly defines a nonlinear principal component model in the original data space. If we assume that the projected data set has zero mean , The covariance matrix in feature space is given by $$ \\mathbf{C} = \\frac{1}{N}\\sum_{n=1}^N\\phi(\\mathbf{x}_n)\\phi(\\mathbf{x}_n)^T $$ and its eigenvector expansion is defined by $$\\begin{aligned} \\mathbf{C}\\mathbf{v}_i &= \\lambda_i\\mathbf{v}_i\\\\ \\frac{1}{N}\\sum_{n=1}^N\\phi(\\mathbf{x}_n)\\underset{\\text{scalar}}{\\left\\{\\phi(\\mathbf{x}_n)^T\\mathbf{v}_i\\right\\}} &= \\lambda_i\\mathbf{v}_i\\quad (\\ast)\\\\ \\end{aligned}$$ From this equation, we see that the vector \\(\\mathbf{v}_i\\) is given by a linear combination of the \\(\\phi(\\mathbf{x}_n)\\) $$\\mathbf{v}_i = \\sum_{n=1}^N a_{in}\\phi(\\mathbf{x}_n)$$ Substituting this expansion back into the eigenvector equation \\((\\ast)\\) , we obtain $$\\begin{aligned} \\frac{1}{N}\\sum_{n=1}^N\\phi(\\mathbf{x_n})\\phi(\\mathbf{x}_n)^T\\sum_{m=1}^Na_{im}\\phi(\\mathbf{x}_m) &= \\lambda_i\\sum_{n=1}^Na_{in}\\phi(\\mathbf{x}_n)\\\\ \\frac{1}{N}\\sum_{n=1}^Nk(\\mathbf{x}_l,\\mathbf{x}_n)\\sum_{m=1}^Na_{i,m}k(\\mathbf{x}_n,\\mathbf{x}_m) &= \\lambda_i\\sum_{n=1}^Na_{in}k(\\mathbf{x}_l,\\mathbf{x}_n)\\quad(\\because\\text{multiplied $\\phi(\\mathbf{x}_l)^T$})\\\\ \\mathbf{K}^2\\mathbf{a}_i &= \\lambda_iN\\mathbf{Ka}_i\\quad (\\because\\text{written in matrix notation})\\\\ \\end{aligned}$$ We can find solutions for \\(\\mathbf{a}_i\\) by solving the following eigenvalue problem. The variance in feature space will be a maximum when we set \\(\\mathbf{a}_1\\) eaual to the eigenvector having the largest eigenvalue \\(\\lambda_i\\) . $$\\mathbf{Ka}_i = \\lambda_iN\\mathbf{a}_i$$ Having solved the eigenvector problem, the projection of a point \\(\\mathbf{x}\\) onto eigenvector \\(i\\) is given by $$y_i(\\mathbf{x}) = \\phi(\\mathbf{x})^T\\mathbf{v}_i = \\sum_{n=1}^Na_{in}\\phi(\\mathbf{x})^T\\phi(\\mathbf{x}_n) = \\sum_{n=1}^Na_{in}k(\\mathbf{x},\\mathbf{x}_n)$$ If projected data set doesn't have zero mean , the projected data points after centralizing are given by $$\\tilde{\\phi(\\mathbf{x}_n)} = \\phi(\\mathbf{x}_n)-\\frac{1}{N}\\sum_{l=1}^N\\phi(\\mathbf{x}_l)$$ and the corresponding elements of the Gram matrix are given by $$\\tilde{\\mathbf{K}} = \\mathbf{K} - \\mathbf{1_NK} - \\mathbf{K1_N} +\\mathbf{1_NK1_N}$$ where \\(\\mathbf{1}_N\\) denotes the \\(N\\times N\\) matrix in which every element takes the value \\(1/N\\) . Warning I don't understand clearly how to deal with new data point xn. SNE Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between datapoints into conditional probabilities . The similarity of datapoint \\(\\mathbf{x}_j\\) to datapoint \\(\\mathbf{x}_i\\) is conditional probability \\(p_{j|i}\\) , that \\(\\mathbf{x}_i\\) would pick \\(\\mathbf{x}_j\\) as its neighbor if neighbors were picked in propotion to their probability density under Gaussian centered at \\(\\mathbf{x}_i\\) . Mathmatically, the conditional probability \\(p_{j|i}\\) is given by $$p_{j|i} = \\frac{\\exp\\left(-\\|\\mathbf{x}_i-\\mathbf{x}_j\\|^2/2\\boldsymbol{\\sigma}_i^2\\right)}{\\sum_{k\\neq i}\\exp\\left(-\\|\\mathbf{x}_i-\\mathbf{x}_k\\|^2/2\\boldsymbol{\\sigma}_i^2\\right)}$$ Because we are only interested in modeling pairwise similarities, we set the value of \\(p_{i|i}\\) to zero. For the low-dimensional counterparts \\(\\mathbf{y}_i\\) and \\(\\mathbf{y}_j\\) of the high-dimensional data points \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) , it is possible to compute a similar conditional probability, which we denote by \\(q_{j|i}\\) $$q_{j|i} = \\frac{\\exp\\left(-\\|\\mathbf{y}_i-\\mathbf{y}_j\\|^2\\right)}{\\sum_{k\\neq i}\\exp\\left(-\\|\\mathbf{y}_i-\\mathbf{y}_k\\|^2\\right)}$$ Again, we set \\(q_{i|i} = 0\\) , and the variance of the Gaussian to \\(\\frac{1}{\\sqrt{2}}\\) Then, the cost function is given by $$C = \\sum_i\\mathrm{KL}(P_i\\|Q_i) = \\sum_i\\sum_j p_{j|i}\\log\\frac{p_{j|i}}{q_{j|i}}$$ in which \\(P_i\\) represents the conditional probability distribution over all other data points given data points \\(\\mathbf{x}_i\\) The remaining parameter to be selected is the variance \\(\\boldsymbol{\\sigma_i}\\) . It is defined to produce a \\(P_i\\) with a fixed perplexity that is specified by the user. The perplexity is defined as $$Perp(P_i) = 2^{H(P_i)},\\quad H(P_i) = -\\sum_jp_{j|i}\\log_2p_{j|i}$$ The minimization of the cost function is performed using a gradient descent method. The gradient has a surprisingly simple form $$\\frac{\\partial C}{\\partial \\mathbf{y}_i} = 2\\sum_j\\left(p_{j|i} - q_{j|i} + p_{i|j} - q_{i|j}\\right)\\left(\\mathbf{y}_i-\\mathbf{y}_j\\right)$$ tSNE class kerasy . ML . decomposition . PCA ( initial_momentum = 0.5 , final_momoentum = 0.8 , eta = 500 , min_gain = 0.1 , tol = 1e-5 , prec_max_iter = 50 ) As an alternative to minimizing the sum of the Kullback-Leibler divergence between a joint probability distribution \\(p_{j|i}\\) and \\(q_{j|i}\\) , it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution: $$C = KL\\left(P\\|Q\\right) = \\sum_i\\sum_jp_{ij}\\log\\frac{p_{ij}}{q_{ij}}$$ In t-SNE, - Employ a Student t-distribution with one degree of freedom (which is the same as a Cauchy distribution) as the heavy-tailed distribution in the low-dimensional map. Using this distribution, the joint probabilities \\(q_{ij}\\) are defined as $$q_{ij} = \\frac{\\left(1 + \\|\\mathbf{y}_i-\\mathbf{y}_j\\|^2\\right)^{-1}}{\\sum_{k\\neq l}\\left(1 + \\|\\mathbf{y}_k-\\mathbf{y}_l\\|^2\\right)^{-1}}$$ - If there is an outlier \\(\\mathbf{x}_i\\) , low-dimensional map point \\(\\mathbf{y}_i\\) has very little effect on the cost function, so set $$p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}$$ to ensure that \\(\\sum_jp_{ij}>\\frac{1}{2n}\\) for all datapoints \\(\\mathbf{x}_i\\) The gradient is given by $$\\frac{\\partial C}{\\partial \\mathbf{y}_i} = 4\\sum_j\\left(p_{ij} - q_{ij}\\right) \\left(\\mathbf{y}_i - \\mathbf{y}_j\\right)\\left(1 + \\|\\mathbf{y}_i - \\mathbf{y}_j\\|^2\\right)^{-1}$$ Reference Visualizing Data using t-SNE Pattern Recognition and Machine Learning by Christopher Bishop if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" fonts: [['STIX', 'TeX']],\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Principal Component Analysis"},{"location":"MachineLearning/Support Vector Machine/","text":"Support Vector Machine 2019-11-26(Tue) MachineLearning Notebook Example Notebook: Kerasy.examples.svm.ipynb Hard SVC class kerasy . ML . svm . hardSVC ( kernel = \"gaussian\" , ** kernelargs ) Start from the \"two-class classification\" problem using \"linear models\" , which \"separate the training data set linearly\" . Under the assumption, we can denote The training data: $$D = \\left\\{(t_n,\\mathbf{x}_n)\\in \\left\\{-1, 1\\right\\} \\times \\mathbb{R}^m | n = 1,\\ldots, N\\right\\}$$ linear models: $$y(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x}) + b$$ For all training data points: $$t_ny(\\mathbf{x}_n) > 0$$ Margin is given by: $$\\frac{1}{\\|\\mathbf{w}\\|}$$ \u203b If there are any uncertain points, please check here . Points In support vector machines, the decision boundary is chosen to be the one for which the margin is maximized. From this discussion above, the optimization problem (Minimizing) is given by $$L\\left(\\mathbf{w},b,\\mathbf{a}\\right) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 - \\sum_{n=1}^Na_n\\left\\{t_n\\left(\\mathbf{w}^T\\phi(\\mathbf{x}_n) + b\\right) -1 \\right\\}$$ where \\(\\mathbf{a} = \\left(a_1,\\ldots,a_N\\right)^T\\) are the Lagrange multipliers \\(a_n\\gg0\\) . Warning Note the minus sign in front of the Lagrange multiplier term. Check HERE . To solve this optimization problem, set the derivatives of \\(L\\left(\\mathbf{w},b,\\mathbf{a}\\right)\\) with respect to \\(\\mathbf{w}\\) and \\(b\\) equal to zero. $$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{w}} &= \\mathbf{w}-\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x}_n) =0 &\\therefore \\mathbf{w}^{\\ast}=\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x}_n)\\\\ \\frac{\\partial L}{\\partial b} &= -\\sum_{n=1}^N a_nt_n = 0 &\\therefore\\sum_{n=1}^Na_n^{\\ast}t_n = 0\\\\ \\end{aligned} $$ Eliminating \\(\\mathbf{w}\\) and \\(b\\) from \\(L\\left(\\mathbf{w},b,\\mathbf{a}\\right)\\) using these conditions then gives the dual representation of the maximum margin problem in which we maximize $$ \\begin{aligned} \\tilde{L}\\left(\\mathbf{a}\\right) &= \\frac{1}{2}\\|\\mathbf{w}\\|^2 - \\mathbf{w}^T\\sum_{n=1}^Na_nt_n\\phi(\\mathbf{x}_n) - b\\sum_{n=1}^Na_nt_n + \\sum_{n=1}^Na_n\\\\ &= -\\frac{1}{2}\\|\\mathbf{w}\\|^2 - b\\cdot0 + \\sum_{n=1}^Na_n\\\\ &= \\sum_{n=1}^Na_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^Na_na_mt_nt_mk\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)\\quad k\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)=\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\end{aligned} $$ with respect to \\(\\mathbf{a}\\) subject to the constraints $$ \\begin{aligned} a_n & \\geq 0,\\quad n=1,\\ldots,N\\\\ \\sum_{n=1}^Na_nt_n &= 0 \\end{aligned} $$ This takes the form of a quadratic programming problems , and there are some well-known algorithms to solve them. (ex. SMO; Sequential Minimal Optimization ) Once optimal \\(\\mathbf{a}\\) are calculated, the decision boundary is given by $$y(\\mathbf{x}) = \\mathbf{w}^{\\ast T}\\phi(\\mathbf{x})+b = \\sum_{n=1}^N a_nt_nk(\\mathbf{x},\\mathbf{x}_n) + b$$ In terms of bias parameter \\(b\\) , we can calculate it by noting that any support vector \\(\\mathbf{x}_n\\) satisfies \\(t_ny(\\mathbf{x}_n) = 1\\) . $$ \\begin{aligned} &t_n\\left(\\sum_{m\\in\\mathcal{S}}a_mt_mk(\\mathbf{x}_n,\\mathbf{x}_m) + b\\right) = 1\\\\ &\\therefore b^{\\ast}= \\frac{1}{N_{\\mathcal{S}}}\\sum_{n\\in\\mathcal{S}}\\left(t_n-\\sum_{n\\in\\mathcal{S}}a_mt_mk(\\mathbf{x}_n,\\mathbf{x}_m)\\right) \\end{aligned} $$ Points To minimize the numerical error, it is more appropriate to average these equation over all support vectors instead of choosing an arbitrarily support vector. Soft SVC class kerasy . ML . svm . SVC ( kernel = \"gaussian\" , C = 10 , ** kernelargs ) In the Hard SVC , we have assumed that the training data points are linearly separable in the feature space \\(\\phi(\\mathbf{x})\\) . However, in practice, this property may cause the bad effect on the generalization performance . Therefore, we introduce slack variables, \\(\\xi_n\\gg0\\) to allow some of the training points to be misclassified. $$ t_ny(\\mathbf{x}_n) \\geq 1 \\Longrightarrow t_ny(\\mathbf{x}_n) \\geq 1 - \\xi_n $$ Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary $$C\\sum_{n=1}^N\\xi_n + \\frac{1}{2}\\|\\mathbf{w}\\|$$ \\(C\\) controls the trade-off between minimizing training errors and controlling model complexity. The corresponding Lagrangian is given by $$L(\\mathbf{w},b,\\boldsymbol{\\xi},\\mathbf{a},\\boldsymbol{\\mu}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{n=1}^N\\xi_n - \\sum_{n=1}^{N}a_n\\left\\{t_n\\left(\\mathbf{w}^T\\phi(\\mathbf{x}_n) + b\\right) - 1 + \\xi_n\\right\\} - \\sum_{n=1}^N\\mu_n\\xi_n$$ Same as before, we set the derivatives of \\(L\\left(\\mathbf{w},b,\\mathbf{a}\\right)\\) with respect to \\(\\mathbf{w}, b\\) and \\(\\{\\xi_n\\}\\) equal to zero. $$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf{w}} &= \\mathbf{w}-\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x}_n) =0 &\\therefore \\mathbf{w}^{\\ast}=\\sum_{n=1}^N a_nt_n\\phi(\\mathbf{x}_n)\\\\ \\frac{\\partial L}{\\partial b} &= -\\sum_{n=1}^N a_nt_n = 0 &\\therefore\\sum_{n=1}^Na_n^{\\ast}t_n = 0\\\\ \\frac{\\partial L}{\\partial \\xi_n} &= C-a_n-\\mu_n &\\therefore a_n = C-\\mu_n \\end{aligned} $$ Eliminating them from \\(L\\) using these conditions then gives the dual representation of the maximum margin problem in which we maximize $$ \\begin{aligned} \\tilde{L}\\left(\\mathbf{a}\\right) &= \\frac{1}{2}\\|\\mathbf{w}\\|^2 +C\\sum_{n=1}^N\\xi_n - \\|\\mathbf{w}\\|^2 - b\\sum_{n=1}^Na_nt_n + \\sum_{n=1}^Na_n - \\sum_{n=1}^Na_n\\xi_n - \\sum_{n=1}^N\\mu_n\\xi_n\\\\ &= -\\frac{1}{2}\\|\\mathbf{w}\\|^2 - b\\cdot0 + \\sum_{n=1}^Na_n + \\sum_{n=1}^N\\left(C - a_n - \\mu_n\\right)\\xi_n\\\\ &= \\sum_{n=1}^Na_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^Na_na_mt_nt_mk\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)\\quad k\\left(\\mathbf{x}_n,\\mathbf{x}_m\\right)=\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\end{aligned} $$ with respect to \\(\\mathbf{a}\\) subject to the constraints $$ \\begin{aligned} &a_n \\geq 0,\\mu_n\\geq 0 \\quad n=1,\\ldots,N\\\\ &\\therefore 0\\leq a_n \\leq C\\\\ &\\sum_{n=1}^Na_nt_n = 0 \\end{aligned} $$ In this case, As shown in the table below, we consider only the data points having \\(0 < a_n < C\\) to calculate the biasparameter \\(b\\) \\(a_n\\) \\(\\mu_n\\) \\(\\xi_n\\) Meaning \\(a_n=0\\) They do not contribute to the predictive model. \\(0 < a_n < C\\) \\(\\mu_n>0\\) \\(\\xi_n=0\\) satisfy the \\(t_ny(\\mathbf{x}_n) = 1\\) \\(a_n=C\\) \\(\\mu_n=0\\) \\(\\xi_n>0\\) lie inside the margin and can either be correctly classified if \\(\\xi\\ll1\\) or misclassified if \\(\\xi_n>1\\) Multiclass SVMs class kerasy . ML . svm . MultipleSVM ( kernel = \"gaussian\" , C = 10 , ** kernelargs ) The support vector machine is fundamentally a two-class classifier, but in practice, we often face the multiple class problems. In that cases, we have to combine the multiple two-class SVMs. Various methods have therefore been proposed for combining, and one commonly used approach (this method is also used in Kerasy) is to construct \\(K\\) separate SVMs, in which the \\(k^{\\text{th}}\\) model \\(y_k(\\mathbf{x})\\) is trained to distinguish whether \\(\\mathbf{x}\\) belongs to class \\(k\\) or not, and final decision is given by $$y(\\mathbf{x}) = \\underset{k}{\\max}y_k(\\mathbf{x})$$ This is known as the one-versus-the-rest approach. Warning This heuristic approach suffers from some problems: There is no guarantee that the real-valued quantities yk(x) for different classifiers will have appropriate scales. The training sets are imbalanced. Reference Pattern Recognition and Machine Learning by Christopher Bishop if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" fonts: [['STIX', 'TeX']],\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Support Vector Machine"},{"location":"MachineLearning/Tree/","text":"Support Vector Machine 2019-11-26(Tue) MachineLearning Notebook Example Notebook: Kerasy.examples.tree.ipynb Decision Tree class kerasy . ML . tree . DecisionTreeClassifier ( criterion = \"gini\" , max_depth = None , random_state = None ) There are various simple , but widely used, models that work by partitioning the input space into cuboid regions , whose edges are aligned with the axes , and assigning a simple model to each region. In this models, one specific model is responsible for making predictions at any given point in input space. Within each region, there is an independent model to predict the target variable. Regression : simply predict a constant over each region Classification : assign each region to a specific class. In order to learn such a model from a training set, we have to determine the structure of the tree about How many edges ? (When to stop adding nodes?) How to chose the variable and threshold ? One simple approach is minimizing the objective function \\(C(T)\\) $$C(T) = \\sum_{\\tau=1}^{|T|}Q_{\\tau}(T) + \\lambda|T|$$ \\(\\lambda\\) determines the trade-off . \\(|T|\\) is the number of leaf nodes. \\(Q(T)\\) is the loss functions like: Residual Sum-of-Squares (Regression): $$Q_{\\tau}(T) = \\sum_{\\mathbf{x}_n\\in\\mathcal{R}_{\\tau}}\\left\\{t_n-y_{\\tau}\\right\\}^2$$ Cross-Entropy (Classification): $$Q_{\\tau}(T) = \\sum_{k=1}^Kp_{\\tau k}\\ln p_{\\tau k}$$ Gini index (Classification): $$Q_{\\tau}(T) = \\sum_{k=1}^Kp_{\\tau k}\\left(1 - p_{\\tau k}\\right)$$ \u203b \\(p_{\\tau k}\\) is the proportion of data points in region \\(\\mathcal{R}_{\\tau}\\) assigned to class \\(k\\) . Strength Human interpretability. Does not require the preprocessing (Scaling, Standardization...) Reference Pattern Recognition and Machine Learning by Christopher Bishop if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" fonts: [['STIX', 'TeX']],\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","title":"Support Vector Machine"}]}